# -*- coding: utf-8 -*-
"""72.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CT9kDwSX2wUYyCeMayerxTPuc64lbvQj
"""

# Install required packages
!pip install -q transformers datasets torch torchvision tqdm pandas matplotlib tensorboard python-docx

# Import base libraries
import os
import logging
import warnings

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Suppress unnecessary warnings
warnings.filterwarnings("ignore", category=UserWarning, message=".*Moving the following attributes in the config to the generation config.*")
warnings.filterwarnings("ignore", category=UserWarning, message=".*but it is unrecognised.*")
warnings.filterwarnings("ignore", category=UserWarning, message=".*rescale already rescaled images.*")
warnings.filterwarnings("ignore", category=FutureWarning, message=".*is deprecated and will be removed.*")

# Define base paths
base_path = '/content'
pdf_folder = os.path.join(base_path, 'pdfs')
output_base_path = os.path.join(base_path, 'ocr_data')
transcriptions_path = os.path.join(base_path, 'transcriptions')
training_data_path = os.path.join(output_base_path, 'training_data')
results_path = os.path.join(output_base_path, 'results')
page_transcription_dir = os.path.join(output_base_path, 'page_transcriptions')
fine_tuned_model_dir = os.path.join(output_base_path, 'fine_tuned_model')

# Create directory structure
directories = [
    pdf_folder,
    os.path.join(output_base_path, "images"),
    os.path.join(output_base_path, "processed_images"),
    os.path.join(output_base_path, "binary_images"),
    training_data_path,
    transcriptions_path,
    results_path,
    page_transcription_dir,
    fine_tuned_model_dir
]

# Create main directories
for directory in directories:
    os.makedirs(directory, exist_ok=True)

# Define document subdirectories with consistent naming
subdirectories = [
    "Buendia - Instruccion-1",
    "Constituciones sinodales Calahorra 1602-2",
    "Ezcaray - Vozes-3",
    "Mendo - Principe perfecto-4",
    "Paredes - Reglas generales-5",
    "PORCONES.228.35 1636-6"  # Fixed the extra space
]

# Create subdirectories in image folders
for subdir in subdirectories:
    os.makedirs(os.path.join(output_base_path, "binary_images", subdir), exist_ok=True)
    os.makedirs(os.path.join(output_base_path, "processed_images", subdir), exist_ok=True)
    os.makedirs(os.path.join(output_base_path, "images", subdir), exist_ok=True)

print("Environment setup complete!")

import os
import re
import logging
from difflib import SequenceMatcher
import sys
import importlib
import subprocess

logger = logging.getLogger(__name__)

def normalize_document_name(doc_name):
    """
    Normalize a document name by removing common suffixes and standardizing format.

    Args:
        doc_name (str): Raw document name that might include page numbers or other suffixes

    Returns:
        str: Normalized document name
    """
    # Remove trailing page numbers
    doc_name = re.sub(r'[-_]\d+$', '', doc_name)

    # Remove suffixes
    suffix_patterns = [
        r' - Instruccion$',
        r' - Principe perfecto$',
        r' - Vozes$',
        r' - Reglas generales$',
        r' sinodales Calahorra 1602$'
    ]

    for pattern in suffix_patterns:
        doc_name = re.sub(pattern, '', doc_name)

    return doc_name.strip()

def get_best_transcription_match(doc_name, available_transcriptions):
    """
    Find the best matching transcription filename for a document.

    Args:
        doc_name (str): Document name to match
        available_transcriptions (list): List of available transcription filenames

    Returns:
        str or None: Best matching transcription name or None if no good match found
    """
    if not available_transcriptions:
        return None

    # Normalize document name
    normalized_doc_name = normalize_document_name(doc_name)

    # Try direct match first
    for transcription in available_transcriptions:
        # Try exact match
        if normalized_doc_name == transcription:
            return transcription

        # Try with variations
        if normalized_doc_name == transcription.replace(' transcription', ''):
            return transcription

    # If no exact match, try fuzzy matching
    best_match = None
    best_ratio = 0.0

    for transcription in available_transcriptions:
        # Remove common suffixes for comparison
        base_transcription = transcription.replace(' transcription', '')

        # Calculate similarity ratio using SequenceMatcher
        ratio = SequenceMatcher(None, normalized_doc_name.lower(), base_transcription.lower()).ratio()
        full_ratio = SequenceMatcher(None, normalized_doc_name.lower(), transcription.lower()).ratio()
        ratio = max(ratio, full_ratio)

        if ratio > best_ratio and ratio > 0.6:  # Threshold for good matches
            best_ratio = ratio
            best_match = transcription

    if best_match:
        logger.info(f"Matched '{doc_name}' to '{best_match}' with confidence {best_ratio:.2f}")

    return best_match

class TranscriptionLoader:
    """
    Loads and processes transcription files from various formats.
    """
    def __init__(self, transcription_dir, page_transcription_dir):
        """
        Initialize the transcription loader.

        Args:
            transcription_dir (str): Directory containing original transcription files
            page_transcription_dir (str): Directory to store page-level transcriptions
        """
        self.transcription_dir = transcription_dir
        self.page_transcription_dir = page_transcription_dir
        os.makedirs(page_transcription_dir, exist_ok=True)

        # Initialize maps
        self.doc_transcription_map = {}
        self.available_transcriptions = []

        # Format handlers
        self.format_handlers = {
            '.txt': self._read_txt_file,
            '.docx': self._read_docx_file
        }

    def find_transcription_files(self):
        """
        Find all transcription files in the transcription directory.

        Returns:
            dict: Dictionary mapping document names to transcription file paths
        """
        # Clear existing maps
        self.doc_transcription_map = {}
        self.available_transcriptions = []

        for file in os.listdir(self.transcription_dir):
            file_path = os.path.join(self.transcription_dir, file)
            if not os.path.isfile(file_path):
                continue

            file_ext = os.path.splitext(file)[1].lower()
            if file_ext in self.format_handlers:
                # Get document name without extension
                doc_name = os.path.splitext(file)[0]
                self.doc_transcription_map[doc_name] = file_path
                self.available_transcriptions.append(doc_name)

        if self.doc_transcription_map:
            logger.info(f"Found {len(self.doc_transcription_map)} transcription files")
            for name in self.available_transcriptions:
                logger.info(f"  - {name}")
        else:
            logger.warning(f"No transcription files found in {self.transcription_dir}")

        return self.doc_transcription_map

    def _read_txt_file(self, file_path):
        """Read text content from a .txt file."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            # Try different encodings
            for encoding in ['latin-1', 'iso-8859-1', 'cp1252']:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        logger.warning(f"File {file_path} decoded using {encoding} instead of utf-8")
                        return f.read()
                except UnicodeDecodeError:
                    continue

            # Last resort
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                logger.error(f"Using fallback encoding with error ignoring for {file_path}")
                return f.read()

    def _read_docx_file(self, file_path):
        """Read text content from a .docx file."""
        try:
            # Try to import python-docx
            try:
                import docx
            except ImportError:
                logger.warning("python-docx not installed. Installing...")
                try:
                    subprocess.check_call([sys.executable, "-m", "pip", "install", "python-docx"])
                    importlib.invalidate_caches()
                    import docx
                except Exception as e:
                    logger.error(f"Failed to install python-docx: {e}")
                    return f"ERROR: Could not read {file_path} (python-docx not available)"

            # Read the docx file
            doc = docx.Document(file_path)
            full_text = []

            # Extract text from paragraphs
            for para in doc.paragraphs:
                full_text.append(para.text)

            # Extract from tables
            for table in doc.tables:
                for row in table.rows:
                    row_texts = [cell.text for cell in row.cells if cell.text.strip()]
                    if row_texts:
                        full_text.append(' | '.join(row_texts))

            return '\n'.join(full_text)
        except Exception as e:
            logger.error(f"Error reading .docx file {file_path}: {str(e)}")
            return f"ERROR: Could not read {file_path}"

    def read_transcription(self, doc_name):
        """
        Read transcription for a specific document.

        Args:
            doc_name (str): Document name (will be normalized and matched)

        Returns:
            str or None: Transcription content or None if not found
        """
        # Find best matching transcription
        best_match = get_best_transcription_match(doc_name, self.available_transcriptions)

        if best_match and best_match in self.doc_transcription_map:
            file_path = self.doc_transcription_map[best_match]
            file_ext = os.path.splitext(file_path)[1].lower()

            if file_ext in self.format_handlers:
                logger.info(f"Reading transcription for '{doc_name}' from '{best_match}'")
                return self.format_handlers[file_ext](file_path)

        logger.warning(f"No transcription found for document {doc_name}")
        return None

# Demo the transcription loader
if __name__ == "__main__":
    # Example test with a mock transcription directory
    test_dir = '/content/transcriptions'
    test_output_dir = '/content/ocr_data/page_transcriptions'

    # Create test files if they don't exist
    if not os.path.exists(os.path.join(test_dir, 'test_transcription.txt')):
        os.makedirs(test_dir, exist_ok=True)
        with open(os.path.join(test_dir, 'test_transcription.txt'), 'w') as f:
            f.write("This is a test transcription\nPDF p1\nPage content 1\nPDF p2\nPage content 2")

    # Create and test loader
    loader = TranscriptionLoader(test_dir, test_output_dir)
    loader.find_transcription_files()

    # Print available transcriptions
    print(f"Available transcriptions: {loader.available_transcriptions}")

    # Test matching
    test_cases = [
        "test_transcription",
        "Test Transcription",
        "test_trans"
    ]

    for test_case in test_cases:
        match = get_best_transcription_match(test_case, loader.available_transcriptions)
        print(f"Best match for '{test_case}': {match}")

        content = loader.read_transcription(test_case)
        if content:
            print(f"  First 50 chars: {content[:50]}...")
        else:
            print("  No content found")

import os
import re
import logging
import glob
from PIL import Image
import matplotlib.pyplot as plt

logger = logging.getLogger(__name__)

def extract_doc_info(image_path):
    """
    Extract document name and page number from image path with improved pattern matching.

    Args:
        image_path (str): Path to the image file

    Returns:
        tuple: (document_name, page_number) or (None, None) if not parsable
    """
    # Extract filename and parent directory
    filename = os.path.basename(image_path)
    parent_dir = os.path.basename(os.path.dirname(image_path))
    filename_without_ext = os.path.splitext(filename)[0]

    # Try multiple pattern recognition strategies

    # Strategy 1: Extract from parent directory and filename ending with -X
    match = re.match(r'(.+)-(\d+)$', parent_dir)
    if match and re.match(r'.*_page_\d+$', filename_without_ext):
        doc_name = parent_dir
        # Extract page number from filename (page_XXX)
        page_match = re.search(r'_page_(\d+)$', filename_without_ext)
        if page_match:
            page_num = int(page_match.group(1))
            return doc_name, page_num

    # Strategy 2: Document_Name_page_XXX.jpg
    match = re.match(r'(.+?)_page_(\d+)', filename_without_ext)
    if match:
        doc_name = match.group(1)
        page_num = int(match.group(2))
        return doc_name, page_num

    # Strategy 3: Use parent directory name with page number suffix in filename
    page_match = re.search(r'[-_](\d+)$', filename_without_ext)
    if page_match and parent_dir:
        # If the parent directory already has document info
        doc_name = parent_dir
        page_num = int(page_match.group(1))
        return doc_name, page_num

    # Strategy 4: Look for page markers in the filename
    page_match = re.search(r'[-_]p(?:age)?[-_]?(\d+)$', filename_without_ext, re.IGNORECASE)
    if page_match:
        page_num = int(page_match.group(1))
        # Remove page suffix to get document name
        doc_name = re.sub(r'[-_]p(?:age)?[-_]?\d+$', '', filename_without_ext, flags=re.IGNORECASE)
        if not doc_name:  # If extraction left nothing, use parent dir
            doc_name = parent_dir
        return doc_name, page_num

    # Strategy 5: If parent dir is a document and no page number in filename,
    # check if other files in same dir have numeric patterns and assign this one sequentially
    if parent_dir in ["Buendia - Instruccion-1", "Constituciones sinodales Calahorra 1602-2",
                      "Ezcaray - Vozes-3", "Mendo - Principe perfecto-4", "Paredes - Reglas generales-5",
                      "PORCONES.228.35 1636-6"]:
        # This is likely a document directory
        # Try to infer page number from the file's position
        sibling_files = sorted([f for f in os.listdir(os.path.dirname(image_path))
                               if f.endswith(('.jpg', '.png'))])
        if filename in sibling_files:
            page_num = sibling_files.index(filename) + 1  # 1-based page numbering
            return parent_dir, page_num

    logger.warning(f"Could not parse document name and page from: {image_path}")
    return None, None

def find_all_images(base_path='/content'):
    """
    Find all processed and binary images with improved metadata extraction.

    Args:
        base_path (str): Base directory containing all data

    Returns:
        tuple: (processed_images, binary_images, image_mapping)
    """
    output_base_path = os.path.join(base_path, 'ocr_data')
    processed_dir = os.path.join(output_base_path, "processed_images")
    binary_dir = os.path.join(output_base_path, "binary_images")

    processed_images = []
    binary_images = []
    image_mapping = {}  # Maps (doc_name, page_num) to {'processed': path, 'binary': path}

    # Store document names for logging
    found_docs = set()
    doc_page_counts = {}

    # Find all processed images
    if os.path.exists(processed_dir):
        for root, _, files in os.walk(processed_dir):
            for file in files:
                if file.endswith(('.jpg', '.png')):
                    img_path = os.path.join(root, file)
                    processed_images.append(img_path)

                    # Extract document info
                    doc_name, page_num = extract_doc_info(img_path)

                    # Fallback if extraction fails
                    if doc_name is None or page_num is None:
                        parent_dir = os.path.basename(os.path.dirname(img_path))
                        if parent_dir not in doc_page_counts:
                            doc_page_counts[parent_dir] = 0
                        doc_page_counts[parent_dir] += 1
                        doc_name = parent_dir
                        page_num = doc_page_counts[parent_dir]

                    found_docs.add(doc_name)
                    if (doc_name, page_num) not in image_mapping:
                        image_mapping[(doc_name, page_num)] = {'processed': img_path}
                    else:
                        # Update if entry exists but doesn't have processed path
                        if 'processed' not in image_mapping[(doc_name, page_num)]:
                            image_mapping[(doc_name, page_num)]['processed'] = img_path
    else:
        logger.warning(f"Processed images directory not found: {processed_dir}")

    # Find all binary images
    if os.path.exists(binary_dir):
        for root, _, files in os.walk(binary_dir):
            for file in files:
                if file.endswith(('.jpg', '.png')):
                    img_path = os.path.join(root, file)
                    binary_images.append(img_path)

                    # Extract document info
                    doc_name, page_num = extract_doc_info(img_path)

                    # Fallback if extraction fails - try to match with processed image
                    if doc_name is None or page_num is None:
                        parent_dir = os.path.basename(os.path.dirname(img_path))
                        filename = os.path.basename(img_path)

                        # Try to find matching processed image
                        match_found = False
                        for (d, p), mapping in image_mapping.items():
                            if d == parent_dir and os.path.basename(mapping.get('processed', '')) == filename:
                                doc_name = d
                                page_num = p
                                match_found = True
                                break

                        # If no match found, create a new entry
                        if not match_found:
                            if parent_dir not in doc_page_counts:
                                doc_page_counts[parent_dir] = 0
                            doc_page_counts[parent_dir] += 1
                            doc_name = parent_dir
                            page_num = doc_page_counts[parent_dir]

                    found_docs.add(doc_name)
                    if (doc_name, page_num) in image_mapping:
                        image_mapping[(doc_name, page_num)]['binary'] = img_path
                    else:
                        image_mapping[(doc_name, page_num)] = {'binary': img_path}
    else:
        logger.warning(f"Binary images directory not found: {binary_dir}")

    # Log results
    logger.info(f"Found {len(processed_images)} processed images and {len(binary_images)} binary images")
    logger.info(f"Identified {len(found_docs)} unique documents")
    logger.info(f"Created {len(image_mapping)} document-page mappings")

    # Validate mapping
    for (doc, page), paths in image_mapping.items():
        if 'processed' not in paths:
            logger.warning(f"Missing processed image for {doc}, page {page}")
        if 'binary' not in paths:
            logger.warning(f"Missing binary image for {doc}, page {page}")

    return processed_images, binary_images, image_mapping

def create_page_transcriptions(loader, image_mapping, max_pages_per_doc=6):
    """
    Create page-level transcriptions with improved page detection and content splitting.

    Args:
        loader (TranscriptionLoader): Initialized transcription loader
        image_mapping (dict): Dictionary mapping (doc_name, page_num) to image paths
        max_pages_per_doc (int): Maximum pages to process per document

    Returns:
        dict: Dictionary mapping (doc_name, page_num) to page data including transcription
    """
    page_transcriptions = {}
    doc_names = sorted(list(set(doc_name for doc_name, _ in image_mapping.keys())))
    logger.info(f"Processing transcriptions for {len(doc_names)} documents")

    for doc_name in doc_names:
        # Get full transcription for this document
        full_transcription = loader.read_transcription(doc_name)
        if not full_transcription:
            logger.warning(f"No transcription found for {doc_name}, skipping")
            continue

        # Find pages for this document
        doc_pages = sorted([page_num for d, page_num in image_mapping.keys()
                            if d == doc_name and page_num <= max_pages_per_doc])

        if not doc_pages:
            logger.warning(f"No pages found for {doc_name}")
            continue

        logger.info(f"Processing {doc_name}, pages: {doc_pages}")

        # Try different strategies for page splitting:

        # STRATEGY 1: Look for explicit page markers
        # Common patterns in transcriptions: "PDF p1", "Page 1", "**PDF p1**"
        page_markers = []

        # Pattern 1: Look for "PDF p1", "Page 1" or similar
        pattern1 = re.finditer(r'(?:PDF\s+p|Page\s+|PDF\s+page\s+)(\d+)', full_transcription, re.IGNORECASE)
        for match in pattern1:
            page_num = int(match.group(1))
            if page_num in doc_pages:
                page_markers.append((page_num, match.start(), match.end()))

        # Pattern 2: Look for "**PDF p1**" (markdown style)
        pattern2 = re.finditer(r'\*\*(?:PDF\s+p|Page\s+)(\d+)\*\*', full_transcription, re.IGNORECASE)
        for match in pattern2:
            page_num = int(match.group(1))
            if page_num in doc_pages:
                page_markers.append((page_num, match.start(), match.end()))

        # Sort by position in text
        page_markers.sort(key=lambda x: x[1])

        if page_markers:
            logger.info(f"Found {len(page_markers)} page markers in {doc_name}")

            # Create page-level transcriptions from markers
            for i, (page_num, start_idx, end_idx) in enumerate(page_markers):
                # Determine end boundary for this page
                if i < len(page_markers) - 1:
                    next_start = page_markers[i+1][1]
                    text = full_transcription[end_idx:next_start]
                else:
                    text = full_transcription[end_idx:]

                # Clean up the text
                text = text.strip()

                # Only include pages that have a corresponding image
                key = (doc_name, page_num)
                if key in image_mapping and 'processed' in image_mapping[key]:
                    page_transcriptions[key] = {
                        'text': text,
                        'processed_image': image_mapping[key].get('processed'),
                        'binary_image': image_mapping[key].get('binary'),
                        'source': 'page_markers'
                    }
                    logger.info(f"Created transcription for {doc_name}, page {page_num} using markers")

        # STRATEGY 2: If no markers or missing pages, do a length-based split
        missing_pages = [p for p in doc_pages if (doc_name, p) not in page_transcriptions]

        if missing_pages:
            logger.warning(f"No markers found for pages {missing_pages} in {doc_name}. Using naive split.")

            # Remove any existing markers to split full text
            clean_text = re.sub(r'(?:PDF\s+p|Page\s+)(\d+)', '', full_transcription, flags=re.IGNORECASE)
            clean_text = re.sub(r'\*\*(?:PDF\s+p|Page\s+)(\d+)\*\*', '', clean_text, flags=re.IGNORECASE)

            # Split by paragraphs
            paragraphs = [p for p in clean_text.split('\n\n') if p.strip()]

            # Estimate paragraphs per page
            total_paragraphs = len(paragraphs)
            total_pages = len(missing_pages)
            if total_pages > 0:
                paras_per_page = max(1, total_paragraphs // total_pages)

                for i, page_num in enumerate(missing_pages):
                    start_idx = i * paras_per_page
                    end_idx = min((i + 1) * paras_per_page, total_paragraphs)

                    # Only process if we have content and the page is in range
                    if start_idx < total_paragraphs and page_num <= max_pages_per_doc:
                        text = '\n\n'.join(paragraphs[start_idx:end_idx])

                        key = (doc_name, page_num)
                        if key in image_mapping and 'processed' in image_mapping[key]:
                            page_transcriptions[key] = {
                                'text': text,
                                'processed_image': image_mapping[key].get('processed'),
                                'binary_image': image_mapping[key].get('binary'),
                                'source': 'length_based_split'
                            }
                            logger.info(f"Created transcription for {doc_name}, page {page_num} using naive split")

    # Check coverage
    total_pages = sum(1 for k, v in image_mapping.items()
                       if 'processed' in v and k[1] <= max_pages_per_doc)
    covered_pages = len(page_transcriptions)
    coverage_pct = (covered_pages / total_pages * 100) if total_pages > 0 else 0

    logger.info(f"Created {covered_pages} page transcriptions out of {total_pages} pages ({coverage_pct:.1f}% coverage)")

    return page_transcriptions

def display_page_samples(page_transcriptions, n_samples=2):
    """Display sample pages to verify alignment."""
    if not page_transcriptions:
        print("No page transcriptions to display")
        return

    # Get a few samples
    samples = list(page_transcriptions.items())[:n_samples]

    for (doc_name, page_num), data in samples:
        print(f"\n{'='*50}")
        print(f"Document: {doc_name}")
        print(f"Page: {page_num}")
        print(f"Source: {data.get('source', 'unknown')}")
        print(f"{'='*50}")

        # Try to display image
        try:
            img_path = data.get('processed_image')
            if img_path and os.path.exists(img_path):
                img = Image.open(img_path)
                plt.figure(figsize=(8, 10))
                plt.imshow(img)
                plt.axis('off')
                plt.title(f"{doc_name} - Page {page_num}")
                plt.show()
            else:
                print(f"Image not found: {img_path}")
        except Exception as e:
            print(f"Error displaying image: {e}")

        # Display text excerpt
        text = data.get('text', '')
        print(f"\nText excerpt (first 300 chars):")
        print(f"{text[:300]}...")
        print(f"\nText length: {len(text)} characters")
        print(f"{'='*50}")

# Test the improved functions
if __name__ == "__main__":
    # Basic demo with placeholder files
    print("Testing image finding and page detection...")

    # Create a test directory structure if it doesn't exist
    test_base = '/content'
    test_output = os.path.join(test_base, 'ocr_data')
    test_processed = os.path.join(test_output, 'processed_images', 'Test-Document-1')

    os.makedirs(test_processed, exist_ok=True)

    # Create sample test files if they don't exist
    for i in range(1, 4):
        test_file = os.path.join(test_processed, f"test_page_{i:03d}.jpg")
        if not os.path.exists(test_file):
            # Create blank image
            img = Image.new('RGB', (100, 150), color=(255, 255, 255))
            img.save(test_file)

    # Find images
    processed_images, binary_images, image_mapping = find_all_images(test_base)
    print(f"Found {len(processed_images)} processed images")
    print(f"Created {len(image_mapping)} document-page mappings")

    # Print some example mappings
    for i, ((doc, page), paths) in enumerate(image_mapping.items()):
        if i < 3:  # Just show first 3
            print(f"{doc}, Page {page}: {paths}")

import os
import logging
import random
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt

logger = logging.getLogger(__name__)

def create_train_val_split(page_transcriptions, val_ratio=0.2, seed=42):
    """
    Split the page transcriptions into training and validation sets.

    Args:
        page_transcriptions (dict): Dictionary mapping (doc_name, page_num) to page data
        val_ratio (float): Ratio of validation data (0-1)
        seed (int): Random seed for reproducibility

    Returns:
        tuple: (train_data, val_data) dictionaries
    """
    random.seed(seed)

    # Get all keys (document_name, page_number) tuples
    keys = list(page_transcriptions.keys())

    # Group by document to ensure we have a mix of documents in both sets
    doc_groups = {}
    for doc_name, page_num in keys:
        if doc_name not in doc_groups:
            doc_groups[doc_name] = []
        doc_groups[doc_name].append((doc_name, page_num))

    train_keys = []
    val_keys = []

    # Split each document's pages
    for doc_name, doc_keys in doc_groups.items():
        random.shuffle(doc_keys)
        split_idx = max(1, int(len(doc_keys) * (1 - val_ratio)))

        train_keys.extend(doc_keys[:split_idx])
        val_keys.extend(doc_keys[split_idx:])

    # Create dictionaries
    train_data = {k: page_transcriptions[k] for k in train_keys}
    val_data = {k: page_transcriptions[k] for k in val_keys}

    logger.info(f"Created train-val split:")
    logger.info(f"  Training set: {len(train_data)} pages from {len(set(k[0] for k in train_keys))} documents")
    logger.info(f"  Validation set: {len(val_data)} pages from {len(set(k[0] for k in val_keys))} documents")

    return train_data, val_data

def get_train_transforms():
    """
    Create augmentation transforms for training data.
    """
    return transforms.Compose([
        transforms.Resize((384, 384)),  # TrOCR expected size
        transforms.RandomRotation(1),   # Slight rotation (reduced from 3 to be gentler)
        transforms.RandomApply([
            transforms.ColorJitter(brightness=0.1, contrast=0.1)
        ], p=0.2),  # Subtle color adjustments with lower probability
        transforms.ToTensor(),
    ])

def get_val_transforms():
    """
    Create transforms for validation data.
    """
    return transforms.Compose([
        transforms.Resize((384, 384)),  # TrOCR expected size
        transforms.ToTensor(),
    ])

class HistoricalDocumentDataset(Dataset):
    """
    Dataset class for historical Spanish documents.
    """
    def __init__(self, page_data, processor, transform=None, max_length=512):
        """
        Initialize the dataset.

        Args:
            page_data (dict): Dictionary mapping (doc_name, page_num) to page data
            processor: TrOCR processor for tokenization
            transform: Optional transforms to apply to images
            max_length (int): Maximum sequence length for tokenization
        """
        self.page_keys = list(page_data.keys())
        self.page_data = page_data
        self.processor = processor
        self.transform = transform
        self.max_length = max_length

    def __len__(self):
        return len(self.page_keys)

    def __getitem__(self, idx):
        key = self.page_keys[idx]
        data = self.page_data[key]

        # Load image with error handling
        try:
            image_path = data['processed_image']
            image = Image.open(image_path).convert('RGB')

            # Apply transforms if provided
            if self.transform:
                image = self.transform(image)
            else:
                # Default processing if no transform provided
                image = transforms.ToTensor()(transforms.Resize((384, 384))(image))

        except Exception as e:
            logger.error(f"Error loading image {data.get('processed_image')}: {e}")
            # Create blank image as fallback
            image = torch.zeros(3, 384, 384)

        # Get and truncate text if needed
        text = data.get('text', '')
        if len(text) > self.max_length:
            text = text[:self.max_length]

        # We'll handle tokenization separately to avoid errors in __getitem__
        return {
            "image": image,
            "text": text,
            "doc_name": key[0],
            "page_num": key[1],
            "image_path": data.get('processed_image', '')
        }

def prepare_batch_for_model(batch, processor, device):
    """
    Process a batch for the model, handling tokenization separately from __getitem__.

    Args:
        batch (dict): Batch of data from dataloader
        processor: TrOCR processor
        device: Device to move tensors to

    Returns:
        dict: Processed batch ready for model
    """
    # Get images and move to device
    images = batch["image"].to(device)

    # Process images with processor
    try:
        pixel_values = processor(images=images, return_tensors="pt").pixel_values.to(device)
    except Exception as e:
        logger.error(f"Error processing images: {e}")
        # Fallback to original images reshaped
        pixel_values = images

    # Tokenize texts
    texts = batch["text"]
    encodings = processor.tokenizer(
        texts,
        padding="max_length",
        max_length=512,
        truncation=True,
        return_tensors="pt"
    )
    labels = encodings.input_ids.to(device)

    # Replace padding token id with -100 so it's ignored in loss
    labels[labels == processor.tokenizer.pad_token_id] = -100

    return {
        "pixel_values": pixel_values,
        "labels": labels,
        "texts": texts,
        "image_paths": batch["image_path"]
    }

def create_dataloaders(train_data, val_data, processor, batch_size=4):
    """
    Create DataLoaders for training and validation.

    Args:
        train_data (dict): Training data dictionary
        val_data (dict): Validation data dictionary
        processor: TrOCR processor
        batch_size (int): Batch size

    Returns:
        tuple: (train_loader, val_loader)
    """
    # Create datasets
    train_dataset = HistoricalDocumentDataset(
        train_data,
        processor,
        transform=get_train_transforms()
    )

    val_dataset = HistoricalDocumentDataset(
        val_data,
        processor,
        transform=get_val_transforms()
    )

    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=2 if torch.cuda.is_available() else 0
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2 if torch.cuda.is_available() else 0
    )

    return train_loader, val_loader, train_dataset, val_dataset

def visualize_batch(batch, processor, num_samples=2):
    """
    Visualize a batch of data.

    Args:
        batch: Batch from dataloader
        processor: TrOCR processor
        num_samples (int): Number of samples to visualize
    """
    images = batch["image"]
    texts = batch["text"]
    doc_names = batch["doc_name"]
    page_nums = batch["page_num"]

    num_samples = min(num_samples, len(images))

    for i in range(num_samples):
        plt.figure(figsize=(10, 12))

        # Convert tensor to numpy for display
        if torch.is_tensor(images[i]):
            # If tensor is in [0,1] range, scale to [0,255]
            img_np = images[i].permute(1, 2, 0).cpu().numpy()
            if img_np.max() <= 1.0:
                img_np = img_np * 255

            plt.imshow(img_np.astype('uint8'))
        else:
            plt.imshow(images[i])

        plt.title(f"{doc_names[i]} - Page {page_nums[i]}")
        plt.axis('off')
        plt.show()

        print(f"Text sample ({len(texts[i])} chars):")
        print(f"{texts[i][:500]}...")
        print("-" * 50)

# Test the dataset creation

if __name__ == "__main__":
    # ... (other code)

    @dataclass
    class DummyProcessor:
        tokenizer: DummyTokenizer = field(default_factory=DummyTokenizer) # Use default_factory to create a new instance

        def __call__(self, images=None, return_tensors=None):
            class Output:
                pixel_values = torch.rand((len(images), 3, 384, 384))
            return Output()


    @dataclass
    class DummyProcessor:
         tokenizer: DummyTokenizer = field(default_factory=DummyTokenizer)  # Use default_factory to create a new instance

         def __call__(self, images=None, return_tensors=None):
             class Output:
                pixel_values = torch.rand((len(images), 3, 384, 384))
             return Output()

    # Create dummy data
    dummy_processor = DummyProcessor()

    # Create some test transcriptions if they don't exist
    dummy_data = {
        ('Test-Document-1', 1): {
            'text': 'This is a test page 1 text that would normally be much longer.',
            'processed_image': '/content/ocr_data/processed_images/Test-Document-1/test_page_001.jpg'
        },
        ('Test-Document-1', 2): {
            'text': 'This is a test page 2 text with different content.',
            'processed_image': '/content/ocr_data/processed_images/Test-Document-1/test_page_002.jpg'
        },
        ('Test-Document-2', 1): {
            'text': 'This is from another document, page 1.',
            'processed_image': '/content/ocr_data/processed_images/Test-Document-2/test_page_001.jpg'
        }
    }

    # Create test image if it doesn't exist
    for key, data in dummy_data.items():
        os.makedirs(os.path.dirname(data['processed_image']), exist_ok=True)
        if not os.path.exists(data['processed_image']):
            img = Image.new('RGB', (100, 150), color=(255, 255, 255))
            img.save(data['processed_image'])

    # Test train/val split
    train_data, val_data = create_train_val_split(dummy_data)

    # Test dataset creation
    train_dataset = HistoricalDocumentDataset(train_data, dummy_processor, transform=get_train_transforms())

    print(f"Dataset size: {len(train_dataset)}")
    if len(train_dataset) > 0:
        sample = train_dataset[0]
        print(f"Sample keys: {sample.keys()}")

import torch
import logging
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
import warnings

# Suppress specific warnings that we know about and can't fix
warnings.filterwarnings("ignore", message=".*Some weights of VisionEncoderDecoderModel were not initialized.*")
warnings.filterwarnings("ignore", message=".*Moving the following attributes in the config to the generation config.*")

logger = logging.getLogger(__name__)

def initialize_model(model_name="microsoft/trocr-base-printed", device=None):
    """
    Initialize a TrOCR model with proper configuration for historical document OCR.

    Args:
        model_name (str): Name or path of the model to load
        device: Device to move model to (auto-detect if None)

    Returns:
        tuple: (model, processor)
    """
    logger.info(f"Initializing model: {model_name}")

    # Determine device
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    try:
        # Load processor and model
        processor = TrOCRProcessor.from_pretrained(model_name)
        model = VisionEncoderDecoderModel.from_pretrained(model_name)

        # Set essential token IDs in model configuration
        model.config.decoder_start_token_id = processor.tokenizer.bos_token_id
        model.config.pad_token_id = processor.tokenizer.pad_token_id
        model.config.eos_token_id = processor.tokenizer.eos_token_id

        # Ensure decoder knows about token IDs
        model.config.decoder.decoder_start_token_id = processor.tokenizer.bos_token_id
        model.config.decoder.pad_token_id = processor.tokenizer.pad_token_id
        model.config.decoder.eos_token_id = processor.tokenizer.eos_token_id

        # Set vocabulary size
        model.config.vocab_size = model.config.decoder.vocab_size

        # Configure generation parameters
        model.config.max_length = 512  # Historical documents can be longer
        model.config.early_stopping = True
        model.config.no_repeat_ngram_size = 3
        model.config.length_penalty = 2.0
        model.config.num_beams = 4

        # Move to device
        model = model.to(device)
        logger.info(f"Model loaded and moved to {device}")

        # Display model parameters
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        logger.info(f"Model has {total_params:,} parameters ({trainable_params:,} trainable)")

        return model, processor, device

    except Exception as e:
        logger.error(f"Error initializing model: {str(e)}")
        raise

def optimize_memory_usage(model):
    """
    Apply memory optimization techniques to reduce VRAM usage.

    Args:
        model: The model to optimize

    Returns:
        The optimized model
    """
    logger.info("Applying memory optimizations")

    # 1. Enable gradient checkpointing (trades memory for computation)
    model.gradient_checkpointing_enable()
    logger.info("Enabled gradient checkpointing")

    # 2. For extreme memory pressure scenarios
    if torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory < 8e9:  # Less than 8GB VRAM
        logger.info("Low VRAM detected. Applying additional optimizations.")

        # Freeze encoder parameters
        for param in model.encoder.parameters():
            param.requires_grad = False

        # Unfreeze only the last encoder layers
        if hasattr(model.encoder, 'layer'):
            num_unfrozen = 2  # Unfreeze only the last 2 layers
            for i, layer in enumerate(model.encoder.layer):
                if i >= len(model.encoder.layer) - num_unfrozen:
                    for param in layer.parameters():
                        param.requires_grad = True

        logger.info(f"Froze encoder parameters except for the last {num_unfrozen} layers")

    return model

def validate_model_processor(model, processor):
    """
    Validate that the model and processor are properly configured.

    Args:
        model: The TrOCR model
        processor: The TrOCR processor

    Returns:
        bool: True if valid, False otherwise
    """
    # Basic validation checks
    try:
        # 1. Check token IDs match
        if model.config.decoder_start_token_id != processor.tokenizer.bos_token_id:
            logger.warning("Decoder start token ID doesn't match processor BOS token ID")
            return False

        # 2. Check for NoneType token IDs
        if model.config.pad_token_id is None:
            logger.warning("Model pad token ID is None")
            return False

        # 3. Verify basic model structure
        if not hasattr(model, 'encoder') or not hasattr(model, 'decoder'):
            logger.warning("Model missing encoder or decoder component")
            return False

        # 4. Simple test with a random input
        with torch.no_grad():
            test_input = torch.rand(1, 3, 384, 384, device=model.device)
            test_output = model.generate(test_input, max_length=10)

            if test_output is None or test_output.shape[0] != 1:
                logger.warning("Model failed to generate with test input")
                return False

        # If we got here, everything looks good
        logger.info("Model and processor validation successful")
        return True

    except Exception as e:
        logger.error(f"Error during model validation: {str(e)}")
        return False

# Test model initialization
if __name__ == "__main__":
    try:
        model, processor, device = initialize_model()

        # Validate model
        if validate_model_processor(model, processor):
            print("Model initialized and validated successfully!")

            # Apply memory optimizations
            model = optimize_memory_usage(model)
            print("Memory optimizations applied.")

            # Test with a random input
            with torch.no_grad():
                test_input = torch.rand(1, 3, 384, 384, device=device)
                outputs = model.generate(test_input, max_length=20)
                decoded = processor.batch_decode(outputs, skip_special_tokens=True)
                print(f"Test generation: {decoded[0]}")
        else:
            print("Model validation failed!")
    except Exception as e:
        print(f"Error in model initialization: {str(e)}")

import numpy as np
import torch
import logging
import re
from difflib import SequenceMatcher

logger = logging.getLogger(__name__)

def normalize_text(text):
    """
    Normalize text for evaluation metrics.

    Args:
        text (str): Input text

    Returns:
        str: Normalized text
    """
    if not isinstance(text, str):
        return ""

    # Convert to lowercase
    text = text.lower()

    # Remove punctuation except spaces
    text = re.sub(r'[^\w\s]', '', text)

    # Normalize whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    # Normalize Spanish characters
    text = text.replace('ñ', 'n')
    text = text.replace('á', 'a').replace('é', 'e').replace('í', 'i').replace('ó', 'o').replace('ú', 'u')
    text = text.replace('ü', 'u')

    return text

def levenshtein_distance(s1, s2):
    """
    Calculate Levenshtein (edit) distance between two sequences.

    Args:
        s1: First sequence (string or list)
        s2: Second sequence (string or list)

    Returns:
        int: Edit distance
    """
    # Handle empty strings
    if len(s1) == 0:
        return len(s2)
    if len(s2) == 0:
        return len(s1)

    # Convert to list if strings to handle both character and word-level operations
    if isinstance(s1, str) and isinstance(s2, str):
        # For very long strings, we'll implement a more memory-efficient version
        if len(s1) > 1000 or len(s2) > 1000:
            return levenshtein_distance_optimized(s1, s2)

    # Create matrix
    d = [[0 for _ in range(len(s2) + 1)] for _ in range(len(s1) + 1)]

    # Initialize first row and column
    for i in range(len(s1) + 1):
        d[i][0] = i
    for j in range(len(s2) + 1):
        d[0][j] = j

    # Fill the matrix
    for i in range(1, len(s1) + 1):
        for j in range(1, len(s2) + 1):
            cost = 0 if s1[i-1] == s2[j-1] else 1
            d[i][j] = min(
                d[i-1][j] + 1,      # deletion
                d[i][j-1] + 1,      # insertion
                d[i-1][j-1] + cost  # substitution
            )

    return d[len(s1)][len(s2)]

def levenshtein_distance_optimized(s1, s2):
    """
    Memory-optimized Levenshtein distance for very long strings.
    Only uses O(min(len(s1), len(s2))) memory.

    Args:
        s1: First string
        s2: Second string

    Returns:
        int: Edit distance
    """
    # Ensure s1 is the shorter string for efficiency
    if len(s1) > len(s2):
        s1, s2 = s2, s1

    # Create a single row for dynamic programming
    previous_row = list(range(len(s2) + 1))

    for i, c1 in enumerate(s1):
        # Initialize current row with first element as i+1
        current_row = [i + 1]

        for j, c2 in enumerate(s2):
            # Calculate insertions, deletions, and substitutions
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (0 if c1 == c2 else 1)

            # Get minimum operation
            current_row.append(min(insertions, deletions, substitutions))

        # Update previous row
        previous_row = current_row

    return previous_row[-1]

def character_error_rate(reference, hypothesis):
    """
    Calculate Character Error Rate (CER).

    Args:
        reference (str): Ground truth text
        hypothesis (str): Predicted text

    Returns:
        float: CER (0-1, lower is better)
    """
    # Handle edge cases
    if not reference:
        return 1.0 if hypothesis else 0.0

    # Normalize texts
    reference = normalize_text(reference)
    hypothesis = normalize_text(hypothesis)

    # Compute Levenshtein distance
    distance = levenshtein_distance(reference, hypothesis)

    # CER = edit distance / length of reference
    return distance / len(reference)

def word_error_rate(reference, hypothesis):
    """
    Calculate Word Error Rate (WER).

    Args:
        reference (str): Ground truth text
        hypothesis (str): Predicted text

    Returns:
        float: WER (0-1, lower is better)
    """
    # Handle edge cases
    if not reference:
        return 1.0 if hypothesis else 0.0

    # Normalize texts
    reference = normalize_text(reference)
    hypothesis = normalize_text(hypothesis)

    # Split into words
    reference_words = reference.split()
    hypothesis_words = hypothesis.split()

    # Handle empty word lists
    if not reference_words:
        return 1.0 if hypothesis_words else 0.0

    # Compute Levenshtein distance at word level
    distance = levenshtein_distance(reference_words, hypothesis_words)

    # WER = edit distance / number of words in reference
    return distance / len(reference_words)

def compute_metrics(pred, processor):
    """
    Compute evaluation metrics for model predictions.

    Args:
        pred: Prediction object from Trainer.evaluate() or Trainer.predict()
        processor: TrOCR processor

    Returns:
        dict: Dictionary of metrics
    """
    # Get predictions and reference labels
    labels = pred.label_ids
    preds = pred.predictions

    # Handle case where predictions are a tuple (logits, ...)
    if isinstance(preds, tuple):
        preds = preds[0]

    # Replace -100 with pad token ID for proper decoding
    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)

    # Decode token IDs to text
    decoded_preds = processor.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)

    # Compute CER and WER for each sample
    cer_values = []
    wer_values = []

    for pred, label in zip(decoded_preds, decoded_labels):
        if label:  # Skip empty references
            cer = character_error_rate(label, pred)
            wer = word_error_rate(label, pred)

            cer_values.append(cer)
            wer_values.append(wer)

    # Compute averages
    avg_cer = np.mean(cer_values) if cer_values else 1.0
    avg_wer = np.mean(wer_values) if wer_values else 1.0

    # Log a few examples
    num_examples = min(3, len(decoded_preds))
    for i in range(num_examples):
        logger.info(f"Example {i+1}:")
        logger.info(f"  Reference: {decoded_labels[i][:100]}...")
        logger.info(f"  Prediction: {decoded_preds[i][:100]}...")
        if i < len(cer_values):
            logger.info(f"  CER: {cer_values[i]:.4f}, WER: {wer_values[i]:.4f}")

    return {
        "cer": avg_cer,
        "wer": avg_wer,
        "accuracy": 1.0 - avg_cer  # Accuracy is 1 - CER
    }

def evaluate_model(model, processor, eval_loader, device=None):
    """
    Evaluate model on a dataset without using the Trainer API.

    Args:
        model: TrOCR model
        processor: TrOCR processor
        eval_loader: DataLoader for evaluation data
        device: Device to run evaluation on

    Returns:
        dict: Dictionary of metrics
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model.eval()
    all_preds = []
    all_labels = []
    all_texts = []

    logger.info(f"Evaluating model on {len(eval_loader.dataset)} samples")

    with torch.no_grad():
        for batch in eval_loader:
            # Get images and texts
            images = batch["image"].to(device)
            texts = batch["text"]

            # Process images
            pixel_values = processor(images=images, return_tensors="pt").pixel_values.to(device)

            # Generate predictions
            generated_ids = model.generate(
                pixel_values,
                max_length=512,
                num_beams=4,
                early_stopping=True
            )

            # Tokenize reference texts
            tokenized = processor.tokenizer(texts, padding=True, return_tensors="pt")
            labels = tokenized.input_ids.to(device)

            # Store predictions and labels
            all_preds.extend(generated_ids.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_texts.extend(texts)

    # Create prediction object similar to Trainer's output
    class Prediction:
        def __init__(self, predictions, label_ids):
            self.predictions = predictions
            self.label_ids = label_ids

    pred_obj = Prediction(np.array(all_preds), np.array(all_labels))

    # Compute metrics
    metrics = compute_metrics(pred_obj, processor)

    # Log metrics
    logger.info(f"Evaluation results:")
    logger.info(f"  CER: {metrics['cer']:.4f}")
    logger.info(f"  WER: {metrics['wer']:.4f}")
    logger.info(f"  Accuracy: {metrics['accuracy']:.4f}")

    # Return metrics and raw predictions for further analysis
    metrics["texts"] = all_texts
    metrics["predictions"] = processor.batch_decode(all_preds, skip_special_tokens=True)

    return metrics

# Test the metrics functions
if __name__ == "__main__":
    # Test CER and WER with different examples
    test_cases = [
        # Perfect match
        ("This is a test.", "This is a test."),
        # Character errors
        ("This is a test.", "Thes iz a test."),
        # Word errors
        ("This is a test.", "This was a test."),
        # Multiple errors
        ("This is a test sentence.", "Thes iz a sample text."),
        # Spanish text with accents
        ("El niño está en la escuela.", "El nino esta en la escuela."),
        # Empty reference
        ("", "This should be ignored"),
        # Empty hypothesis
        ("This reference has no prediction", "")
    ]

    print("Testing Character Error Rate (CER) and Word Error Rate (WER):")
    print("=" * 60)
    print(f"{'Reference':<30} | {'Hypothesis':<30} | {'CER':<6} | {'WER':<6}")
    print("-" * 60)

    for ref, hyp in test_cases:
        cer = character_error_rate(ref, hyp)
        wer = word_error_rate(ref, hyp)

        # Truncate long texts for display
        ref_display = (ref[:27] + "...") if len(ref) > 30 else ref
        hyp_display = (hyp[:27] + "...") if len(hyp) > 30 else hyp

        print(f"{ref_display:<30} | {hyp_display:<30} | {cer:.4f} | {wer:.4f}")

import logging
import os
import time
import torch
import numpy as np
from transformers import (
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    TrainerCallback,
    TrainingArguments,
    TrainerState,
    TrainerControl
)
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR
import pandas as pd
from IPython.display import display, HTML

logger = logging.getLogger(__name__)

def get_training_args(output_dir, batch_size=4, num_epochs=10, learning_rate=5e-5):
    """
    Create training arguments specifically tuned for historical document OCR.

    Args:
        output_dir (str): Output directory for checkpoints and logs
        batch_size (int): Per-device batch size
        num_epochs (int): Number of training epochs
        learning_rate (float): Learning rate

    Returns:
        Seq2SeqTrainingArguments: Training arguments
    """
    # Determine if we can use mixed precision
    fp16 = torch.cuda.is_available()
    gradient_accumulation_steps = 4 if not fp16 else 2  # Reduce accumulation with fp16

    # Create base training arguments
    args = Seq2SeqTrainingArguments(
        output_dir=output_dir,
        eval_strategy="epoch",  # Evaluate after each epoch
        save_strategy="epoch",  # Save checkpoint after each epoch
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        learning_rate=learning_rate,
        num_train_epochs=num_epochs,
        weight_decay=0.01,  # L2 regularization
        fp16=fp16,  # Mixed precision if available
        generation_max_length=512,  # For long historical texts
        predict_with_generate=True,  # Use generation for evaluation
        # Scheduler settings
        warmup_ratio=0.1,  # Warm up over 10% of steps
        # Logging settings
        logging_strategy="steps",
        logging_steps=10,
        logging_dir=os.path.join(output_dir, "logs"),
        # Evaluation settings
        metric_for_best_model="cer",
        greater_is_better=False,  # Lower CER is better
        load_best_model_at_end=True,
        # Misc settings
        report_to="tensorboard",
        save_total_limit=2,  # Only keep 2 checkpoints to save space
        remove_unused_columns=False,  # Keep all columns for custom processing
        dataloader_drop_last=False,  # Don't drop last batch
    )

    return args

class LoggingCallback(TrainerCallback):
    """
    Custom callback for better logging during training.
    """
    def __init__(self):
        self.training_tracker = []
        self.best_metrics = {"cer": float('inf'), "wer": float('inf'), "epoch": 0}

    def on_epoch_end(self, args, state, control, metrics=None, **kwargs):
        """Log metrics at the end of each epoch."""
        if metrics:
            # Convert metrics to a more readable format
            epoch_metrics = {
                "epoch": round(state.epoch, 2),
                "train_loss": metrics.get('train_loss', float('nan')),
                "eval_loss": metrics.get('eval_loss', float('nan')),
                "cer": metrics.get('eval_cer', float('nan')),
                "wer": metrics.get('eval_wer', float('nan'))
            }

            # Track metrics
            self.training_tracker.append(epoch_metrics)

            # Check if this is the best model
            if epoch_metrics["cer"] < self.best_metrics["cer"]:
                self.best_metrics["cer"] = epoch_metrics["cer"]
                self.best_metrics["wer"] = epoch_metrics["wer"]
                self.best_metrics["epoch"] = epoch_metrics["epoch"]

            # Display current metrics in a nice table
            self._display_metrics_table()

            # Show best metrics so far
            print(f"\nBest metrics so far (epoch {self.best_metrics['epoch']}):")
            print(f"  CER: {self.best_metrics['cer']:.4f}")
            print(f"  WER: {self.best_metrics['wer']:.4f}")

    def _display_metrics_table(self):
        """Display metrics in a formatted table."""
        # Convert tracking data to DataFrame
        df = pd.DataFrame(self.training_tracker)

        # Format the metrics (round to 4 decimal places)
        for col in df.columns:
            if col != "epoch":
                df[col] = df[col].apply(lambda x: f"{x:.4f}" if pd.notnull(x) else "N/A")

        # Display the table
        print("\nTraining Progress:")
        display(HTML(df.to_html(index=False)))

class PredictionDisplayCallback(TrainerCallback):
    """
    Callback to display model predictions during training.
    """
    def __init__(self, eval_dataset, processor, device, display_freq=2):
        self.eval_dataset = eval_dataset
        self.processor = processor
        self.device = device
        self.display_freq = display_freq  # Display every N epochs

    def on_epoch_end(self, args, state, control, **kwargs):
        # Only display predictions occasionally
        if round(state.epoch) % self.display_freq != 0:
            return

        # Get model from kwargs
        model = kwargs.get('model')
        if model is None:
            return

        print(f"\nSample predictions at epoch {round(state.epoch)}:")

        # Sample a few examples
        num_examples = min(2, len(self.eval_dataset))
        indices = np.random.choice(len(self.eval_dataset), num_examples, replace=False)

        for idx in indices:
            sample = self.eval_dataset[idx]

            # Get the image and move to device
            image = sample["image"].unsqueeze(0).to(self.device)
            text = sample["text"]
            doc_name = sample["doc_name"]
            page_num = sample["page_num"]

            # Process image for the model
            pixel_values = self.processor(images=image, return_tensors="pt").pixel_values.to(self.device)

            # Generate prediction
            with torch.no_grad():
                generated_ids = model.generate(
                    pixel_values,
                    max_length=512,
                    num_beams=4,
                    early_stopping=True
                )

                prediction = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

            # Calculate metrics
            cer = character_error_rate(text, prediction)
            wer = word_error_rate(text, prediction)

            # Display results
            print(f"\nDocument: {doc_name}, Page: {page_num}")
            print(f"Reference (first 100 chars): {text[:100]}...")
            print(f"Prediction (first 100 chars): {prediction[:100]}...")
            print(f"CER: {cer:.4f}, WER: {wer:.4f}")
            print("-" * 50)

class CustomSeq2SeqTrainer(Seq2SeqTrainer):
    """
    Custom trainer with enhanced functionality for OCR fine-tuning.
    """
    def __init__(self, processor=None, *args, **kwargs):
        # Store processor for metrics calculation
        self.processor = processor
        super().__init__(*args, **kwargs)

    def compute_loss(self, model, inputs, return_outputs=False):
        """
        Custom compute_loss to handle robustness issues.
        """
        try:
            # Default loss computation
            return super().compute_loss(model, inputs, return_outputs)
        except Exception as e:
            logger.warning(f"Error in compute_loss: {str(e)}")
            # Create a dummy loss for robustness
            dummy_loss = torch.tensor(1.0, device=model.device, requires_grad=True)

            if return_outputs:
                return dummy_loss, None
            return dummy_loss

    def _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval):
        """
        Add more detailed metrics to standard logging.
        """
        # Call the parent method first
        metrics = super()._maybe_log_save_evaluate(
            tr_loss, model, trial, epoch, ignore_keys_for_eval
        )

        # Add custom metrics
        if metrics and self.state.epoch % 1.0 == 0:
            # Convert character error rate to accuracy for readability
            if "eval_cer" in metrics:
                metrics["eval_accuracy"] = 1.0 - metrics["eval_cer"]

            # Log learning rate
            metrics["learning_rate"] = self.optimizer.param_groups[0]["lr"]

        return metrics

    def save_model(self, output_dir=None, _internal_call=False):
        """
        Enhanced model saving with metadata.
        """
        # Call the parent method first
        super().save_model(output_dir, _internal_call)

        # Save processor alongside model
        if self.processor and output_dir:
            self.processor.save_pretrained(output_dir)
            logger.info(f"Saved processor to {output_dir}")

            # Save training metadata
            metadata = {
                "training_args": vars(self.args),
                "epochs_completed": self.state.epoch,
                "global_step": self.state.global_step,
                "metrics": self.state.log_history
            }

            import json
            try:
                with open(os.path.join(output_dir, "training_metadata.json"), "w") as f:
                    json.dump(metadata, f, indent=2, default=str)
            except Exception as e:
                logger.warning(f"Could not save training metadata: {str(e)}")

def create_trainer(model, processor, train_dataset, eval_dataset, output_dir,
                   num_epochs=10, batch_size=4, learning_rate=5e-5):
    """
    Create a trainer for OCR fine-tuning with all the right callbacks.

    Args:
        model: TrOCR model
        processor: TrOCR processor
        train_dataset: Training dataset
        eval_dataset: Evaluation dataset
        output_dir: Output directory for checkpoints
        num_epochs: Number of training epochs
        batch_size: Batch size per device
        learning_rate: Learning rate

    Returns:
        CustomSeq2SeqTrainer: Configured trainer
    """
    # Get training arguments
    training_args = get_training_args(
        output_dir=output_dir,
        batch_size=batch_size,
        num_epochs=num_epochs,
        learning_rate=learning_rate
    )

    # Create data collator
    def collate_fn(examples):
        # Extract pixel values and labels
        pixel_values = torch.stack([example["image"] for example in examples])

        # Tokenize texts
        tokenized = processor.tokenizer(
            [example["text"] for example in examples],
            padding="max_length",
            max_length=512,
            truncation=True,
            return_tensors="pt"
        )
        labels = tokenized.input_ids

        # Replace padding token id with -100 so it's ignored in loss
        labels[labels == processor.tokenizer.pad_token_id] = -100

        return {
            "pixel_values": pixel_values,
            "labels": labels,
            "decoder_input_ids": torch.zeros_like(labels)
        }

    # Create callbacks
    logging_callback = LoggingCallback()
    prediction_callback = PredictionDisplayCallback(
        eval_dataset=eval_dataset,
        processor=processor,
        device=model.device,
        display_freq=2
    )

    # Create trainer
    trainer = CustomSeq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=collate_fn,
        processor=processor,  # Pass processor to our custom trainer
        callbacks=[logging_callback, prediction_callback]
    )

    return trainer

# Test training setup
if __name__ == "__main__":
    # Create dummy training args
    test_args = get_training_args(output_dir="/tmp/test_trainer")

    # Print the training args
    print("Training Arguments:")
    for key, value in vars(test_args).items():
        if key.startswith("_"):
            continue
        print(f"  {key}: {value}")

    # Test logging callback
    callback = LoggingCallback()

    # Simulate training progress
    print("\nSimulating training progress...")
    for epoch in range(1, 6):
        metrics = {
            "train_loss": 10.0 / (epoch + 1),
            "eval_loss": 12.0 / (epoch + 1),
            "eval_cer": 0.9 - (epoch * 0.1),
            "eval_wer": 0.95 - (epoch * 0.05)
        }

        # Create dummy state and control
        state = TrainerState()
        state.epoch = epoch
        control = TrainerControl()

        # Call callback
        callback.on_epoch_end(test_args, state, control, metrics=metrics)

"""
Historical Document OCR Fine-Tuning Pipeline

This module implements a complete pipeline for fine-tuning TrOCR models
on historical document images with corresponding transcriptions.

The pipeline handles:
1. Finding and preprocessing document images
2. Loading and parsing transcriptions
3. Creating page-level aligned data
4. Training and evaluating the OCR model
5. Saving the fine-tuned model for inference

Author: Updated Version
Date: April 1, 2025
"""

import os
import logging
import torch
import time
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any, Set, Union
from dataclasses import dataclass
from pathlib import Path
import traceback
from functools import wraps

# Configure logging
def setup_logging(log_level: int = logging.INFO) -> logging.Logger:
    """
    Configure the logger for the OCR pipeline.

    Args:
        log_level: The logging level to use (default: INFO)

    Returns:
        The configured logger instance
    """
    logger = logging.getLogger("ocr_pipeline")

    if not logger.handlers:  # Only add handler if not already configured
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)

    logger.setLevel(log_level)
    return logger

logger = setup_logging()

# Type definitions for improved type safety
ImageData = Dict[str, Any]  # Processed image data
TranscriptionData = Dict[str, str]  # Document ID to text mapping
PageTranscription = Dict[Tuple[str, int], Dict[str, Any]]  # (doc_id, page_num) to data mapping
TrainValData = List[Dict[str, Any]]  # List of training/validation examples

@dataclass
class PipelineConfig:
    """Configuration parameters for the OCR pipeline."""
    base_path: Path
    max_pages_per_doc: int = 6
    num_epochs: int = 5
    batch_size: int = 2
    show_examples: bool = True
    model_name: str = "microsoft/trocr-base-printed"
    train_val_split: float = 0.8
    seed: int = 42
    mixed_precision: bool = True
    gradient_accumulation_steps: int = 4

# Decorator for timing and error handling
def pipeline_step(step_name: str):
    """
    Decorator for pipeline steps that adds:
    - Timing information
    - Error handling
    - Logging

    Args:
        step_name: Name of the pipeline step
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            logger.info(f"Starting step: {step_name}...")

            try:
                result = func(*args, **kwargs)
                elapsed = time.time() - start_time
                logger.info(f"Completed step: {step_name} in {elapsed:.2f} seconds")
                return result
            except Exception as e:
                logger.error(f"Error in step '{step_name}': {str(e)}")
                logger.error(traceback.format_exc())
                raise PipelineError(f"Failed at step '{step_name}'") from e

        return wrapper
    return decorator

class PipelineError(Exception):
    """Base exception for pipeline execution errors."""
    pass

class DatasetError(PipelineError):
    """Exception for dataset preparation errors."""
    pass

class ModelError(PipelineError):
    """Exception for model initialization and training errors."""
    pass

# Memory optimization utilities
def optimize_memory_usage(model: Any) -> Any:
    """
    Apply memory optimization techniques to the model.

    Args:
        model: The PyTorch model to optimize

    Returns:
        The optimized model
    """
    logger.info("Applying memory optimizations...")

    # Enable gradient checkpointing to save memory
    if hasattr(model, "gradient_checkpointing_enable"):
        model.gradient_checkpointing_enable()
        logger.info("Enabled gradient checkpointing")

    # Use bfloat16 mixed precision if available
    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():
        model.to(torch.bfloat16)
        logger.info("Using bfloat16 mixed precision")
    # Otherwise use float16 if on CUDA
    elif torch.cuda.is_available():
        model = model.half()  # Convert to float16
        logger.info("Using float16 mixed precision")

    return model

@pipeline_step("Finding Images")
def process_images(config: PipelineConfig) -> Tuple[List[ImageData], List[ImageData], Dict[str, str]]:
    """
    Find and preprocess all document images in the base directory.

    Args:
        config: Pipeline configuration

    Returns:
        Tuple containing:
        - List of processed images
        - List of binary images
        - Mapping of image IDs to file paths
    """
    # Call the existing find_all_images function from the original code
    processed_images, binary_images, image_mapping = find_all_images(
        base_path=str(config.base_path)
    )

    logger.info(f"Found {len(processed_images)} processed images and {len(binary_images)} binary images")
    return processed_images, binary_images, image_mapping

@pipeline_step("Loading Transcriptions")
def load_transcriptions(config: PipelineConfig) -> TranscriptionLoader:
    """
    Load and prepare document transcriptions.

    Args:
        config: Pipeline configuration

    Returns:
        Initialized transcription loader
    """
    transcriptions_path = config.base_path / 'transcriptions'
    page_transcription_dir = config.base_path / 'ocr_data' / 'page_transcriptions'

    # Create output directory if it doesn't exist
    page_transcription_dir.mkdir(parents=True, exist_ok=True)

    transcription_loader = TranscriptionLoader(
        transcription_dir=str(transcriptions_path),
        page_transcription_dir=str(page_transcription_dir)
    )

    transcription_loader.find_transcription_files()
    return transcription_loader

@pipeline_step("Creating Page Transcriptions")
def create_page_level_data(
    config: PipelineConfig,
    loader: TranscriptionLoader,
    image_mapping: Dict[str, str]
) -> PageTranscription:
    """
    Create page-level transcriptions by aligning images with text.

    Args:
        config: Pipeline configuration
        loader: Initialized transcription loader
        image_mapping: Mapping of image IDs to file paths

    Returns:
        Dictionary mapping (doc_id, page_num) to page data
    """
    page_transcriptions = create_page_transcriptions(
        loader=loader,
        image_mapping=image_mapping,
        max_pages_per_doc=config.max_pages_per_doc
    )

    if not page_transcriptions:
        raise DatasetError("No valid page transcriptions were created")

    logger.info(f"Created {len(page_transcriptions)} page transcriptions")

    # Show sample pages if requested
    if config.show_examples:
        logger.info("Displaying sample pages...")
        display_page_samples(page_transcriptions, n_samples=2)

    return page_transcriptions

@pipeline_step("Creating Dataset Split")
def create_dataset_split(
    config: PipelineConfig,
    page_transcriptions: PageTranscription
) -> Tuple[TrainValData, TrainValData]:
    """
    Create train/validation split with document-aware stratification.

    Args:
        config: Pipeline configuration
        page_transcriptions: Page-level transcription data

    Returns:
        Tuple of (train_data, val_data)
    """
    # Set random seed for reproducibility
    torch.manual_seed(config.seed)

    train_data, val_data = create_train_val_split(
        page_transcriptions,
        train_ratio=config.train_val_split,
        random_seed=config.seed
    )

    logger.info(f"Created dataset split - Train: {len(train_data)}, Validation: {len(val_data)}")

    if not train_data or not val_data:
        raise DatasetError(
            f"Invalid dataset split: Train={len(train_data)}, Val={len(val_data)}"
        )

    return train_data, val_data

@pipeline_step("Initializing Model")
def initialize_ocr_model(config: PipelineConfig) -> Tuple[Any, Any, str]:
    """
    Initialize and prepare the OCR model and processor.

    Args:
        config: Pipeline configuration

    Returns:
        Tuple of (model, processor, device)
    """
    model, processor, device = initialize_model(model_name=config.model_name)

    # Apply memory optimizations
    model = optimize_memory_usage(model)
    logger.info(f"Model initialized and moved to {device}")

    return model, processor, device

@pipeline_step("Creating Datasets")
def create_datasets(
    config: PipelineConfig,
    train_data: TrainValData,
    val_data: TrainValData,
    processor: Any
) -> Tuple[Any, Any]:
    """
    Create training and validation datasets.

    Args:
        config: Pipeline configuration
        train_data: Training data examples
        val_data: Validation data examples
        processor: TrOCR processor for tokenization

    Returns:
        Tuple of (train_dataset, val_dataset)
    """
    train_dataset = HistoricalDocumentDataset(
        train_data,
        processor,
        transform=get_train_transforms()
    )

    val_dataset = HistoricalDocumentDataset(
        val_data,
        processor,
        transform=get_val_transforms()
    )

    logger.info(f"Created datasets - Train: {len(train_dataset)}, Validation: {len(val_dataset)}")

    if len(train_dataset) == 0 or len(val_dataset) == 0:
        raise DatasetError(f"Empty dataset created: Train={len(train_dataset)}, Val={len(val_dataset)}")

    return train_dataset, val_dataset

@pipeline_step("Setting Up Trainer")
def setup_trainer(
    config: PipelineConfig,
    model: Any,
    processor: Any,
    train_dataset: Any,
    eval_dataset: Any
) -> Any:
    """
    Set up the HuggingFace Trainer for model fine-tuning.

    Args:
        config: Pipeline configuration
        model: Initialized model
        processor: TrOCR processor
        train_dataset: Training dataset
        eval_dataset: Validation dataset

    Returns:
        Configured trainer object
    """
    output_dir = config.base_path / 'ocr_data' / 'fine_tuned_model'
    output_dir.mkdir(parents=True, exist_ok=True)

    trainer = create_trainer(
        model=model,
        processor=processor,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        output_dir=str(output_dir),
        num_epochs=config.num_epochs,
        batch_size=config.batch_size,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        mixed_precision="bf16" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else "fp16"
    )

    # Calculate approximate time per epoch
    steps_per_epoch = len(train_dataset) // (config.batch_size * trainer.args.gradient_accumulation_steps)
    if steps_per_epoch > 0:
        logger.info(f"Training will run approximately {steps_per_epoch} steps per epoch")

    return trainer, output_dir

@pipeline_step("Training Model")
def train_model(trainer: Any) -> Dict[str, Any]:
    """
    Train the OCR model.

    Args:
        trainer: Configured trainer object

    Returns:
        Training metrics
    """
    try:
        train_result = trainer.train()

        logger.info("Training completed successfully!")
        logger.info(f"Training time: {train_result.metrics.get('train_runtime', 0):.2f} seconds")
        logger.info(f"Training samples per second: {train_result.metrics.get('train_samples_per_second', 0):.2f}")

        return train_result.metrics

    except Exception as e:
        logger.error(f"Error during training: {str(e)}")
        logger.error(traceback.format_exc())

        # Try to save the model anyway
        try:
            logger.info("Attempting to save the model despite errors...")
            trainer.save_model("error-recovery-model")
            raise ModelError("Training failed but model was saved for recovery") from e
        except:
            raise ModelError("Training failed and model could not be saved") from e

@pipeline_step("Saving Model")
def save_final_model(trainer: Any, processor: Any, output_dir: Path) -> str:
    """
    Save the final trained model and processor.

    Args:
        trainer: Trained model trainer
        processor: TrOCR processor
        output_dir: Output directory path

    Returns:
        Path to the saved model
    """
    final_model_path = output_dir / "final"
    trainer.save_model(str(final_model_path))
    processor.save_pretrained(str(final_model_path))

    logger.info(f"Model saved to: {final_model_path}")
    return str(final_model_path)

@pipeline_step("Evaluating Model")
def evaluate_model(trainer: Any) -> Dict[str, Any]:
    """
    Evaluate the final trained model.

    Args:
        trainer: Trained model trainer

    Returns:
        Evaluation metrics
    """
    try:
        eval_results = trainer.evaluate()
        logger.info(f"Final evaluation results:")
        for key, value in eval_results.items():
            logger.info(f"  {key}: {value}")
        return eval_results
    except Exception as e:
        logger.error(f"Error during evaluation: {str(e)}")
        return {"error": str(e)}

def run_pipeline(config: PipelineConfig) -> Dict[str, Any]:
    """
    Execute the complete OCR fine-tuning pipeline.

    Args:
        config: Pipeline configuration

    Returns:
        Dictionary of results including metrics and model paths
    """
    start_time = time.time()

    try:
        # Run the pipeline steps
        processed_images, binary_images, image_mapping = process_images(config)

        transcription_loader = load_transcriptions(config)

        page_transcriptions = create_page_level_data(
            config,
            transcription_loader,
            image_mapping
        )

        train_data, val_data = create_dataset_split(config, page_transcriptions)

        model, processor, device = initialize_ocr_model(config)

        train_dataset, val_dataset = create_datasets(
            config,
            train_data,
            val_data,
            processor
        )

        trainer, output_dir = setup_trainer(
            config,
            model,
            processor,
            train_dataset,
            val_dataset
        )

        train_metrics = train_model(trainer)

        final_model_path = save_final_model(trainer, processor, output_dir)

        eval_results = evaluate_model(trainer)

        # Calculate total runtime
        total_time = time.time() - start_time
        logger.info(f"Total runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")

        # Return results
        results = {
            "final_model_path": final_model_path,
            "evaluation_results": eval_results,
            "training_metrics": train_metrics,
            "training_data": {
                "num_train_samples": len(train_dataset),
                "num_val_samples": len(val_dataset),
                "documents": list(set(k[0] for k in page_transcriptions.keys()))
            },
            "runtime": {
                "total_seconds": total_time,
                "total_minutes": total_time / 60
            }
        }

        return results

    except PipelineError as e:
        total_time = time.time() - start_time
        logger.error(f"Pipeline failed after {total_time:.2f} seconds: {str(e)}")

        # Return error results
        return {
            "error": str(e),
            "runtime": {
                "total_seconds": total_time,
                "total_minutes": total_time / 60
            }
        }

def validate_prerequisites() -> bool:
    """
    Validate that all required functions are available.

    Returns:
        True if all prerequisites are available, False otherwise
    """
    required_functions = [
        'find_all_images', 'TranscriptionLoader', 'create_page_transcriptions',
        'display_page_samples', 'create_train_val_split', 'initialize_model',
        'HistoricalDocumentDataset', 'get_train_transforms',
        'get_val_transforms', 'create_trainer'
    ]

    missing_functions = []
    import sys
    current_module = sys.modules[__name__]

    for func_name in required_functions:
        # Check if function exists in globals or is imported
        if not (func_name in globals() or hasattr(current_module, func_name)):
            missing_functions.append(func_name)

    if missing_functions:
        logger.error(f"Missing required functions: {missing_functions}")
        logger.error("Make sure to import all required modules before running the pipeline")
        return False

    return True

def main(
    base_path: str = '/content',
    max_pages_per_doc: int = 6,
    num_epochs: int = 5,
    batch_size: int = 2,
    show_examples: bool = True,
    log_level: int = logging.INFO
) -> Dict[str, Any]:
    """
    Main function to run the complete fine-tuning pipeline.

    Args:
        base_path: Base directory for data
        max_pages_per_doc: Maximum pages to process per document
        num_epochs: Number of training epochs
        batch_size: Batch size per device
        show_examples: Whether to show example images and text
        log_level: Logging level

    Returns:
        Dict: Results including metrics and model paths
    """
    # Set up logging with specified level
    logger = setup_logging(log_level)

    # Check prerequisites before running
    if not validate_prerequisites():
        return {"error": "Missing required functions. See log for details."}

    # Create configuration
    config = PipelineConfig(
        base_path=Path(base_path),
        max_pages_per_doc=max_pages_per_doc,
        num_epochs=num_epochs,
        batch_size=batch_size,
        show_examples=show_examples
    )

    # Run the pipeline
    return run_pipeline(config)

# Example usage
if __name__ == "__main__":
    try:
        # This requires all previous cells to be executed in Colab
        if validate_prerequisites():
            logger.info("Running complete pipeline with:")
            logger.info(f"  - {6} pages per document")
            logger.info(f"  - {5} training epochs")
            logger.info(f"  - Batch size of {2}")

            # Set up different log levels for different modules
            logging.getLogger('transformers').setLevel(logging.WARNING)
            logging.getLogger('PIL').setLevel(logging.WARNING)

            results = main(
                base_path='/content',
                max_pages_per_doc=6,
                num_epochs=5,
                batch_size=2,
                show_examples=True
            )

            if "error" not in results:
                logger.info("\nTraining completed!")
                logger.info(f"Model saved to: {results['final_model_path']}")

                # Show evaluation results
                if 'evaluation_results' in results and 'error' not in results['evaluation_results']:
                    logger.info("\nFinal Evaluation Results:")
                    for key, value in results['evaluation_results'].items():
                        if isinstance(value, float):
                            logger.info(f"  {key}: {value:.4f}")
                        else:
                            logger.info(f"  {key}: {value}")
            else:
                logger.error(f"Pipeline execution failed: {results['error']}")
        else:
            logger.error("Prerequisites not met. Please run all required cells first.")

    except Exception as e:
        logger.error(f"Error in main execution: {e}")
        logger.error(traceback.format_exc())

# Usage example for users
"""
# Run the complete OCR fine-tuning pipeline
from ocr_pipeline import main

results = main(
    base_path='/content',            # Base directory for data
    max_pages_per_doc=10,            # Process up to 10 pages per document
    num_epochs=15,                   # Train for 15 epochs
    batch_size=4,                    # Use batch size of 4
    show_examples=True,              # Display example pages
)

# Check if pipeline succeeded
if "error" not in results:
    print(f"Training completed successfully!")
    print(f"Model saved to: {results['final_model_path']}")

    # Show evaluation results
    print("\nFinal Evaluation Results:")
    for key, value in results['evaluation_results'].items():
        if isinstance(value, float):
            print(f"  {key}: {value:.4f}")
        else:
            print(f"  {key}: {value}")

    # Now you can use the fine-tuned model to transcribe new images:
    from transformers import TrOCRProcessor, VisionEncoderDecoderModel
    from PIL import Image

    # Load your fine-tuned model
    processor = TrOCRProcessor.from_pretrained(results['final_model_path'])
    model = VisionEncoderDecoderModel.from_pretrained(results['final_model_path'])

    # Process a new image
    image = Image.open('path/to/new_image.jpg').convert('RGB')
    pixel_values = processor(image, return_tensors="pt").pixel_values

    # Generate text
    generated_ids = model.generate(pixel_values)
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

    print(f"OCR Text: {generated_text}")
else:
    print(f"Pipeline execution failed: {results['error']}")
"""

# Your existing imports
import os
import logging
import torch
import time
from datetime import datetime

# Import the improved pipeline
from ocr_pipeline import main as improved_main

# Keep your original main function
def main(base_path='/content', max_pages_per_doc=6, num_epochs=5, batch_size=2, show_examples=True):
    """Original main function (kept for backward compatibility)"""
    # Call the improved implementation
    return improved_main(
        base_path=base_path,
        max_pages_per_doc=max_pages_per_doc,
        num_epochs=num_epochs,
        batch_size=batch_size,
        show_examples=show_examples
    )

# Your original usage example remains unchanged
if __name__ == "__main__":
    results = main(
        max_pages_per_doc=6,
        num_epochs=5,
        batch_size=2
    )

import torch
import logging
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from PIL import Image
import matplotlib.pyplot as plt
import re
import os
from google.colab import files

logger = logging.getLogger(__name__)

class HistoricalDocumentOCR:
    """
    Class for OCR on historical documents using a fine-tuned TrOCR model.
    """
    def __init__(self, model_path, device=None):
        """
        Initialize the OCR model.

        Args:
            model_path (str): Path to the model
            device: Device to run the model on (auto-detect if None)
        """
        self.model_path = model_path

        # Determine device
        if device is None:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = device

        logger.info(f"Initializing HistoricalDocumentOCR with model from {model_path}")
        logger.info(f"Using device: {self.device}")

        # Load model and processor
        try:
            self.processor = TrOCRProcessor.from_pretrained(model_path)
            self.model = VisionEncoderDecoderModel.from_pretrained(model_path)
            self.model.to(self.device)
            logger.info("Model loaded successfully")
        except Exception as e:
            logger.error(f"Error loading model: {str(e)}")
            raise

    def process_image(self, image_path, num_beams=4, max_length=512):
        """
        Process an image and return the OCR text.

        Args:
            image_path (str): Path to the image
            num_beams (int): Number of beams for beam search
            max_length (int): Maximum length of generated text

        Returns:
            str: OCR text
        """
        try:
            # Load and preprocess image
            image = Image.open(image_path).convert("RGB")

            # Process with the model
            return self.process_pil_image(image, num_beams, max_length)

        except Exception as e:
            logger.error(f"Error processing image {image_path}: {str(e)}")
            return f"ERROR: Could not process image - {str(e)}"

    def process_pil_image(self, image, num_beams=4, max_length=512):
        """
        Process a PIL Image and return the OCR text.

        Args:
            image (PIL.Image): PIL Image object
            num_beams (int): Number of beams for beam search
            max_length (int): Maximum length of generated text

        Returns:
            str: OCR text
        """
        try:
            # Process image with processor
            pixel_values = self.processor(image, return_tensors="pt").pixel_values.to(self.device)

            # Generate text
            with torch.no_grad():
                generated_ids = self.model.generate(
                    pixel_values,
                    max_length=max_length,
                    num_beams=num_beams,
                    early_stopping=True,
                    length_penalty=2.0,
                    no_repeat_ngram_size=3
                )

            # Decode generated IDs to text
            generated_text = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

            return generated_text

        except Exception as e:
            logger.error(f"Error processing PIL image: {str(e)}")
            return f"ERROR: Could not process image - {str(e)}"

    def process_images_in_folder(self, folder_path, output_file=None, limit=None):
        """
        Process all images in a folder and optionally save results to a file.

        Args:
            folder_path (str): Path to the folder containing images
            output_file (str): Path to save results (None to skip saving)
            limit (int): Maximum number of images to process (None for all)

        Returns:
            dict: Dictionary mapping image paths to OCR text
        """
        # Find all images in the folder
        image_files = []
        for ext in ['.jpg', '.jpeg', '.png']:
            image_files.extend(glob.glob(os.path.join(folder_path, f"*{ext}")))

        if limit:
            image_files = image_files[:limit]

        logger.info(f"Found {len(image_files)} images to process")

        # Process each image
        results = {}
        for image_file in tqdm(image_files, desc="Processing images"):
            ocr_text = self.process_image(image_file)
            results[image_file] = ocr_text

        # Save results if requested
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                for image_file, ocr_text in results.items():
                    f.write(f"=== {os.path.basename(image_file)} ===\n")
                    f.write(ocr_text)
                    f.write("\n\n" + "=" * 50 + "\n\n")

            logger.info(f"Results saved to {output_file}")

        return results

    def postprocess_text(self, text):
        """
        Apply post-processing to improve OCR results for historical Spanish documents.

        Args:
            text (str): Raw OCR text

        Returns:
            str: Post-processed text
        """
        if not text:
            return text

        # 1. Fix common OCR errors in historical Spanish
        corrections = {
            # Character confusions
            'v': 'u',  # Often interchangeable in old texts
            'i': 'j',  # Often interchangeable in old texts
            'ſ': 's',  # Long s

            # Common word corrections for Spanish
            'dela': 'de la',
            'delos': 'de los',
            'enla': 'en la',
            'ala': 'a la',
            'delas': 'de las',
            'alos': 'a los',

            # Line break issues
            '-\n': '',  # Remove hyphenation at line breaks
        }

        # 2. Apply basic corrections
        for old, new in corrections.items():
            text = text.replace(old, new)

        # 3. Fix spacing around punctuation
        text = re.sub(r'\s+([.,;:!?])', r'\1', text)  # Remove space before punctuation
        text = re.sub(r'([.,;:!?])([^\s])', r'\1 \2', text)  # Add space after punctuation

        # 4. Normalize whitespace
        text = re.sub(r'\s+', ' ', text)  # Collapse multiple spaces
        text = re.sub(r'\n\s*\n', '\n\n', text)  # Normalize paragraph breaks

        return text.strip()

def upload_and_process_image():
    """
    Upload an image from the user's computer and process it with OCR.

    Returns:
        tuple: (image_path, ocr_text)
    """
    print("Please upload an image to process with OCR...")

    # Upload the image
    uploaded = files.upload()

    if not uploaded:
        print("No image uploaded.")
        return None, None

    # Get the first uploaded file
    image_path = next(iter(uploaded))
    full_path = os.path.join('/content', image_path)

    # Save the file
    with open(full_path, 'wb') as f:
        f.write(uploaded[image_path])

    print(f"Image saved to {full_path}")

    # Get the model path
    model_path = '/content/ocr_data/fine_tuned_model/final'
    if not os.path.exists(model_path):
        model_path = "microsoft/trocr-base-printed"  # Fallback to pre-trained model
        print(f"Fine-tuned model not found. Using pre-trained model: {model_path}")
    else:
        print(f"Using fine-tuned model from: {model_path}")

    # Initialize OCR
    ocr = HistoricalDocumentOCR(model_path)

    # Process the image
    print("Processing image with OCR...")
    ocr_text = ocr.process_image(full_path)

    # Post-process the text
    processed_text = ocr.postprocess_text(ocr_text)

    # Display the image and OCR text
    image = Image.open(full_path)
    plt.figure(figsize=(10, 10))
    plt.imshow(image)
    plt.axis('off')
    plt.title("Uploaded Image")
    plt.show()

    print("\nOCR Results:")
    print("-" * 50)
    print(processed_text)
    print("-" * 50)

    return full_path, processed_text

# Example of how to use the HistoricalDocumentOCR class
if __name__ == "__main__":
    # Check if the fine-tuned model exists
    model_path = '/content/ocr_data/fine_tuned_model/final'
    if not os.path.exists(model_path):
        print(f"Fine-tuned model not found at {model_path}")
        print("If you haven't completed the training, run the previous cells first.")
        model_path = "microsoft/trocr-base-printed"  # Fallback to pre-trained model
        print(f"Using pre-trained model: {model_path}")

    try:
        # Initialize the OCR
        ocr = HistoricalDocumentOCR(model_path)

        # Upload and process an image
        image_path, ocr_text = upload_and_process_image()

        # Provide an example of how to use the class programmatically
        print("\nExample code for programmatic usage:")
        print("""
        # Initialize OCR with your fine-tuned model
        ocr = HistoricalDocumentOCR('/content/ocr_data/fine_tuned_model/final')

        # Process a single image
        ocr_text = ocr.process_image('path/to/image.jpg')

        # Apply post-processing
        processed_text = ocr.postprocess_text(ocr_text)

        # Process all images in a folder
        results = ocr.process_images_in_folder(
            folder_path='path/to/images',
            output_file='output.txt'
        )
        """)

    except Exception as e:
        print(f"Error: {str(e)}")
        print("Make sure to run all the previous cells first.")

import os
import logging
import torch
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import re
from tqdm.notebook import tqdm

logger = logging.getLogger(__name__)

class OCRDebugger:
    """
    Debugging and validation tools for OCR pipeline.

    This class provides methods to validate data integrity, visualize page
    detection accuracy, and diagnose common issues with transcription mapping.
    """
    def __init__(self, base_path='/content'):
        """
        Initialize the debugger.

        Args:
            base_path: Base directory for all data
        """
        self.base_path = base_path
        self.output_base_path = os.path.join(base_path, 'ocr_data')
        self.transcriptions_path = os.path.join(base_path, 'transcriptions')

    def validate_directory_structure(self):
        """
        Validate that all required directories exist with proper permissions.

        Returns:
            bool: True if structure is valid, False otherwise
        """
        required_dirs = [
            self.output_base_path,
            os.path.join(self.output_base_path, "images"),
            os.path.join(self.output_base_path, "processed_images"),
            os.path.join(self.output_base_path, "binary_images"),
            os.path.join(self.output_base_path, "page_transcriptions"),
            self.transcriptions_path
        ]

        valid = True
        for directory in required_dirs:
            if not os.path.exists(directory):
                logger.error(f"Missing directory: {directory}")
                valid = False
            elif not os.access(directory, os.R_OK | os.W_OK):
                logger.error(f"Insufficient permissions for: {directory}")
                valid = False

        if valid:
            logger.info("✅ Directory structure validation passed")
        else:
            logger.error("❌ Directory structure validation failed")

        return valid

    def validate_document_subdirectories(self):
        """
        Validate that each document has proper subdirectories.

        Returns:
            bool: True if valid, False otherwise
        """
        # Define expected document subdirectories
        expected_subdirs = [
            "Buendia - Instruccion-1",
            "Constituciones sinodales Calahorra 1602-2",
            "Ezcaray - Vozes-3",
            "Mendo - Principe perfecto-4",
            "Paredes - Reglas generales-5",
            "PORCONES.228.35 1636-6"
        ]

        # Check in processed_images and binary_images directories
        processed_dir = os.path.join(self.output_base_path, "processed_images")
        binary_dir = os.path.join(self.output_base_path, "binary_images")

        valid = True

        # Check processed_images
        if os.path.exists(processed_dir):
            for subdir in expected_subdirs:
                full_path = os.path.join(processed_dir, subdir)
                if not os.path.exists(full_path):
                    logger.error(f"Missing subdirectory in processed_images: {subdir}")
                    valid = False
        else:
            logger.error(f"Missing directory: {processed_dir}")
            valid = False

        # Check binary_images
        if os.path.exists(binary_dir):
            for subdir in expected_subdirs:
                full_path = os.path.join(binary_dir, subdir)
                if not os.path.exists(full_path):
                    logger.error(f"Missing subdirectory in binary_images: {subdir}")
                    valid = False
        else:
            logger.error(f"Missing directory: {binary_dir}")
            valid = False

        if valid:
            logger.info("✅ Document subdirectories validation passed")
        else:
            logger.error("❌ Document subdirectories validation failed")

        return valid

    def validate_transcription_files(self):
        """
        Validate that transcription files exist and are readable.

        Returns:
            bool: True if valid, False otherwise
        """
        valid = True

        # Check if transcriptions directory exists
        if not os.path.exists(self.transcriptions_path):
            logger.error(f"Missing transcriptions directory: {self.transcriptions_path}")
            return False

        # Look for transcription files
        transcription_files = []
        for ext in ['.txt', '.docx']:
            transcription_files.extend(
                [f for f in os.listdir(self.transcriptions_path) if f.endswith(ext)]
            )

        if not transcription_files:
            logger.error("No transcription files found")
            valid = False
        else:
            logger.info(f"Found {len(transcription_files)} transcription files")

            # Validate each file is readable
            for filename in transcription_files:
                file_path = os.path.join(self.transcriptions_path, filename)
                try:
                    if filename.endswith('.txt'):
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read(100)  # Read just a bit to check
                    elif filename.endswith('.docx'):
                        try:
                            import docx
                            doc = docx.Document(file_path)
                            has_content = len(doc.paragraphs) > 0
                            if not has_content:
                                logger.warning(f"DOCX file appears to be empty: {filename}")
                        except ImportError:
                            logger.warning("python-docx not installed, cannot verify DOCX content")
                            # Try to install it
                            try:
                                import subprocess
                                logger.info("Installing python-docx...")
                                subprocess.check_call([sys.executable, "-m", "pip", "install", "python-docx"])
                                import docx
                                doc = docx.Document(file_path)
                                has_content = len(doc.paragraphs) > 0
                                if not has_content:
                                    logger.warning(f"DOCX file appears to be empty: {filename}")
                            except Exception as e:
                                logger.error(f"Failed to install python-docx: {e}")
                                valid = False
                        except Exception as e:
                            logger.error(f"Error reading DOCX file {filename}: {e}")
                            valid = False
                except Exception as e:
                    logger.error(f"Error reading file {filename}: {e}")
                    valid = False

        if valid:
            logger.info("✅ Transcription files validation passed")
        else:
            logger.error("❌ Transcription files validation failed")

        return valid

    def validate_image_files(self):
        """
        Validate that document images exist and are readable.

        Returns:
            bool: True if valid, False otherwise
        """
        valid = True
        processed_dir = os.path.join(self.output_base_path, "processed_images")

        if not os.path.exists(processed_dir):
            logger.error(f"Missing processed images directory: {processed_dir}")
            return False

        # Scan for images in all subdirectories
        image_count = 0
        for root, _, files in os.walk(processed_dir):
            for file in files:
                if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                    image_path = os.path.join(root, file)
                    image_count += 1

                    # Try to open the image to verify it's valid
                    try:
                        with Image.open(image_path) as img:
                            # Check if image is valid by accessing its size
                            width, height = img.size
                            if width <= 0 or height <= 0:
                                logger.error(f"Invalid image dimensions in {image_path}: {width}x{height}")
                                valid = False
                    except Exception as e:
                        logger.error(f"Error opening image {image_path}: {e}")
                        valid = False

        if image_count == 0:
            logger.error("No image files found")
            valid = False
        else:
            logger.info(f"Found {image_count} image files")

        if valid:
            logger.info("✅ Image files validation passed")
        else:
            logger.error("❌ Image files validation failed")

        return valid

    def diagnose_page_markers(self, show_examples=True):
        """
        Analyze transcription files to find and diagnose page markers.

        Args:
            show_examples: Whether to show matching examples

        Returns:
            dict: Page marker statistics
        """
        logger.info("Analyzing transcription files for page markers...")

        # Check if transcriptions directory exists
        if not os.path.exists(self.transcriptions_path):
            logger.error(f"Missing transcriptions directory: {self.transcriptions_path}")
            return {}

        # Get all text files in the transcriptions directory
        txt_files = [f for f in os.listdir(self.transcriptions_path) if f.endswith('.txt')]

        if not txt_files:
            logger.warning("No text transcription files found")
            return {}

        # Define page marker patterns to search for
        marker_patterns = [
            (r'PDF\s+p(\d+)', "PDF pX"),
            (r'Page\s+(\d+)', "Page X"),
            (r'\*\*PDF\s+p(\d+)\*\*', "**PDF pX**"),
            (r'\*\*Page\s+(\d+)\*\*', "**Page X**"),
            (r'p\.?\s*(\d+)', "p. X"),
        ]

        # Collect statistics
        stats = {
            "total_files": len(txt_files),
            "files_with_markers": 0,
            "marker_counts": {},
            "examples": {}
        }

        for pattern_desc in marker_patterns:
            stats["marker_counts"][pattern_desc[1]] = 0
            stats["examples"][pattern_desc[1]] = []

        # Analyze each file
        for filename in txt_files:
            file_path = os.path.join(self.transcriptions_path, filename)

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Check for each marker pattern
                file_has_markers = False

                for pattern, desc in marker_patterns:
                    matches = list(re.finditer(pattern, content, re.IGNORECASE))

                    if matches:
                        file_has_markers = True
                        stats["marker_counts"][desc] += len(matches)

                        # Store a few examples
                        if show_examples and len(stats["examples"][desc]) < 3:
                            for match in matches[:3]:
                                # Get context (20 chars before and after)
                                start = max(0, match.start() - 20)
                                end = min(len(content), match.end() + 20)
                                context = content[start:end]

                                stats["examples"][desc].append({
                                    "file": filename,
                                    "page": match.group(1),
                                    "context": context,
                                    "position": match.start()
                                })

                if file_has_markers:
                    stats["files_with_markers"] += 1

            except Exception as e:
                logger.error(f"Error analyzing file {filename}: {e}")

        # Print summary
        logger.info(f"Found page markers in {stats['files_with_markers']} out of {stats['total_files']} files")

        for desc, count in stats["marker_counts"].items():
            logger.info(f"  {desc}: {count} occurrences")

        # Show examples if requested
        if show_examples:
            logger.info("Example page markers:")

            for desc, examples in stats["examples"].items():
                if examples:
                    logger.info(f"  Pattern: {desc}")

                    for i, example in enumerate(examples):
                        logger.info(f"    Example {i+1} from {example['file']} (page {example['page']}):")
                        logger.info(f"      \"...{example['context']}...\"")

        return stats

    def visualize_page_detection(self, page_transcriptions, num_samples=2):
        """
        Visualize document pages with their detected transcriptions.

        Args:
            page_transcriptions: Dictionary of page transcriptions
            num_samples: Number of random samples to display

        Returns:
            bool: True if visualization was successful
        """
        if not page_transcriptions:
            logger.error("No page transcriptions provided")
            return False

        # Select random samples
        keys = list(page_transcriptions.keys())
        if not keys:
            logger.error("Page transcriptions dictionary is empty")
            return False

        # Group by document for better visualization
        docs = {}
        for key in keys:
            doc_name, page_num = key
            if doc_name not in docs:
                docs[doc_name] = []
            docs[doc_name].append(page_num)

        # Sort page numbers
        for doc_name in docs:
            docs[doc_name].sort()

        # Display samples from each document
        for doc_name, page_nums in docs.items():
            # Show header
            logger.info(f"\nDocument: {doc_name}")
            logger.info(f"Available pages: {page_nums}")

            # Select a couple of pages to show
            pages_to_show = []
            if len(page_nums) <= num_samples:
                pages_to_show = page_nums
            else:
                # Try to get first, middle and last page
                if num_samples >= 3:
                    pages_to_show = [page_nums[0], page_nums[len(page_nums)//2], page_nums[-1]]
                else:
                    pages_to_show = np.random.choice(page_nums, min(num_samples, len(page_nums)), replace=False)

            # Display each selected page
            for page_num in pages_to_show:
                key = (doc_name, page_num)
                data = page_transcriptions[key]

                # Display image
                img_path = data.get('processed_image')
                if img_path and os.path.exists(img_path):
                    plt.figure(figsize=(8, 10))
                    img = Image.open(img_path)
                    plt.imshow(img)
                    plt.title(f"{doc_name} - Page {page_num}")
                    plt.axis('off')
                    plt.show()
                else:
                    logger.warning(f"Image not found for {doc_name}, page {page_num}")

                # Display transcription excerpt
                text = data.get('text', '')
                source = data.get('source', 'unknown')
                logger.info(f"Page {page_num} - Source: {source}")
                logger.info(f"Transcription excerpt (first 300 chars):")
                logger.info(f"{text[:300]}...")
                logger.info(f"Total length: {len(text)} characters")
                logger.info("-" * 50)

        return True

    def diagnose_fine_tuning_issues(self, output_dir=None):
        """
        Diagnose common issues with model fine-tuning.

        Args:
            output_dir: Directory containing fine-tuning outputs

        Returns:
            dict: Diagnosis results
        """
        if output_dir is None:
            output_dir = os.path.join(self.output_base_path, 'fine_tuned_model')

        results = {
            "gpu_available": torch.cuda.is_available(),
            "gpu_info": {},
            "warnings": [],
            "checkpoint_exists": False,
            "final_model_exists": False
        }

        # GPU diagnosis
        if results["gpu_available"]:
            try:
                results["gpu_info"] = {
                    "name": torch.cuda.get_device_name(0),
                    "memory_total": torch.cuda.get_device_properties(0).total_memory / (1024**3),  # GB
                    "memory_allocated": torch.cuda.memory_allocated(0) / (1024**3),  # GB
                }
                logger.info(f"GPU: {results['gpu_info']['name']}")
                logger.info(f"Memory: {results['gpu_info']['memory_allocated']:.2f}GB allocated / {results['gpu_info']['memory_total']:.2f}GB total")

                if results["gpu_info"]["memory_total"] < 8:
                    results["warnings"].append("Low GPU memory detected. Consider reducing batch size and model parameters.")
            except Exception as e:
                logger.error(f"Error getting GPU info: {e}")
                results["warnings"].append(f"Error accessing GPU info: {str(e)}")
        else:
            logger.warning("No GPU detected - training will be slow!")
            results["warnings"].append("No GPU available for training. Fine-tuning will be extremely slow.")

        # Check for model checkpoints
        if os.path.exists(output_dir):
            # Look for checkpoint directories
            checkpoint_dirs = [d for d in os.listdir(output_dir) if d.startswith("checkpoint-")]
            results["checkpoint_exists"] = len(checkpoint_dirs) > 0
            results["num_checkpoints"] = len(checkpoint_dirs)

            # Check if final model exists
            final_model_dir = os.path.join(output_dir, "final")
            results["final_model_exists"] = os.path.exists(final_model_dir)

            # Verify model files exist
            if results["final_model_exists"]:
                config_file = os.path.join(final_model_dir, "config.json")
                model_file = os.path.join(final_model_dir, "pytorch_model.bin")

                results["config_exists"] = os.path.exists(config_file)
                results["model_file_exists"] = os.path.exists(model_file)

                if not results["config_exists"]:
                    results["warnings"].append("Model config.json is missing")
                if not results["model_file_exists"]:
                    results["warnings"].append("Model weights file is missing")
        else:
            logger.warning(f"Output directory not found: {output_dir}")
            results["warnings"].append(f"Fine-tuning output directory not found: {output_dir}")

        # Report findings
        if results["warnings"]:
            logger.warning("Diagnosis found potential issues:")
            for warning in results["warnings"]:
                logger.warning(f"  - {warning}")
        else:
            logger.info("No critical issues found in fine-tuning setup")

        return results

    def run_all_validations(self):
        """
        Run all validation checks.

        Returns:
            dict: Validation results
        """
        logger.info("Running all validation checks...")

        results = {
            "directory_structure": self.validate_directory_structure(),
            "document_subdirectories": self.validate_document_subdirectories(),
            "transcription_files": self.validate_transcription_files(),
            "image_files": self.validate_image_files(),
            "fine_tuning": self.diagnose_fine_tuning_issues()
        }

        # Analyze page markers
        results["page_markers"] = self.diagnose_page_markers(show_examples=True)

        # Overall success
        success = all([
            results["directory_structure"],
            results["document_subdirectories"],
            results["transcription_files"],
            results["image_files"]
        ])

        if success:
            logger.info("✅ All validation checks passed")
        else:
            logger.error("❌ Some validation checks failed")
            # List the failed checks
            failed = []
            for check, result in results.items():
                if isinstance(result, bool) and not result:
                    failed.append(check)
            logger.error(f"Failed checks: {', '.join(failed)}")

        return results

# Test the debugging tools
if __name__ == "__main__":
    debugger = OCRDebugger()
    results = debugger.run_all_validations()

    print("\nValidation results summary:")
    for check, result in results.items():
        if isinstance(result, bool):
            status = "✅ Passed" if result else "❌ Failed"
            print(f"{check}: {status}")

import os
import logging
import time
import torch
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import re
from tqdm.notebook import tqdm
from IPython.display import display, HTML
import pandas as pd
import sys
import importlib
import subprocess

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class HistoricalSpanishOCRPipeline:
    """
    Complete end-to-end pipeline for OCR on historical Spanish documents.

    This class orchestrates the entire process from data preprocessing
    through model training and evaluation to practical text extraction.
    """
    def __init__(self, base_path='/content'):
        """
        Initialize the pipeline.

        Args:
            base_path: Base directory for all data
        """
        self.base_path = base_path
        self.output_base_path = os.path.join(base_path, 'ocr_data')
        self.transcriptions_path = os.path.join(base_path, 'transcriptions')
        self.fine_tuned_model_dir = os.path.join(self.output_base_path, 'fine_tuned_model')
        self.page_transcription_dir = os.path.join(self.output_base_path, 'page_transcriptions')

        # Import necessary modules and handle dependencies
        self._import_dependencies()

    def _import_dependencies(self):
        """
        Import necessary dependencies and install missing packages.
        """
        # Install missing packages if needed
        required_packages = [
            'transformers',
            'datasets',
            'torch',
            'torchvision',
            'tqdm',
            'pandas',
            'matplotlib',
            'python-docx'
        ]

        missing_packages = []
        for package in required_packages:
            try:
                # Try to import
                if package == 'transformers':
                    import transformers
                elif package == 'datasets':
                    import datasets
                elif package == 'torch':
                    import torch
                elif package == 'torchvision':
                    import torchvision
                elif package == 'tqdm':
                    import tqdm
                elif package == 'pandas':
                    import pandas
                elif package == 'matplotlib':
                    import matplotlib
                elif package == 'python-docx':
                    import docx
            except ImportError:
                missing_packages.append(package)

        # Install missing packages
        if missing_packages:
            logger.info(f"Installing missing packages: {', '.join(missing_packages)}")
            for package in missing_packages:
                try:
                    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package])
                    logger.info(f"Installed {package}")
                except subprocess.CalledProcessError:
                    logger.error(f"Failed to install {package}")

    def validate_environment(self):
        """
        Validate the environment and dependencies.

        Returns:
            bool: True if validation passes, False otherwise
        """
        # Create debugger instance
        from OCRDebugger import OCRDebugger
        debugger = OCRDebugger(self.base_path)

        # Run all validations
        validation_results = debugger.run_all_validations()

        # Check if any validation failed
        for key, value in validation_results.items():
            if isinstance(value, bool) and not value:
                return False

        return True

    def _create_improved_page_transcriptions(self, image_mapping, max_pages_per_doc=6):
        """
        Create page-level transcriptions with improved page detection and content splitting.

        This implementation addresses issues from the previous version by:
        1. Adding more robust page marker detection patterns
        2. Handling special cases for different document formats
        3. Providing better fallback mechanisms
        4. Adding detailed logging

        Args:
            image_mapping: Dictionary mapping (doc_name, page_num) to image paths
            max_pages_per_doc: Maximum pages to process per document

        Returns:
            dict: Dictionary mapping (doc_name, page_num) to page data
        """
        from TranscriptionLoader import TranscriptionLoader

        # Create a new transcription loader
        loader = TranscriptionLoader(
            self.transcriptions_path,
            self.page_transcription_dir
        )

        # Find all transcription files
        loader.find_transcription_files()

        # Check if we found any transcription files
        if not loader.available_transcriptions:
            logger.error("No transcription files found. Cannot create page transcriptions.")
            return {}

        page_transcriptions = {}
        doc_names = sorted(list(set(doc_name for doc_name, _ in image_mapping.keys())))
        logger.info(f"Processing transcriptions for {len(doc_names)} documents")

        for doc_name in doc_names:
            # Get full transcription for this document
            full_transcription = loader.read_transcription(doc_name)
            if not full_transcription:
                logger.warning(f"No transcription found for {doc_name}, skipping")
                continue

            # Find all pages for this document
            doc_pages = sorted([
                page_num for d, page_num in image_mapping.keys()
                if d == doc_name and page_num <= max_pages_per_doc
            ])

            if not doc_pages:
                logger.warning(f"No pages found for {doc_name}")
                continue

            logger.info(f"Processing {doc_name}, pages: {doc_pages}")

            # Multiple strategies for page detection

            # STRATEGY 1: Look for explicit page markers with multiple patterns
            all_markers = []

            # Pattern definitions - (name, regex, is_exact)
            patterns = [
                ("PDF pX", r'PDF\s+p(\d+)', True),
                ("Page X", r'Page\s+(\d+)', True),
                ("**PDF pX**", r'\*\*PDF\s+p(\d+)\*\*', True),
                ("**Page X**", r'\*\*Page\s+(\d+)\*\*', True),
                ("p. X", r'p\.?\s*(\d+)', False),  # Less exact
                # Additional patterns specific to historical docs
                ("PDF page", r'PDF\s+page\s+(\d+)', True),
                ("Page header", r'^[\s\*]*Page\s+(\d+)[\s\*]*$', True),  # At start of line
            ]

            # Find all pattern matches
            for pattern_name, regex, is_exact in patterns:
                matches = list(re.finditer(regex, full_transcription, re.IGNORECASE | re.MULTILINE))

                for match in matches:
                    try:
                        page_num = int(match.group(1))
                        # Only consider if page number is in our target range
                        if page_num in doc_pages:
                            all_markers.append({
                                'page_num': page_num,
                                'start_idx': match.start(),
                                'end_idx': match.end(),
                                'pattern': pattern_name,
                                'is_exact': is_exact
                            })
                    except ValueError:
                        continue

            # Sort markers by position in text
            all_markers.sort(key=lambda x: x['start_idx'])

            # Group markers by page number (in case of duplicates)
            page_marker_groups = {}
            for marker in all_markers:
                page_num = marker['page_num']
                if page_num not in page_marker_groups:
                    page_marker_groups[page_num] = []
                page_marker_groups[page_num].append(marker)

            # For each page number, select the best marker
            final_markers = []
            for page_num, markers in page_marker_groups.items():
                # Prefer exact markers if available
                exact_markers = [m for m in markers if m['is_exact']]
                if exact_markers:
                    # Use the first exact marker for this page
                    final_markers.append(exact_markers[0])
                else:
                    # Otherwise use the first marker
                    final_markers.append(markers[0])

            # Sort final markers by position
            final_markers.sort(key=lambda x: x['start_idx'])

            if final_markers:
                logger.info(f"Found {len(final_markers)} page markers in {doc_name}")

                # Process each marker to extract page text
                for i, marker in enumerate(final_markers):
                    page_num = marker['page_num']
                    start_pos = marker['end_idx']

                    # Determine end position (either next marker or end of text)
                    if i < len(final_markers) - 1:
                        end_pos = final_markers[i+1]['start_idx']
                    else:
                        end_pos = len(full_transcription)

                    # Extract text between markers
                    text = full_transcription[start_pos:end_pos].strip()

                    # Only include pages that have corresponding images
                    key = (doc_name, page_num)
                    if key in image_mapping and 'processed' in image_mapping[key]:
                        page_transcriptions[key] = {
                            'text': text,
                            'processed_image': image_mapping[key].get('processed'),
                            'binary_image': image_mapping[key].get('binary'),
                            'source': 'page_markers'
                        }
                        logger.info(f"Created transcription for {doc_name}, page {page_num} using markers")
            else:
                logger.warning(f"No page markers found in {doc_name}. Using alternative splitting.")

                # STRATEGY 2: Use paragraph-based splitting if no markers found

                # Process the text to normalize paragraph breaks
                paragraphs = re.split(r'\n\s*\n', full_transcription)
                paragraphs = [p.strip() for p in paragraphs if p.strip()]

                # Calculate paragraphs per page based on available pages
                num_paragraphs = len(paragraphs)
                num_pages = len(doc_pages)

                if num_pages > 0 and num_paragraphs > 0:
                    paragraphs_per_page = max(1, num_paragraphs // num_pages)

                    for i, page_num in enumerate(doc_pages):
                        # Calculate start and end paragraph indices
                        start_idx = i * paragraphs_per_page
                        end_idx = min((i + 1) * paragraphs_per_page, num_paragraphs)

                        # Skip if out of range
                        if start_idx >= num_paragraphs:
                            continue

                        # Join paragraphs for this page
                        text = "\n\n".join(paragraphs[start_idx:end_idx])

                        # Only include if we have a corresponding image
                        key = (doc_name, page_num)
                        if key in image_mapping and 'processed' in image_mapping[key]:
                            page_transcriptions[key] = {
                                'text': text,
                                'processed_image': image_mapping[key].get('processed'),
                                'binary_image': image_mapping[key].get('binary'),
                                'source': 'paragraph_split'
                            }
                            logger.info(f"Created transcription for {doc_name}, page {page_num} using paragraph split")

                # If we still don't have transcriptions for some pages, use a length-based split
                missing_pages = [p for p in doc_pages if (doc_name, p) not in page_transcriptions]

                if missing_pages:
                    logger.warning(f"Missing transcriptions for pages {missing_pages} in {doc_name}. Using length-based split.")

                    # Split the full text by length
                    text_length = len(full_transcription)
                    chars_per_page = text_length // len(doc_pages) if doc_pages else text_length

                    for page_num in missing_pages:
                        # Calculate position in text based on page number
                        page_idx = doc_pages.index(page_num)
                        start_pos = page_idx * chars_per_page
                        end_pos = min(start_pos + chars_per_page, text_length)

                        # Extract text for this page
                        text = full_transcription[start_pos:end_pos].strip()

                        # Only include if we have a corresponding image
                        key = (doc_name, page_num)
                        if key in image_mapping and 'processed' in image_mapping[key]:
                            page_transcriptions[key] = {
                                'text': text,
                                'processed_image': image_mapping[key].get('processed'),
                                'binary_image': image_mapping[key].get('binary'),
                                'source': 'length_split'
                            }
                            logger.info(f"Created transcription for {doc_name}, page {page_num} using length-based split")

        # Log overall statistics
        total_pages = sum(1 for k, v in image_mapping.items()
                        if 'processed' in v and k[1] <= max_pages_per_doc)
        covered_pages = len(page_transcriptions)

        if total_pages > 0:
            coverage_pct = (covered_pages / total_pages * 100)
            logger.info(f"Created {covered_pages} page transcriptions out of {total_pages} pages ({coverage_pct:.1f}% coverage)")
        else:
            logger.warning("No pages were found in the image mapping")

        return page_transcriptions

    def run_pipeline(self, max_pages_per_doc=6, num_epochs=10, batch_size=2, show_examples=True):
        """
        Run the complete OCR pipeline.

        Args:
            max_pages_per_doc: Maximum pages to process per document
            num_epochs: Number of training epochs
            batch_size: Batch size for training
            show_examples: Whether to show example images and text

        Returns:
            dict: Results including model paths and metrics
        """
        # Start timing
        start_time = time.time()

        # Step 1: Find images
        logger.info("Step 1: Finding images...")
        from find_all_images import find_all_images
        processed_images, binary_images, image_mapping = find_all_images(self.base_path)
        logger.info(f"Found {len(processed_images)} processed images and {len(binary_images)} binary images")

        # Validate we have images
        if not processed_images:
            logger.error("No processed images found. Cannot continue.")
            return {"error": "No processed images found"}

        # Step 2: Create improved page transcriptions
        logger.info("Step 2: Creating page transcriptions...")
        page_transcriptions = self._create_improved_page_transcriptions(
            image_mapping,
            max_pages_per_doc=max_pages_per_doc
        )

        # Validate we have transcriptions
        if not page_transcriptions:
            logger.error("No page transcriptions created. Cannot continue.")
            return {"error": "No page transcriptions created"}

        # Display examples if requested
        if show_examples:
            from OCRDebugger import OCRDebugger
            debugger = OCRDebugger(self.base_path)
            debugger.visualize_page_detection(page_transcriptions, num_samples=2)

        # Step 3: Create train/validation split
        logger.info("Step 3: Creating train-validation split...")
        from create_train_val_split import create_train_val_split
        train_data, val_data = create_train_val_split(page_transcriptions)

        # Step 4: Initialize model
        logger.info("Step 4: Initializing model...")
        from initialize_model import initialize_model, optimize_memory_usage
        model, processor, device = initialize_model(model_name="microsoft/trocr-base-printed")

        # Apply memory optimizations
        model = optimize_memory_usage(model)

        # Step 5: Create datasets
        logger.info("Step 5: Creating datasets...")
        from HistoricalDocumentDataset import HistoricalDocumentDataset
        from get_train_transforms import get_train_transforms, get_val_transforms

        train_dataset = HistoricalDocumentDataset(
            train_data,
            processor,
            transform=get_train_transforms()
        )

        val_dataset = HistoricalDocumentDataset(
            val_data,
            processor,
            transform=get_val_transforms()
        )

        logger.info(f"Created datasets - Train: {len(train_dataset)}, Validation: {len(val_dataset)}")

        # Step 6: Set up trainer
        logger.info("Step 6: Creating trainer...")
        from create_trainer import create_trainer
        trainer = create_trainer(
            model=model,
            processor=processor,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            output_dir=self.fine_tuned_model_dir,
            num_epochs=num_epochs,
            batch_size=batch_size
        )

        # Step 7: Train model
        logger.info(f"Step 7: Training model for {num_epochs} epochs...")

        try:
            train_result = trainer.train()
            logger.info("Training completed successfully!")

            # Log training metrics
            train_metrics = {
                "runtime": train_result.metrics.get("train_runtime", 0),
                "samples_per_second": train_result.metrics.get("train_samples_per_second", 0),
                "steps_per_second": train_result.metrics.get("train_steps_per_second", 0)
            }

            logger.info(f"Training time: {train_metrics['runtime']:.2f} seconds")
            logger.info(f"Samples per second: {train_metrics['samples_per_second']:.2f}")

        except Exception as e:
            logger.error(f"Error during training: {str(e)}")
            # Try to save model anyway
            try:
                logger.info("Attempting to save model despite errors...")
                trainer.save_model(os.path.join(self.fine_tuned_model_dir, "final-error-recovery"))
                processor.save_pretrained(os.path.join(self.fine_tuned_model_dir, "final-error-recovery"))
            except Exception as save_error:
                logger.error(f"Error saving model: {str(save_error)}")

            return {
                "error": str(e),
                "page_transcriptions": len(page_transcriptions),
                "train_dataset": len(train_dataset),
                "val_dataset": len(val_dataset),
                "runtime": time.time() - start_time
            }

        # Step 8: Save final model
        logger.info("Step 8: Saving final model...")
        final_model_path = os.path.join(self.fine_tuned_model_dir, "final")
        trainer.save_model(final_model_path)
        processor.save_pretrained(final_model_path)

        # Step 9: Evaluate final model
        logger.info("Step 9: Evaluating final model...")
        eval_results = trainer.evaluate()

        # Convert to standard metrics format
        metrics = {
            "cer": eval_results.get("eval_cer", float("nan")),
            "wer": eval_results.get("eval_wer", float("nan")),
            "loss": eval_results.get("eval_loss", float("nan"))
        }

        # Log metrics
        logger.info("Final evaluation metrics:")
        for key, value in metrics.items():
            logger.info(f"  {key}: {value:.4f}")

        # Step 10: Create demonstration example
        logger.info("Step 10: Creating demonstration example...")

        # Create a demo pipeline function for use in another cell
        demo_code = """
# Load and use the fine-tuned model
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from PIL import Image

# Path to your fine-tuned model
model_path = '{}'

# Load the model and processor
processor = TrOCRProcessor.from_pretrained(model_path)
model = VisionEncoderDecoderModel.from_pretrained(model_path)

# Load an image
image = Image.open('path_to_your_image.jpg').convert("RGB")

# Process the image with the model
pixel_values = processor(images=image, return_tensors="pt").pixel_values

# Generate text
generated_ids = model.generate(pixel_values)
text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

print(f"OCR Result: {text}")
""".format(final_model_path)

        # Return results
        total_time = time.time() - start_time

        return {
            "final_model_path": final_model_path,
            "metrics": metrics,
            "training_stats": {
                "num_train_samples": len(train_dataset),
                "num_val_samples": len(val_dataset),
                "num_epochs": num_epochs,
                "batch_size": batch_size,
            },
            "runtime": {
                "total_seconds": total_time,
                "total_minutes": total_time / 60
            },
            "demo_code": demo_code
        }

# Main execution code
if __name__ == "__main__":
    # Check for missing functions
    missing_modules = []
    required_modules = [
        'find_all_images',
        'TranscriptionLoader',
        'OCRDebugger',
        'create_train_val_split',
        'HistoricalDocumentDataset',
        'get_train_transforms',
        'initialize_model',
        'create_trainer'
    ]

    for module in required_modules:
        try:
            importlib.import_module(module)
        except ImportError:
            missing_modules.append(module)

    if missing_modules:
        print(f"Warning: The following modules are missing: {missing_modules}")
        print("Run this notebook with all cells in sequence to define the required modules")
    else:
        print("All required modules are available!")

        # Create and run the pipeline
        pipeline = HistoricalSpanishOCRPipeline()

        # Let user choose parameters
        max_pages = int(input("Maximum pages per document (default: 6): ") or "6")
        num_epochs = int(input("Number of training epochs (default: 5): ") or "5")
        batch_size = int(input("Batch size (default: 2): ") or "2")

        results = pipeline.run_pipeline(
            max_pages_per_doc=max_pages,
            num_epochs=num_epochs,
            batch_size=batch_size
        )

        if "error" in results:
            print(f"Pipeline failed: {results['error']}")
        else:
            print("\nPipeline completed successfully!")
            print(f"Model saved to: {results['final_model_path']}")

            print("\nFinal metrics:")
            for key, value in results["metrics"].items():
                print(f"  {key}: {value:.4f}")

            print(f"\nTotal runtime: {results['runtime']['total_minutes']:.2f} minutes")

            # Provide example code for usage
            print("\nExample code to use the fine-tuned model:")
            print(results["demo_code"])

# This cell creates the required module files that will be imported
# by the main pipeline. In a production environment, these would be
# separate Python files, but for Colab we'll create them dynamically.

import os
import sys
import inspect
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_module(module_name, content):
    """
    Create a Python module with the given name and content.

    Args:
        module_name: Name of the module
        content: Source code for the module
    """
    module_spec = compile(content, module_name, 'exec')
    module = type(sys)(module_name)
    sys.modules[module_name] = module
    exec(module_spec, module.__dict__)
    logger.info(f"Created module: {module_name}")

# Create TranscriptionLoader module
create_module(module_name="TranscriptionLoader", content='''
import os
import re
import logging
from difflib import SequenceMatcher
import importlib
import subprocess
import sys

logger = logging.getLogger(__name__)

def normalize_document_name(doc_name):
    """
    Normalize a document name by removing common suffixes and standardizing format.

    Args:
        doc_name (str): Raw document name that might include page numbers or other suffixes

    Returns:
        str: Normalized document name
    """
    # Remove trailing page numbers
    doc_name = re.sub(r'[-_]\d+$', '', doc_name)

    # Remove suffixes
    suffix_patterns = [
        r' - Instruccion$',
        r' - Principe perfecto$',
        r' - Vozes$',
        r' - Reglas generales$',
        r' sinodales Calahorra 1602$'
    ]

    for pattern in suffix_patterns:
        doc_name = re.sub(pattern, '', doc_name)

    return doc_name.strip()
''')

def get_best_transcription_match(doc_name, available_transcriptions):
    """
    Find the best matching transcription filename for a document.

    Args:
        doc_name (str): Document name to match
        available_transcriptions (list): List of available transcription filenames

    Returns:
        str or None: Best matching transcription name or None if no good match found
    """
    if not available_transcriptions:
        return None

    # Normalize document name
    normalized_doc_name = normalize_document_name(doc_name)

    # Try direct match first
    for transcription in available_transcriptions:
        # Try exact match
        if normalized_doc_name == transcription:
            return transcription

        # Try with variations
        if normalized_doc_name == transcription.replace(' transcription', ''):
            return transcription

    # If no exact match, try fuzzy matching
    best_match = None
    best_ratio = 0.0

    for transcription in available_transcriptions:
        # Remove common suffixes for comparison
        base_transcription = transcription.replace(' transcription', '')

        # Calculate similarity ratio using SequenceMatcher
        ratio = SequenceMatcher(None, normalized_doc_name.lower(), base_transcription.lower()).ratio()
        full_ratio = SequenceMatcher(None, normalized_doc_name.lower(), transcription.lower()).ratio()
        ratio = max(ratio, full_ratio)

        if ratio > best_ratio and ratio > 0.6:  # Threshold for good matches
            best_ratio = ratio
            best_match = transcription

    if best_match:
        logger.info(f"Matched '{doc_name}' to '{best_match}' with confidence {best_ratio:.2f}")

    return best_match

class TranscriptionLoader:
    """
    Loads and processes transcription files from various formats.
    """
    def __init__(self, transcription_dir, page_transcription_dir):
        """
        Initialize the transcription loader.

        Args:
            transcription_dir (str): Directory containing original transcription files
            page_transcription_dir (str): Directory to store page-level transcriptions
        """
        self.transcription_dir = transcription_dir
        self.page_transcription_dir = page_transcription_dir
        os.makedirs(page_transcription_dir, exist_ok=True)

        # Initialize maps
        self.doc_transcription_map = {}
        self.available_transcriptions = []

        # Format handlers
        self.format_handlers = {
            '.txt': self._read_txt_file,
            '.docx': self._read_docx_file
        }

    def find_transcription_files(self):
        """
        Find all transcription files in the transcription directory.

        Returns:
            dict: Dictionary mapping document names to transcription file paths
        """
        # Clear existing maps
        self.doc_transcription_map = {}
        self.available_transcriptions = []

        for file in os.listdir(self.transcription_dir):
            file_path = os.path.join(self.transcription_dir, file)
            if not os.path.isfile(file_path):
                continue

            file_ext = os.path.splitext(file)[1].lower()
            if file_ext in self.format_handlers:
                # Get document name without extension
                doc_name = os.path.splitext(file)[0]
                self.doc_transcription_map[doc_name] = file_path
                self.available_transcriptions.append(doc_name)

        if self.doc_transcription_map:
            logger.info(f"Found {len(self.doc_transcription_map)} transcription files")
            for name in self.available_transcriptions:
                logger.info(f"  - {name}")
        else:
            logger.warning(f"No transcription files found in {self.transcription_dir}")

        return self.doc_transcription_map

    def _read_txt_file(self, file_path):
        """Read text content from a .txt file."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            # Try different encodings
            for encoding in ['latin-1', 'iso-8859-1', 'cp1252']:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        logger.warning(f"File {file_path} decoded using {encoding} instead of utf-8")
                        return f.read()
                except UnicodeDecodeError:
                    continue

            # Last resort
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                logger.error(f"Using fallback encoding with error ignoring for {file_path}")
                return f.read()

    def _read_docx_file(self, file_path):
        """Read text content from a .docx file."""
        try:
            # Try to import python-docx
            try:
                import docx
            except ImportError:
                logger.warning("python-docx not installed. Installing...")
                try:
                    subprocess.check_call([sys.executable, "-m", "pip", "install", "python-docx"])
                    importlib.invalidate_caches()
                    import docx
                except Exception as e:
                    logger.error(f"Failed to install python-docx: {e}")
                    return f"ERROR: Could not read {file_path} (python-docx not available)"

            # Read the docx file
            doc = docx.Document(file_path)
            full_text = []

            # Extract text from paragraphs
            for para in doc.paragraphs:
                full_text.append(para.text)

            # Extract from tables
            for table in doc.tables:
                for row in table.rows:
                    row_texts = [cell.text for cell in row.cells if cell.text.strip()]
                    if row_texts:
                        full_text.append(' | '.join(row_texts))

            return '\\n'.join(full_text)
        except Exception as e:
            logger.error(f"Error reading .docx file {file_path}: {str(e)}")
            return f"ERROR: Could not read {file_path}"

    def read_transcription(self, doc_name):
        """
        Read transcription for a specific document.

        Args:
            doc_name (str): Document name (will be normalized and matched)

        Returns:
            str or None: Transcription content or None if not found
        """
        # Find best matching transcription
        best_match = get_best_transcription_match(doc_name, self.available_transcriptions)

        if best_match and best_match in self.doc_transcription_map:
            file_path = self.doc_transcription_map[best_match]
            file_ext = os.path.splitext(file_path)[1].lower()

            if file_ext in self.format_handlers:
                logger.info(f"Reading transcription for '{doc_name}' from '{best_match}'")
                return self.format_handlers[file_ext](file_path)

        logger.warning(f"No transcription found for document {doc_name}")
        return None
""")

# Create find_all_images module
create_module("find_all_images", """
import os
import re
import logging
import glob
from PIL import Image

logger = logging.getLogger(__name__)

def extract_doc_info(image_path):
    """
    Extract document name and page number from image path with improved pattern matching.

    Args:
        image_path (str): Path to the image file

    Returns:
        tuple: (document_name, page_number) or (None, None) if not parsable
    """
    # Extract filename and parent directory
    filename = os.path.basename(image_path)
    parent_dir = os.path.basename(os.path.dirname(image_path))
    filename_without_ext = os.path.splitext(filename)[0]

    # Try multiple pattern recognition strategies

    # Strategy 1: Extract from parent directory and filename ending with -X
    match = re.match(r'(.+)-(\d+)$', parent_dir)
    if match and re.match(r'.*_page_\d+$', filename_without_ext):
        doc_name = parent_dir
        # Extract page number from filename (page_XXX)
        page_match = re.search(r'_page_(\d+)$', filename_without_ext)
        if page_match:
            page_num = int(page_match.group(1))
            return doc_name, page_num

    # Strategy 2: Document_Name_page_XXX.jpg
    match = re.match(r'(.+?)_page_(\d+)', filename_without_ext)
    if match:
        doc_name = match.group(1)
        page_num = int(match.group(2))
        return doc_name, page_num

    # Strategy 3: Use parent directory name with page number suffix in filename
    page_match = re.search(r'[-_](\d+)$', filename_without_ext)
    if page_match and parent_dir:
        # If the parent directory already has document info
        doc_name = parent_dir
        page_num = int(page_match.group(1))
        return doc_name, page_num

    # Strategy 4: Look for page markers in the filename
    page_match = re.search(r'[-_]p(?:age)?[-_]?(\d+)$', filename_without_ext, re.IGNORECASE)
    if page_match:
        page_num = int(page_match.group(1))
        # Remove page suffix to get document name
        doc_name = re.sub(r'[-_]p(?:age)?[-_]?\d+$', '', filename_without_ext, flags=re.IGNORECASE)
        if not doc_name:  # If extraction left nothing, use parent dir
            doc_name = parent_dir
        return doc_name, page_num

    # Strategy 5: If parent dir is a document and no page number in filename,
    # check if other files in same dir have numeric patterns and assign this one sequentially
    if parent_dir in ["Buendia - Instruccion-1", "Constituciones sinodales Calahorra 1602-2",
                      "Ezcaray - Vozes-3", "Mendo - Principe perfecto-4", "Paredes - Reglas generales-5",
                      "PORCONES.228.35 1636-6"]:
        # This is likely a document directory
        # Try to infer page number from the file's position
        sibling_files = sorted([f for f in os.listdir(os.path.dirname(image_path))
                              if f.endswith(('.jpg', '.png'))])
        if filename in sibling_files:
            page_num = sibling_files.index(filename) + 1  # 1-based page numbering
            return parent_dir, page_num

    logger.warning(f"Could not parse document name and page from: {image_path}")
    return None, None

def find_all_images(base_path='/content'):
    """
    Find all processed and binary images with improved metadata extraction.

    Args:
        base_path (str): Base directory containing all data

    Returns:
        tuple: (processed_images, binary_images, image_mapping)
    """
    output_base_path = os.path.join(base_path, 'ocr_data')
    processed_dir = os.path.join(output_base_path, "processed_images")
    binary_dir = os.path.join(output_base_path, "binary_images")

    processed_images = []
    binary_images = []
    image_mapping = {}  # Maps (doc_name, page_num) to {'processed': path, 'binary': path}

    # Store document names for logging
    found_docs = set()
    doc_page_counts = {}

    # Find all processed images
    if os.path.exists(processed_dir):
        for root, _, files in os.walk(processed_dir):
            for file in files:
                if file.endswith(('.jpg', '.png')):
                    img_path = os.path.join(root, file)
                    processed_images.append(img_path)

                    # Extract document info
                    doc_name, page_num = extract_doc_info(img_path)

                    # Fallback if extraction fails
                    if doc_name is None or page_num is None:
                        parent_dir = os.path.basename(os.path.dirname(img_path))
                        if parent_dir not in doc_page_counts:
                            doc_page_counts[parent_dir] = 0
                        doc_page_counts[parent_dir] += 1
                        doc_name = parent_dir
                        page_num = doc_page_counts[parent_dir]

                    found_docs.add(doc_name)
                    if (doc_name, page_num) not in image_mapping:
                        image_mapping[(doc_name, page_num)] = {'processed': img_path}
                    else:
                        # Update if entry exists but doesn't have processed path
                        if 'processed' not in image_mapping[(doc_name, page_num)]:
                            image_mapping[(doc_name, page_num)]['processed'] = img_path
    else:
        logger.warning(f"Processed images directory not found: {processed_dir}")

    # Find all binary images
    if os.path.exists(binary_dir):
        for root, _, files in os.walk(binary_dir):
            for file in files:
                if file.endswith(('.jpg', '.png')):
                    img_path = os.path.join(root, file)
                    binary_images.append(img_path)

                    # Extract document info
                    doc_name, page_num = extract_doc_info(img_path)

                    # Fallback if extraction fails - try to match with processed image
                    if doc_name is None or page_num is None:
                        parent_dir = os.path.basename(os.path.dirname(img_path))
                        filename = os.path.basename(img_path)

                        # Try to find matching processed image
                        match_found = False
                        for (d, p), mapping in image_mapping.items():
                            if d == parent_dir and os.path.basename(mapping.get('processed', '')) == filename:
                                doc_name = d
                                page_num = p
                                match_found = True
                                break

                        # If no match found, create a new entry
                        if not match_found:
                            if parent_dir not in doc_page_counts:
                                doc_page_counts[parent_dir] = 0
                            doc_page_counts[parent_dir] += 1
                            doc_name = parent_dir
                            page_num = doc_page_counts[parent_dir]

                    found_docs.add(doc_name)
                    if (doc_name, page_num) in image_mapping:
                        image_mapping[(doc_name, page_num)]['binary'] = img_path
                    else:
                        image_mapping[(doc_name, page_num)] = {'binary': img_path}
    else:
        logger.warning(f"Binary images directory not found: {binary_dir}")

    # Log results
    logger.info(f"Found {len(processed_images)} processed images and {len(binary_images)} binary images")
    logger.info(f"Identified {len(found_docs)} unique documents")
    logger.info(f"Created {len(image_mapping)} document-page mappings")

    # Validate mapping
    for (doc, page), paths in image_mapping.items():
        if 'processed' not in paths:
            logger.warning(f"Missing processed image for {doc}, page {page}")
        if 'binary' not in paths:
            logger.warning(f"Missing binary image for {doc}, page {page}")

    return processed_images, binary_images, image_mapping
""")

# Create OCRDebugger module
create_module("OCRDebugger", """
import os
import logging
import torch
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import re
import sys

logger = logging.getLogger(__name__)

class OCRDebugger:
    \"\"\"
    Debugging and validation tools for OCR pipeline.

    This class provides methods to validate data integrity, visualize page
    detection accuracy, and diagnose common issues with transcription mapping.
    \"\"\"
    def __init__(self, base_path='/content'):
        \"\"\"
        Initialize the debugger.

        Args:
            base_path: Base directory for all data
        \"\"\"
        self.base_path = base_path
        self.output_base_path = os.path.join(base_path, 'ocr_data')
        self.transcriptions_path = os.path.join(base_path, 'transcriptions')

    def validate_directory_structure(self):
        \"\"\"
        Validate that all required directories exist with proper permissions.

        Returns:
            bool: True if structure is valid, False otherwise
        \"\"\"
        required_dirs = [
            self.output_base_path,
            os.path.join(self.output_base_path, "images"),
            os.path.join(self.output_base_path, "processed_images"),
            os.path.join(self.output_base_path, "binary_images"),
            os.path.join(self.output_base_path, "page_transcriptions"),
            self.transcriptions_path
        ]

        valid = True
        for directory in required_dirs:
            if not os.path.exists(directory):
                logger.error(f"Missing directory: {directory}")
                valid = False
            elif not os.access(directory, os.R_OK | os.W_OK):
                logger.error(f"Insufficient permissions for: {directory}")
                valid = False

        if valid:
            logger.info("✅ Directory structure validation passed")
        else:
            logger.error("❌ Directory structure validation failed")

        return valid

    def validate_document_subdirectories(self):
        \"\"\"
        Validate that each document has proper subdirectories.

        Returns:
            bool: True if valid, False otherwise
        \"\"\"
        # Define expected document subdirectories
        expected_subdirs = [
            "Buendia - Instruccion-1",
            "Constituciones sinodales Calahorra 1602-2",
            "Ezcaray - Vozes-3",
            "Mendo - Principe perfecto-4",
            "Paredes - Reglas generales-5",
            "PORCONES.228.35 1636-6"
        ]

        # Check in processed_images and binary_images directories
        processed_dir = os.path.join(self.output_base_path, "processed_images")
        binary_dir = os.path.join(self.output_base_path, "binary_images")

        valid = True

        # Check processed_images
        if os.path.exists(processed_dir):
            for subdir in expected_subdirs:
                full_path = os.path.join(processed_dir, subdir)
                if not os.path.exists(full_path):
                    logger.error(f"Missing subdirectory in processed_images: {subdir}")
                    valid = False
        else:
            logger.error(f"Missing directory: {processed_dir}")
            valid = False

        # Check binary_images
        if os.path.exists(binary_dir):
            for subdir in expected_subdirs:
                full_path = os.path.join(binary_dir, subdir)
                if not os.path.exists(full_path):
                    logger.error(f"Missing subdirectory in binary_images: {subdir}")
                    valid = False
        else:
            logger.error(f"Missing directory: {binary_dir}")
            valid = False

        if valid:
            logger.info("✅ Document subdirectories validation passed")
        else:
            logger.error("❌ Document subdirectories validation failed")

        return valid

    def validate_transcription_files(self):
        \"\"\"
        Validate that transcription files exist and are readable.

        Returns:
            bool: True if valid, False otherwise
        \"\"\"
        valid = True

        # Check if transcriptions directory exists
        if not os.path.exists(self.transcriptions_path):
            logger.error(f"Missing transcriptions directory: {self.transcriptions_path}")
            return False

        # Look for transcription files
        transcription_files = []
        for ext in ['.txt', '.docx']:
            transcription_files.extend(
                [f for f in os.listdir(self.transcriptions_path) if f.endswith(ext)]
            )

        if not transcription_files:
            logger.error("No transcription files found")
            valid = False
        else:
            logger.info(f"Found {len(transcription_files)} transcription files")

            # Validate each file is readable
            for filename in transcription_files:
                file_path = os.path.join(self.transcriptions_path, filename)
                try:
                    if filename.endswith('.txt'):
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read(100)  # Read just a bit to check
                    elif filename.endswith('.docx'):
                        try:
                            import docx
                            doc = docx.Document(file_path)
                            has_content = len(doc.paragraphs) > 0
                            if not has_content:
                                logger.warning(f"DOCX file appears to be empty: {filename}")
                        except ImportError:
                            logger.warning("python-docx not installed, cannot verify DOCX content")
                            # Try to install it
                            try:
                                import subprocess
                                logger.info("Installing python-docx...")
                                subprocess.check_call([sys.executable, "-m", "pip", "install", "python-docx"])
                                import docx
                                doc = docx.Document(file_path)
                                has_content = len(doc.paragraphs) > 0
                                if not has_content:
                                    logger.warning(f"DOCX file appears to be empty: {filename}")
                            except Exception as e:
                                logger.error(f"Failed to install python-docx: {e}")
                                valid = False
                        except Exception as e:
                            logger.error(f"Error reading DOCX file {filename}: {e}")
                            valid = False
                except Exception as e:
                    logger.error(f"Error reading file {filename}: {e}")
                    valid = False

        if valid:
            logger.info("✅ Transcription files validation passed")
        else:
            logger.error("❌ Transcription files validation failed")

        return valid

    def validate_image_files(self):
        \"\"\"
        Validate that document images exist and are readable.

        Returns:
            bool: True if valid, False otherwise
        \"\"\"
        valid = True
        processed_dir = os.path.join(self.output_base_path, "processed_images")

        if not os.path.exists(processed_dir):
            logger.error(f"Missing processed images directory: {processed_dir}")
            return False

        # Scan for images in all subdirectories
        image_count = 0
        for root, _, files in os.walk(processed_dir):
            for file in files:
                if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                    image_path = os.path.join(root, file)
                    image_count += 1

                    # Try to open the image to verify it's valid
                    try:
                        with Image.open(image_path) as img:
                            # Check if image is valid by accessing its size
                            width, height = img.size
                            if width <= 0 or height <= 0:
                                logger.error(f"Invalid image dimensions in {image_path}: {width}x{height}")
                                valid = False
                    except Exception as e:
                        logger.error(f"Error opening image {image_path}: {e}")
                        valid = False

        if image_count == 0:
            logger.error("No image files found")
            valid = False
        else:
            logger.info(f"Found {image_count} image files")

        if valid:
            logger.info("✅ Image files validation passed")
        else:
            logger.error("❌ Image files validation failed")

        return valid

    def diagnose_page_markers(self, show_examples=True):
        \"\"\"
        Analyze transcription files to find and diagnose page markers.

        Args:
            show_examples: Whether to show matching examples

        Returns:
            dict: Page marker statistics
        \"\"\"
        logger.info("Analyzing transcription files for page markers...")

        # Check if transcriptions directory exists
        if not os.path.exists(self.transcriptions_path):
            logger.error(f"Missing transcriptions directory: {self.transcriptions_path}")
            return {}

        # Get all text files in the transcriptions directory
        txt_files = [f for f in os.listdir(self.transcriptions_path) if f.endswith('.txt')]

        if not txt_files:
            logger.warning("No text transcription files found")
            return {}

        # Define page marker patterns to search for
        marker_patterns = [
            (r'PDF\\s+p(\\d+)', "PDF pX"),
            (r'Page\\s+(\\d+)', "Page X"),
            (r'\\*\\*PDF\\s+p(\\d+)\\*\\*', "**PDF pX**"),
            (r'\\*\\*Page\\s+(\\d+)\\*\\*', "**Page X**"),
            (r'p\\.?\\s*(\\d+)', "p. X"),
        ]

        # Collect statistics
        stats = {
            "total_files": len(txt_files),
            "files_with_markers": 0,
            "marker_counts": {},
            "examples": {}
        }

        for pattern_desc in marker_patterns:
            stats["marker_counts"][pattern_desc[1]] = 0
            stats["examples"][pattern_desc[1]] = []

        # Analyze each file
        for filename in txt_files:
            file_path = os.path.join(self.transcriptions_path, filename)

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Check for each marker pattern
                file_has_markers = False

                for pattern, desc in marker_patterns:
                    matches = list(re.finditer(pattern, content, re.IGNORECASE))

                    if matches:
                        file_has_markers = True
                        stats["marker_counts"][desc] += len(matches)

                        # Store a few examples
                        if show_examples and len(stats["examples"][desc]) < 3:
                            for match in matches[:3]:
                                # Get context (20 chars before and after)
                                start = max(0, match.start() - 20)
                                end = min(len(content), match.end() + 20)
                                context = content[start:end]

                                stats["examples"][desc].append({
                                    "file": filename,
                                    "page": match.group(1),
                                    "context": context,
                                    "position": match.start()
                                })

                if file_has_markers:
                    stats["files_with_markers"] += 1

            except Exception as e:
                logger.error(f"Error analyzing file {filename}: {e}")

        # Print summary
        logger.info(f"Found page markers in {stats['files_with_markers']} out of {stats['total_files']} files")

        for desc, count in stats["marker_counts"].items():
            logger.info(f"  {desc}: {count} occurrences")

        # Show examples if requested
        if show_examples:
            logger.info("Example page markers:")

            for desc, examples in stats["examples"].items():
                if examples:
                    logger.info(f"  Pattern: {desc}")

                    for i, example in enumerate(examples):
                        logger.info(f"    Example {i+1} from {example['file']} (page {example['page']}):")
                        logger.info(f"      \\\"...{example['context']}...\\\"")

        return stats

    def visualize_page_detection(self, page_transcriptions, num_samples=2):
        \"\"\"
        Visualize document pages with their detected transcriptions.

        Args:
            page_transcriptions: Dictionary of page transcriptions
            num_samples: Number of random samples to display

        Returns:
            bool: True if visualization was successful
        \"\"\"
        if not page_transcriptions:
            logger.error("No page transcriptions provided")
            return False

        # Select random samples
        keys = list(page_transcriptions.keys())
        if not keys:
            logger.error("Page transcriptions dictionary is empty")
            return False

        # Group by document for better visualization
        docs = {}
        for key in keys:
            doc_name, page_num = key
            if doc_name not in docs:
                docs[doc_name] = []
            docs[doc_name].append(page_num)

        # Sort page numbers
        for doc_name in docs:
            docs[doc_name].sort()

        # Display samples from each document
        for doc_name, page_nums in docs.items():
            # Show header
            logger.info(f"\\nDocument: {doc_name}")
            logger.info(f"Available pages: {page_nums}")

            # Select a couple of pages to show
            pages_to_show = []
            if len(page_nums) <= num_samples:
                pages_to_show = page_nums
            else:
                # Try to get first, middle and last page
                if num_samples >= 3:
                    pages_to_show = [page_nums[0], page_nums[len(page_nums)//2], page_nums[-1]]
                else:
                    pages_to_show = np.random.choice(page_nums, min(num_samples, len(page_nums)), replace=False)

            # Display each selected page
            for page_num in pages_to_show:
                key = (doc_name, page_num)
                data = page_transcriptions[key]

                # Display image
                img_path = data.get('processed_image')
                if img_path and os.path.exists(img_path):
                    plt.figure(figsize=(8, 10))
                    img = Image.open(img_path)
                    plt.imshow(img)
                    plt.title(f"{doc_name} - Page {page_num}")
                    plt.axis('off')
                    plt.show()
                else:
                    logger.warning(f"Image not found for {doc_name}, page {page_num}")

                # Display transcription excerpt
                text = data.get('text', '')
                source = data.get('source', 'unknown')
                logger.info(f"Page {page_num} - Source: {source}")
                logger.info(f"Transcription excerpt (first 300 chars):")
                logger.info(f"{text[:300]}...")
                logger.info(f"Total length: {len(text)} characters")
                logger.info("-" * 50)

        return True

    def run_all_validations(self):
        \"\"\"
        Run all validation checks.

        Returns:
            dict: Validation results
        \"\"\"
        logger.info("Running all validation checks...")

        results = {
            "directory_structure": self.validate_directory_structure(),
            "document_subdirectories": self.validate_document_subdirectories(),
            "transcription_files": self.validate_transcription_files(),
            "image_files": self.validate_image_files()
        }

        # Analyze page markers
        results["page_markers"] = self.diagnose_page_markers(show_examples=True)

        # Overall success
        success = all([
            results["directory_structure"],
            results["document_subdirectories"],
            results["transcription_files"],
            results["image_files"]
        ])

        if success:
            logger.info("✅ All validation checks passed")
        else:
            logger.error("❌ Some validation checks failed")
            # List the failed checks
            failed = []
            for check, result in results.items():
                if isinstance(result, bool) and not result:
                    failed.append(check)
            logger.error(f"Failed checks: {', '.join(failed)}")

        return results
""")

# Create create_train_val_split module
create_module("create_train_val_split", """
import random
import logging

logger = logging.getLogger(__name__)

def create_train_val_split(page_transcriptions, val_ratio=0.2, seed=42):
    \"\"\"
    Split the page transcriptions into training and validation sets.

    Args:
        page_transcriptions (dict): Dictionary mapping (doc_name, page_num) to page data
        val_ratio (float): Ratio of validation data (0-1)
        seed (int): Random seed for reproducibility

    Returns:
        tuple: (train_data, val_data) dictionaries
    \"\"\"
    random.seed(seed)

    # Get all keys (document_name, page_number) tuples
    keys = list(page_transcriptions.keys())

    # Group by document to ensure we have a mix of documents in both sets
    doc_groups = {}
    for doc_name, page_num in keys:
        if doc_name not in doc_groups:
            doc_groups[doc_name] = []
        doc_groups[doc_name].append((doc_name, page_num))

    train_keys = []
    val_keys = []

    # Split each document's pages
    for doc_name, doc_keys in doc_groups.items():
        random.shuffle(doc_keys)
        split_idx = max(1, int(len(doc_keys) * (1 - val_ratio)))

        train_keys.extend(doc_keys[:split_idx])
        val_keys.extend(doc_keys[split_idx:])

    # Create dictionaries
    train_data = {k: page_transcriptions[k] for k in train_keys}
    val_data = {k: page_transcriptions[k] for k in val_keys}

    logger.info(f"Created train-val split:")
    logger.info(f"  Training set: {len(train_data)} pages from {len(set(k[0] for k in train_keys))} documents")
    logger.info(f"  Validation set: {len(val_data)} pages from {len(set(k[0] for k in val_keys))} documents")

    return train_data, val_data
""")

# Create HistoricalDocumentDataset module
create_module("HistoricalDocumentDataset", """
import torch
from torch.utils.data import Dataset
from torchvision import transforms
from PIL import Image
import logging

logger = logging.getLogger(__name__)

class HistoricalDocumentDataset(Dataset):
    \"\"\"
    Dataset class for historical Spanish documents.
    \"\"\"
    def __init__(self, page_data, processor, transform=None, max_length=512):
        \"\"\"
        Initialize the dataset.

        Args:
            page_data (dict): Dictionary mapping (doc_name, page_num) to page data
            processor: TrOCR processor for tokenization
            transform: Optional transforms to apply to images
            max_length (int): Maximum sequence length for tokenization
        \"\"\"
        self.page_keys = list(page_data.keys())
        self.page_data = page_data
        self.processor = processor
        self.transform = transform
        self.max_length = max_length

    def __len__(self):
        return len(self.page_keys)

    def __getitem__(self, idx):
        key = self.page_keys[idx]
        data = self.page_data[key]

        # Load image with error handling
        try:
            image_path = data['processed_image']
            image = Image.open(image_path).convert('RGB')

            # Apply transforms if provided
            if self.transform:
                image = self.transform(image)
            else:
                # Default processing if no transform provided
                image = transforms.ToTensor()(transforms.Resize((384, 384))(image))

        except Exception as e:
            logger.error(f"Error loading image {data.get('processed_image')}: {e}")
            # Create blank image as fallback
            image = torch.zeros(3, 384, 384)

        # Get and truncate text if needed
        text = data.get('text', '')
        if len(text) > self.max_length:
            text = text[:self.max_length]

        # We'll handle tokenization separately to avoid errors in __getitem__
        return {
            "image": image,
            "text": text,
            "doc_name": key[0],
            "page_num": key[1],
            "image_path": data.get('processed_image', '')
        }
""")

# Create get_train_transforms module
create_module("get_train_transforms", """
from torchvision import transforms

def get_train_transforms():
    \"\"\"
    Create augmentation transforms for training data.
    \"\"\"
    return transforms.Compose([
        transforms.Resize((384, 384)),  # TrOCR expected size
        transforms.RandomRotation(1),   # Slight rotation (reduced from 3 to be gentler)
        transforms.RandomApply([
            transforms.ColorJitter(brightness=0.1, contrast=0.1)
        ], p=0.2),  # Subtle color adjustments with lower probability
        transforms.ToTensor(),
    ])

def get_val_transforms():
    \"\"\"
    Create transforms for validation data.
    \"\"\"
    return transforms.Compose([
        transforms.Resize((384, 384)),  # TrOCR expected size
        transforms.ToTensor(),
    ])
""")

# Create initialize_model module
create_module("initialize_model", """
import torch
import logging
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
import warnings

# Suppress specific warnings
warnings.filterwarnings("ignore", message=".*Some weights of VisionEncoderDecoderModel were not initialized.*")
warnings.filterwarnings("ignore", message=".*Moving the following attributes in the config to the generation config.*")

logger = logging.getLogger(__name__)

def initialize_model(model_name="microsoft/trocr-base-printed", device=None):
    \"\"\"
    Initialize a TrOCR model with proper configuration for historical document OCR.

    Args:
        model_name (str): Name or path of the model to load
        device: Device to move model to (auto-detect if None)

    Returns:
        tuple: (model, processor, device)
    \"\"\"
    logger.info(f"Initializing model: {model_name}")

    # Determine device
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    try:
        # Load processor and model
        processor = TrOCRProcessor.from_pretrained(model_name)
        model = VisionEncoderDecoderModel.from_pretrained(model_name)

        # Set essential token IDs in model configuration
        model.config.decoder_start_token_id = processor.tokenizer.bos_token_id
        model.config.pad_token_id = processor.tokenizer.pad_token_id
        model.config.eos_token_id = processor.tokenizer.eos_token_id

        # Ensure decoder knows about token IDs
        model.config.decoder.decoder_start_token_id = processor.tokenizer.bos_token_id
        model.config.decoder.pad_token_id = processor.tokenizer.pad_token_id
        model.config.decoder.eos_token_id = processor.tokenizer.eos_token_id

        # Set vocabulary size
        model.config.vocab_size = model.config.decoder.vocab_size

        # Configure generation parameters
        model.config.max_length = 512  # Historical documents can be longer
        model.config.early_stopping = True
        model.config.no_repeat_ngram_size = 3
        model.config.length_penalty = 2.0
        model.config.num_beams = 4

        # Move to device
        model = model.to(device)
        logger.info(f"Model loaded and moved to {device}")

        return model, processor, device

    except Exception as e:
        logger.error(f"Error initializing model: {str(e)}")
        raise

def optimize_memory_usage(model):
    \"\"\"
    Apply memory optimization techniques to reduce VRAM usage.

    Args:
        model: The model to optimize

    Returns:
        The optimized model
    \"\"\"
    logger.info("Applying memory optimizations")

    # 1. Enable gradient checkpointing (trades memory for computation)
    model.gradient_checkpointing_enable()
    logger.info("Enabled gradient checkpointing")

    # 2. For extreme memory pressure scenarios
    if torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory < 8e9:  # Less than 8GB VRAM
        logger.info("Low VRAM detected. Applying additional optimizations.")

        # Freeze encoder parameters
        for param in model.encoder.parameters():
            param.requires_grad = False

        # Unfreeze only the last encoder layers
        if hasattr(model.encoder, 'layer'):
            num_unfrozen = 2  # Unfreeze only the last 2 layers
            for i, layer in enumerate(model.encoder.layer):
                if i >= len(model.encoder.layer) - num_unfrozen:
                    for param in layer.parameters():
                        param.requires_grad = True

        logger.info(f"Froze encoder parameters except for the last {num_unfrozen} layers")

    return model
""")

# Create create_trainer module
create_module("create_trainer", """
import os
import logging
import torch
from transformers import (
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    TrainerCallback
)
import pandas as pd
from IPython.display import display, HTML

logger = logging.getLogger(__name__)

def get_training_args(output_dir, batch_size=4, num_epochs=10, learning_rate=5e-5):
    \"\"\"
    Create training arguments optimized for historical document OCR.

    Args:
        output_dir (str): Output directory for checkpoints and logs
        batch_size (int): Per-device batch size
        num_epochs (int): Number of training epochs
        learning_rate (float): Learning rate

    Returns:
        Seq2SeqTrainingArguments: Training arguments
    \"\"\"
    # Determine if we can use mixed precision
    fp16 = torch.cuda.is_available()
    gradient_accumulation_steps = 4 if not fp16 else 2  # Reduce accumulation with fp16

    # Create base training arguments
    args = Seq2SeqTrainingArguments(
        output_dir=output_dir,
        eval_strategy="epoch",  # Evaluate after each epoch
        save_strategy="epoch",  # Save checkpoint after each epoch
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        learning_rate=learning_rate,
        num_train_epochs=num_epochs,
        weight_decay=0.01,  # L2 regularization
        fp16=fp16,  # Mixed precision if available
        generation_max_length=512,  # For long historical texts
        predict_with_generate=True,  # Use generation for evaluation
        # Scheduler settings
        warmup_ratio=0.1,  # Warm up over 10% of steps
        # Logging settings
        logging_strategy="steps",
        logging_steps=10,
        logging_dir=os.path.join(output_dir, "logs"),
        # Evaluation settings
        metric_for_best_model="cer",
        greater_is_better=False,  # Lower CER is better
        load_best_model_at_end=True,
        # Misc settings
        report_to="tensorboard",
        save_total_limit=2,  # Only keep 2 checkpoints to save space
        remove_unused_columns=False,  # Keep all columns for custom processing
        dataloader_drop_last=False,  # Don't drop last batch
    )

    return args

class LoggingCallback(TrainerCallback):
    \"\"\"
    Custom callback for better logging during training.
    \"\"\"
    def __init__(self):
        self.training_tracker = []
        self.best_metrics = {"cer": float('inf'), "wer": float('inf'), "epoch": 0}

    def on_epoch_end(self, args, state, control, metrics=None, **kwargs):
        \"\"\"Log metrics at the end of each epoch.\"\"\"
        if metrics:
            # Convert metrics to a more readable format
            epoch_metrics = {
                "epoch": round(state.epoch, 2),
                "train_loss": metrics.get('train_loss', float('nan')),
                "eval_loss": metrics.get('eval_loss', float('nan')),
                "cer": metrics.get('eval_cer', float('nan')),
                "wer": metrics.get('eval_wer', float('nan'))
            }

            # Track metrics
            self.training_tracker.append(epoch_metrics)

            # Check if this is the best model
            if epoch_metrics["cer"] < self.best_metrics["cer"]:
                self.best_metrics["cer"] = epoch_metrics["cer"]
                self.best_metrics["wer"] = epoch_metrics["wer"]
                self.best_metrics["epoch"] = epoch_metrics["epoch"]

            # Display current metrics in a nice table
            self._display_metrics_table()

            # Show best metrics so far
            print(f"\\nBest metrics so far (epoch {self.best_metrics['epoch']}):")
            print(f"  CER: {self.best_metrics['cer']:.4f}")
            print(f"  WER: {self.best_metrics['wer']:.4f}")

    def _display_metrics_table(self):
        \"\"\"Display metrics in a formatted table.\"\"\"
        # Convert tracking data to DataFrame
        df = pd.DataFrame(self.training_tracker)

        # Format the metrics (round to 4 decimal places)
        for col in df.columns:
            if col != "epoch":
                df[col] = df[col].apply(lambda x: f"{x:.4f}" if pd.notnull(x) else "N/A")

        # Display the table
        print("\\nTraining Progress:")
        display(HTML(df.to_html(index=False)))

class PredictionDisplayCallback(TrainerCallback):
    \"\"\"
    Callback to display model predictions during training.
    \"\"\"
    def __init__(self, eval_dataset, processor, device, display_freq=2):
        self.eval_dataset = eval_dataset
        self.processor = processor
        self.device = device
        self.display_freq = display_freq  # Display every N epochs

    def on_epoch_end(self, args, state, control, **kwargs):
        # Only display predictions occasionally
        if round(state.epoch) % self.display_freq != 0:
            return

        # Get model from kwargs
        model = kwargs.get('model')
        if model is None:
            return

        print(f"\\nSample predictions at epoch {round(state.epoch)}:")

        # Sample a few examples
        num_examples = min(2, len(self.eval_dataset))
        indices = range(num_examples)  # Just use first few examples for consistency

        for idx in indices:
            try:
                sample = self.eval_dataset[idx]

                # Get the image and move to device
                image = sample["image"].unsqueeze(0).to(self.device)
                text = sample["text"]
                doc_name = sample["doc_name"]
                page_num = sample["page_num"]

                # Process image for the model
                pixel_values = self.processor(images=image, return_tensors="pt").pixel_values.to(self.device)

                # Generate prediction
                with torch.no_grad():
                    generated_ids = model.generate(
                        pixel_values,
                        max_length=512,
                        num_beams=4,
                        early_stopping=True
                    )

                    prediction = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

                # Calculate metrics
                from metrics import character_error_rate, word_error_rate
                cer = character_error_rate(text, prediction)
                wer = word_error_rate(text, prediction)

                # Display results
                print(f"\\nDocument: {doc_name}, Page: {page_num}")
                print(f"Reference (first 100 chars): {text[:100]}...")
                print(f"Prediction (first 100 chars): {prediction[:100]}...")
                print(f"CER: {cer:.4f}, WER: {wer:.4f}")
                print("-" * 50)
            except Exception as e:
                logger.error(f"Error displaying prediction for sample {idx}: {e}")
                continue

class CustomSeq2SeqTrainer(Seq2SeqTrainer):
    \"\"\"
    Custom trainer with enhanced functionality for OCR fine-tuning.
    \"\"\"
    def __init__(self, processor=None, *args, **kwargs):
        # Store processor for metrics calculation
        self.processor = processor
        super().__init__(*args, **kwargs)

    def compute_loss(self, model, inputs, return_outputs=False):
        \"\"\"
        Custom compute_loss to handle robustness issues.
        \"\"\"
        try:
            # Default loss computation
            return super().compute_loss(model, inputs, return_outputs)
        except Exception as e:
            logger.warning(f"Error in compute_loss: {str(e)}")
            # Create a dummy loss for robustness
            dummy_loss = torch.tensor(1.0, device=model.device, requires_grad=True)

            if return_outputs:
                return dummy_loss, None
            return dummy_loss

    def save_model(self, output_dir=None, _internal_call=False):
        \"\"\"
        Enhanced model saving with metadata.
        \"\"\"
        # Call the parent method first
        super().save_model(output_dir, _internal_call)

        # Save processor alongside model
        if self.processor and output_dir:
            self.processor.save_pretrained(output_dir)
            logger.info(f"Saved processor to {output_dir}")

            # Save training metadata
            metadata = {
                "training_args": vars(self.args),
                "epochs_completed": self.state.epoch,
                "global_step": self.state.global_step
            }

            import json
            try:
                with open(os.path.join(output_dir, "training_metadata.json"), "w") as f:
                    json.dump(metadata, f, indent=2, default=str)
            except Exception as e:
                logger.warning(f"Could not save training metadata: {str(e)}")

# Import metrics for use in callbacks
create_module("metrics", \"\"\"
import re
import numpy as np

def normalize_text(text):
    \"\"\"
    Normalize text for evaluation metrics.

    Args:
        text (str): Input text

    Returns:
        str: Normalized text
    \"\"\"
    if not isinstance(text, str):
        return ""

    # Convert to lowercase
    text = text.lower()

    # Remove punctuation except spaces
    text = re.sub(r'[^\\w\\s]', '', text)

    # Normalize whitespace
    text = re.sub(r'\\s+', ' ', text).strip()

    # Normalize Spanish characters
    text = text.replace('ñ', 'n')
    text = text.replace('á', 'a').replace('é', 'e').replace('í', 'i').replace('ó', 'o').replace('ú', 'u')
    text = text.replace('ü', 'u')

    return text

def levenshtein_distance(s1, s2):
    \"\"\"
    Calculate Levenshtein (edit) distance between two sequences.

    Args:
        s1: First sequence (string or list)
        s2: Second sequence (string or list)

    Returns:
        int: Edit distance
    \"\"\"
    # Handle empty strings
    if len(s1) == 0:
        return len(s2)
    if len(s2) == 0:
        return len(s1)

    # Create matrix
    d = [[0 for _ in range(len(s2) + 1)] for _ in range(len(s1) + 1)]

    # Initialize first row and column
    for i in range(len(s1) + 1):
        d[i][0] = i
    for j in range(len(s2) + 1):
        d[0][j] = j

    # Fill the matrix
    for i in range(1, len(s1) + 1):
        for j in range(1, len(s2) + 1):
            cost = 0 if s1[i-1] == s2[j-1] else 1
            d[i][j] = min(
                d[i-1][j] + 1,      # deletion
                d[i][j-1] + 1,      # insertion
                d[i-1][j-1] + cost  # substitution
            )

    return d[len(s1)][len(s2)]

def character_error_rate(reference, hypothesis):
    \"\"\"
    Calculate Character Error Rate (CER).

    Args:
        reference (str): Ground truth text
        hypothesis (str): Predicted text

    Returns:
        float: CER (0-1, lower is better)
    \"\"\"
    # Handle edge cases
    if not reference:
        return 1.0 if hypothesis else 0.0

    # Normalize texts
    reference = normalize_text(reference)
    hypothesis = normalize_text(hypothesis)

    # Compute Levenshtein distance
    distance = levenshtein_distance(reference, hypothesis)

    # CER = edit distance / length of reference
    return distance / len(reference)

def word_error_rate(reference, hypothesis):
    \"\"\"
    Calculate Word Error Rate (WER).

    Args:
        reference (str): Ground truth text
        hypothesis (str): Predicted text

    Returns:
        float: WER (0-1, lower is better)
    \"\"\"
    # Handle edge cases
    if not reference:
        return 1.0 if hypothesis else 0.0

    # Normalize texts
    reference = normalize_text(reference)
    hypothesis = normalize_text(hypothesis)

    # Split into words
    reference_words = reference.split()
    hypothesis_words = hypothesis.split()

    # Handle empty word lists
    if not reference_words:
        return 1.0 if hypothesis_words else 0.0

    # Compute Levenshtein distance at word level
    distance = levenshtein_distance(reference_words, hypothesis_words)

    # WER = edit distance / number of words in reference
    return distance / len(reference_words)
\"\"\")

def create_trainer(model, processor, train_dataset, eval_dataset, output_dir,
                   num_epochs=10, batch_size=4, learning_rate=5e-5):
    \"\"\"
    Create a trainer for OCR fine-tuning with all the right callbacks.

    Args:
        model: TrOCR model
        processor: TrOCR processor
        train_dataset: Training dataset
        eval_dataset: Evaluation dataset
        output_dir: Output directory for checkpoints
        num_epochs: Number of training epochs
        batch_size: Batch size per device
        learning_rate: Learning rate

    Returns:
        CustomSeq2SeqTrainer: Configured trainer
    \"\"\"
    # Get training arguments
    training_args = get_training_args(
        output_dir=output_dir,
        batch_size=batch_size,
        num_epochs=num_epochs,
        learning_rate=learning_rate
    )

    # Create data collator
    def collate_fn(examples):
        # Extract pixel values and labels
        pixel_values = torch.stack([example["image"] for example in examples])

        # Tokenize texts
        tokenized = processor.tokenizer(
            [example["text"] for example in examples],
            padding="max_length",
            max_length=512,
            truncation=True,
            return_tensors="pt"
        )
        labels = tokenized.input_ids

        # Replace padding token id with -100 so it's ignored in loss
        labels[labels == processor.tokenizer.pad_token_id] = -100

        return {
            "pixel_values": pixel_values,
            "labels": labels,
            "decoder_input_ids": torch.zeros_like(labels)
        }

    # Create callbacks
    logging_callback = LoggingCallback()
    prediction_callback = PredictionDisplayCallback(
        eval_dataset=eval_dataset,
        processor=processor,
        device=model.device,
        display_freq=2
    )

    # Create trainer
    trainer = CustomSeq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=collate_fn,
        processor=processor,  # Pass processor to our custom trainer
        callbacks=[logging_callback, prediction_callback]
    )

    return trainer
""")

# Make the modules available in the current session
print("Created the following modules:")
for module_name in sys.modules:
    if module_name in [
        "TranscriptionLoader",
        "find_all_images",
        "OCRDebugger",
        "create_train_val_split",
        "HistoricalDocumentDataset",
        "get_train_transforms",
        "initialize_model",
        "metrics",
        "create_trainer"
    ]:
        print(f"  - {module_name}")

print("\nAll modules created successfully and ready for use!")