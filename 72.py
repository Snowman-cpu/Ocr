# -*- coding: utf-8 -*-
"""72.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CT9kDwSX2wUYyCeMayerxTPuc64lbvQj

# 1. Setting Up the Environment
"""

import os

# Define base paths
base_path = '/content'
pdf_folder = os.path.join(base_path, 'pdfs')
output_base_path = os.path.join(base_path, 'ocr_data')
transcriptions_path = os.path.join(base_path, 'transcriptions')
training_data_path = os.path.join(output_base_path, 'training_data')
results_path = os.path.join(output_base_path, 'results')

# Create main directories
os.makedirs(pdf_folder, exist_ok=True)
os.makedirs(os.path.join(output_base_path, "images"), exist_ok=True)
os.makedirs(os.path.join(output_base_path, "processed_images"), exist_ok=True)
os.makedirs(os.path.join(output_base_path, "binary_images"), exist_ok=True)
os.makedirs(training_data_path, exist_ok=True)
os.makedirs(transcriptions_path, exist_ok=True)
os.makedirs(results_path, exist_ok=True)

# List of subdirectories to create in binary_images, processed_images, and images
subdirectories = [
    "Buendia - Instruccion-1",
    "Constituciones sinodales Calahorra 1602-2",
    "Ezcaray - Vozes-3",
    "Mendo - Principe perfecto-4",
    "Paredes - Reglas generales-5",
    "PORCONES.228.35  1636-6"
]

# Define paths for binary_images, processed_images, and images directories
binary_images_path = os.path.join(output_base_path, "binary_images")
processed_images_path = os.path.join(output_base_path, "processed_images")
images_path = os.path.join(output_base_path, "images")

# Create subdirectories inside binary_images
for subdir in subdirectories:
    os.makedirs(os.path.join(binary_images_path, subdir), exist_ok=True)

# Create subdirectories inside processed_images
for subdir in subdirectories:
    os.makedirs(os.path.join(processed_images_path, subdir), exist_ok=True)

# Create subdirectories inside images
for subdir in subdirectories:
    os.makedirs(os.path.join(images_path, subdir), exist_ok=True)

print("Setup complete! All directories created.")

# Install required packages
!pip install transformers datasets torch torchvision tqdm pandas matplotlib tensorboard

"""# 2. Data Preparation"""

import os
import torch
from torch.utils.data import Dataset
from torchvision import transforms
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from PIL import Image
import logging

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Define data augmentation for training
def get_train_transforms():
    """
    Create augmentation transforms specifically designed for historical documents.
    """
    return transforms.Compose([
        transforms.RandomRotation(3),  # Slight rotation to simulate alignment variations
        transforms.RandomApply([
            transforms.ColorJitter(brightness=0.2, contrast=0.2)
        ], p=0.3),  # Simulate different document conditions
        transforms.RandomApply([
            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5))
        ], p=0.1),  # Simulate aged or blurry documents
        transforms.Resize((384, 384)),  # TrOCR expected size
        transforms.ToTensor(),
    ])

# Simpler transforms for validation
def get_val_transforms():
    return transforms.Compose([
        transforms.Resize((384, 384)),
        transforms.ToTensor(),
    ])

# Enhanced dataset class for historical documents
class HistoricalDocumentDataset(Dataset):
    """
    Dataset class for historical Spanish documents with proper error handling
    and preprocessing for TrOCR fine-tuning.
    """
    def __init__(self, page_data, processor, max_length=512, transform=None, augment=False):
        """
        Initialize the dataset.

        Args:
            page_data: Dictionary mapping (doc_name, page_num) to page data
            processor: TrOCR processor for image and text processing
            max_length: Maximum sequence length for text
            transform: Image transforms to apply
            augment: Whether to apply data augmentation
        """
        self.page_keys = list(page_data.keys())
        self.page_data = page_data
        self.processor = processor
        self.max_length = max_length
        self.transform = transform
        self.augment = augment

    def __len__(self):
        return len(self.page_keys)

    def __getitem__(self, idx):
        # Get data for the specified index
        key = self.page_keys[idx]
        data = self.page_data[key]

        # Load and process image with robust error handling
        image_path = data['processed_image']
        try:
            image = Image.open(image_path).convert('RGB')

            # Apply augmentation if enabled
            if self.augment and self.transform:
                image = self.transform(image)
            elif self.transform:
                image = self.transform(image)

            # Process image with TrOCR processor
            pixel_values = self.processor(image, return_tensors="pt").pixel_values
            # Remove batch dimension
            pixel_values = pixel_values.squeeze(0)

        except Exception as e:
            logger.error(f"Error loading image {image_path}: {e}")
            # Create a dummy tensor as fallback
            pixel_values = torch.zeros((3, 384, 384))

        # Get and process text
        text = data.get('text', "")
        if len(text) > self.max_length:
            text = text[:self.max_length]

        # Tokenize text
        encodings = self.processor.tokenizer(
            text,
            padding="max_length",
            max_length=self.max_length,
            truncation=True,
            return_tensors="pt"
        )

        labels = encodings.input_ids.squeeze(0)

        # Replace padding token id with -100 so it's ignored in loss calculation
        labels[labels == self.processor.tokenizer.pad_token_id] = -100

        return {
            "pixel_values": pixel_values,
            "labels": labels,
            "text": text,
            "doc_name": key[0],
            "page_num": key[1],
            "image_path": image_path
        }

"""
# 3. Model Initialization and Configuration"""

def initialize_model_for_fine_tuning():
    """
    Initialize and configure a TrOCR model specifically for historical printed documents.

    Returns:
        Tuple of (model, processor)
    """
    # For historical printed documents, the printed model variant works better than handwritten
    model_name = "microsoft/trocr-base-printed"
    logger.info(f"Loading TrOCR model: {model_name}")

    try:
        # Load processor and model
        processor = TrOCRProcessor.from_pretrained(model_name)
        model = VisionEncoderDecoderModel.from_pretrained(model_name)

        # Set essential model configuration
        model.config.decoder_start_token_id = processor.tokenizer.bos_token_id
        model.config.pad_token_id = processor.tokenizer.pad_token_id
        model.config.vocab_size = model.config.decoder.vocab_size

        # Set generation parameters
        model.config.max_length = 512  # Longer for historical documents
        model.config.early_stopping = True
        model.config.no_repeat_ngram_size = 3
        model.config.length_penalty = 2.0
        model.config.num_beams = 4

        # Move to GPU if available
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)

        logger.info(f"Model loaded successfully and moved to {device}")
        return model, processor

    except Exception as e:
        logger.error(f"Error loading model: {e}")
        raise

"""
# 4. Fine-Tuning Setup and Metrics"""

from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer
import numpy as np

def character_error_rate(reference, hypothesis):
    """
    Calculate character error rate between reference and hypothesis strings.

    Args:
        reference: Ground truth text
        hypothesis: Predicted text

    Returns:
        CER value (lower is better)
    """
    # Normalize texts
    reference = reference.lower()
    hypothesis = hypothesis.lower()

    # Handle empty references
    if len(reference) == 0:
        return 1.0 if len(hypothesis) > 0 else 0.0

    # Calculate edit distance
    edit_distance = levenshtein_distance(reference, hypothesis)

    # Calculate CER
    cer = edit_distance / len(reference)
    return cer

def word_error_rate(reference, hypothesis):
    """
    Calculate word error rate between reference and hypothesis strings.

    Args:
        reference: Ground truth text
        hypothesis: Predicted text

    Returns:
        WER value (lower is better)
    """
    # Normalize and split texts
    reference_words = reference.lower().split()
    hypothesis_words = hypothesis.lower().split()

    # Handle empty references
    if len(reference_words) == 0:
        return 1.0 if len(hypothesis_words) > 0 else 0.0

    # Calculate edit distance at word level
    edit_distance = levenshtein_distance(reference_words, hypothesis_words)

    # Calculate WER
    wer = edit_distance / len(reference_words)
    return wer

def levenshtein_distance(s1, s2):
    """
    Calculate Levenshtein (edit) distance between two sequences.
    Works with both strings (character-level) and lists (word-level).

    Args:
        s1: First sequence
        s2: Second sequence

    Returns:
        Edit distance value
    """
    # Ensure s1 is longer
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)

    # Fast path when s2 is empty
    if len(s2) == 0:
        return len(s1)

    # Initialize previous row
    previous_row = range(len(s2) + 1)

    # Calculate edit distance
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            # Calculate cost of operations
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)

            # Take minimum cost
            current_row.append(min(insertions, deletions, substitutions))

        # Update previous row
        previous_row = current_row

    return previous_row[-1]

def compute_metrics(pred):
    """
    Compute evaluation metrics for the model.

    Args:
        pred: Prediction object from Trainer

    Returns:
        Dictionary with metrics
    """
    # Get predictions and labels
    labels = pred.label_ids
    preds = pred.predictions

    # Replace -100 (ignore index) with pad token id for decoding
    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)

    # If the predictions are logits, get the predicted token ids
    if isinstance(preds, tuple):
        preds = preds[0]

    # Decode token ids to text
    decoded_preds = processor.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)

    # Compute metrics for each example
    cer_scores = []
    wer_scores = []

    for pred, label in zip(decoded_preds, decoded_labels):
        cer_scores.append(character_error_rate(label, pred))
        wer_scores.append(word_error_rate(label, pred))

    # Calculate average metrics
    avg_cer = np.mean(cer_scores)
    avg_wer = np.mean(wer_scores)

    # Return metrics
    return {
        "cer": avg_cer,
        "wer": avg_wer,
    }

"""# 5. Training Configuration"""

def get_training_args(output_dir, num_train_epochs=10, per_device_batch_size=4):
    """
    Create training arguments optimized for historical document OCR.

    Args:
        output_dir: Directory to save model checkpoints and logs
        num_train_epochs: Number of training epochs
        per_device_batch_size: Batch size per device

    Returns:
        Seq2SeqTrainingArguments object
    """
    return Seq2SeqTrainingArguments(
        output_dir=output_dir,
        evaluation_strategy="epoch",  # Evaluate after each epoch
        save_strategy="epoch",        # Save checkpoint after each epoch
        logging_strategy="steps",
        logging_steps=50,             # Log every 50 steps
        learning_rate=5e-5,           # Typical fine-tuning learning rate
        weight_decay=0.01,            # L2 regularization
        per_device_train_batch_size=per_device_batch_size,
        per_device_eval_batch_size=per_device_batch_size,
        gradient_accumulation_steps=4,  # Effective batch size = batch_size * gradient_accumulation_steps
        num_train_epochs=num_train_epochs,
        warmup_ratio=0.1,             # Warm up learning rate over 10% of steps
        fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available
        generation_max_length=512,    # Maximum generation length for evaluation
        predict_with_generate=True,   # Use generation for evaluation
        load_best_model_at_end=True,  # Load best model at end of training
        metric_for_best_model="cer",  # Use CER as metric for best model
        greater_is_better=False,      # Lower CER is better
        report_to="tensorboard",      # Report metrics to TensorBoard
        save_total_limit=2,           # Only keep 2 latest checkpoints
    )

"""6. Complete Fine-Tuning Function"""

def fine_tune_trocr(train_data, val_data, output_dir, num_epochs=10, batch_size=4):
    """
    Fine-tune a TrOCR model on historical Spanish documents.

    Args:
        train_data: Dictionary with training data
        val_data: Dictionary with validation data
        output_dir: Directory to save model and results
        num_epochs: Number of training epochs
        batch_size: Batch size per device

    Returns:
        Tuple of (model, processor, evaluation_results)
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Initialize model and processor
    logger.info("Initializing model and processor...")
    model, processor = initialize_model_for_fine_tuning()

    # Create datasets with appropriate transformations
    logger.info("Creating datasets...")
    train_dataset = HistoricalDocumentDataset(
        train_data,
        processor,
        transform=get_train_transforms(),
        augment=True
    )

    val_dataset = HistoricalDocumentDataset(
        val_data,
        processor,
        transform=get_val_transforms(),
        augment=False
    )

    logger.info(f"Train dataset size: {len(train_dataset)}")
    logger.info(f"Validation dataset size: {len(val_dataset)}")

    # Set up training arguments
    logger.info("Setting up training arguments...")
    training_args = get_training_args(
        output_dir=output_dir,
        num_train_epochs=num_epochs,
        per_device_batch_size=batch_size
    )

    # Set up data collator to properly batch the data
    def collate_fn(examples):
        # Extract pixel values and labels from examples
        pixel_values = torch.stack([example["pixel_values"] for example in examples])
        labels = torch.stack([example["labels"] for example in examples])

        return {
            "pixel_values": pixel_values,
            "labels": labels,
        }

    # Initialize trainer
    logger.info("Initializing trainer...")
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
        tokenizer=processor.tokenizer,
        data_collator=collate_fn
    )

    # Add a callback to log example predictions
    class PredictionLogger(transformers.TrainerCallback):
        def on_evaluate(self, args, state, control, metrics=None, **kwargs):
            # Get model and processor from trainer
            model = trainer.model
            processor = trainer.tokenizer.processor

            # Log a few example predictions
            logger.info("Example predictions:")
            for i in range(min(3, len(val_dataset))):
                sample = val_dataset[i]

                # Get image and text
                pixel_values = sample["pixel_values"].unsqueeze(0).to(model.device)
                text = sample["text"]

                # Generate prediction
                with torch.no_grad():
                    generated_ids = model.generate(pixel_values, max_length=512)

                # Decode prediction
                pred = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

                # Log results
                doc_name = sample["doc_name"]
                page_num = sample["page_num"]
                logger.info(f"Document: {doc_name}, Page: {page_num}")
                logger.info(f"Ground truth: {text[:100]}...")
                logger.info(f"Prediction: {pred[:100]}...")
                logger.info(f"CER: {character_error_rate(text, pred):.4f}")
                logger.info("---")

    # Train model with progress tracking
    logger.info("Starting fine-tuning...")
    try:
        trainer.add_callback(PredictionLogger())
        trainer.train()
        logger.info("Fine-tuning completed successfully.")
    except Exception as e:
        logger.error(f"Error during fine-tuning: {e}")
        raise

    # Evaluate model on full validation set
    logger.info("Evaluating final model...")
    eval_results = trainer.evaluate()
    logger.info(f"Final evaluation results: {eval_results}")

    # Save final model
    final_output_dir = os.path.join(output_dir, "final-model")
    logger.info(f"Saving final model to {final_output_dir}")
    model.save_pretrained(final_output_dir)
    processor.save_pretrained(final_output_dir)

    return model, processor, eval_results

"""# 7. Main Execution Script"""

# Add the following line at the beginning of the script, before any other import statements:
import sys
import os
sys.path.append(os.path.join(os.getcwd(), '..'))  # Add the parent directory to the search path

# This line adds the parent directory (..) to the Python search path, which allows Python to find the 'find_all_images' module if it's located in the directory one level above your current working directory.

"""
Fine-Tuning Script for TrOCR on Historical Spanish Documents

This script provides a complete end-to-end solution for fine-tuning
a TrOCR model on historical Spanish documents.
"""

import os
import glob
import re
import logging
import random
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt
from transformers import (
    TrOCRProcessor,
    VisionEncoderDecoderModel,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    TrainerCallback
)

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Set random seed for reproducibility
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)

# Check device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"Using device: {device}")

#---------------------------------------------------------------------------
# Utility Functions
#---------------------------------------------------------------------------

def normalize_document_name(doc_name):
    """
    Normalize a document name by removing common suffixes and standardizing format.

    Args:
        doc_name: Raw document name that might include page numbers or other suffixes

    Returns:
        Normalized document name
    """
    # Remove page number suffixes like "-1", "-2", etc.
    doc_name = re.sub(r'-\d+$', '', doc_name)

    # Remove additional identifiers that might be in document names
    doc_name = re.sub(r' - Instruccion$', '', doc_name)
    doc_name = re.sub(r' - Principe perfecto$', '', doc_name)
    doc_name = re.sub(r' - Vozes$', '', doc_name)
    doc_name = re.sub(r' - Reglas generales$', '', doc_name)
    doc_name = re.sub(r' sinodales Calahorra 1602$', ' sinodales', doc_name)

    return doc_name.strip()

def get_best_transcription_match(doc_name, available_transcriptions):
    """
    Find the best matching transcription filename for a document name using fuzzy matching.

    Args:
        doc_name: Document name to match
        available_transcriptions: List of available transcription filenames (without extensions)

    Returns:
        Best matching transcription name or None if no good match found
    """
    if not available_transcriptions:
        return None

    # Normalize document name
    normalized_doc_name = normalize_document_name(doc_name)

    # Try exact match first with variations
    for transcription in available_transcriptions:
        # Try direct match
        if normalized_doc_name == transcription:
            return transcription

        # Try without "transcription" suffix
        if normalized_doc_name == transcription.replace(' transcription', ''):
            return transcription

    # If no exact match, try fuzzy matching
    best_match = None
    best_ratio = 0.0

    for transcription in available_transcriptions:
        # Compare normalized doc name with transcription name (without "transcription" suffix)
        base_transcription = transcription.replace(' transcription', '')

        # Calculate similarity ratio
        from difflib import SequenceMatcher
        ratio = SequenceMatcher(None, normalized_doc_name.lower(), base_transcription.lower()).ratio()

        # Also try with the full transcription name
        full_ratio = SequenceMatcher(None, normalized_doc_name.lower(), transcription.lower()).ratio()
        ratio = max(ratio, full_ratio)

        if ratio > best_ratio and ratio > 0.6:  # 0.6 is a threshold for good matches
            best_ratio = ratio
            best_match = transcription

    if best_match:
        logger.info(f"Fuzzy matched '{doc_name}' to '{best_match}' with confidence {best_ratio:.2f}")

    return best_match

def levenshtein_distance(s1, s2):
    """
    Calculate Levenshtein (edit) distance between two sequences.
    Works with both strings (character-level) and lists (word-level).

    Args:
        s1: First sequence
        s2: Second sequence

    Returns:
        Edit distance value
    """
    # Ensure s1 is longer
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)

    # Fast path when s2 is empty
    if len(s2) == 0:
        return len(s1)

    # Initialize previous row
    previous_row = list(range(len(s2) + 1))

    # Calculate edit distance
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            # Calculate cost of operations
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)

            # Take minimum cost
            current_row.append(min(insertions, deletions, substitutions))

        # Update previous row
        previous_row = current_row

    return previous_row[-1]

def character_error_rate(reference, hypothesis):
    """
    Calculate character error rate between reference and hypothesis strings.

    Args:
        reference: Ground truth text
        hypothesis: Predicted text

    Returns:
        CER value (lower is better)
    """
    # Normalize texts
    reference = reference.lower()
    hypothesis = hypothesis.lower()

    # Handle empty references
    if len(reference) == 0:
        return 1.0 if len(hypothesis) > 0 else 0.0

    # Calculate edit distance
    edit_distance = levenshtein_distance(reference, hypothesis)

    # Calculate CER
    cer = edit_distance / len(reference)
    return cer

def word_error_rate(reference, hypothesis):
    """
    Calculate word error rate between reference and hypothesis strings.

    Args:
        reference: Ground truth text
        hypothesis: Predicted text

    Returns:
        WER value (lower is better)
    """
    # Normalize and split texts
    reference_words = reference.lower().split()
    hypothesis_words = hypothesis.lower().split()

    # Handle empty references
    if len(reference_words) == 0:
        return 1.0 if len(hypothesis_words) > 0 else 0.0

    # Calculate edit distance at word level
    edit_distance = levenshtein_distance(reference_words, hypothesis_words)

    # Calculate WER
    wer = edit_distance / len(reference_words)
    return wer

def compute_metrics(pred):
    """
    Compute evaluation metrics for the model.

    Args:
        pred: Prediction object from Trainer

    Returns:
        Dictionary with metrics
    """
    global processor  # Use the global processor variable

    # Get predictions and labels
    labels = pred.label_ids
    preds = pred.predictions

    # Replace -100 (ignore index) with pad token id for decoding
    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)

    # If the predictions are logits, get the predicted token ids
    if isinstance(preds, tuple):
        preds = preds[0]

    # Decode token ids to text
    decoded_preds = processor.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)

    # Compute metrics for each example
    cer_scores = []
    wer_scores = []

    for pred, label in zip(decoded_preds, decoded_labels):
        cer_scores.append(character_error_rate(label, pred))
        wer_scores.append(word_error_rate(label, pred))

    # Calculate average metrics
    avg_cer = np.mean(cer_scores)
    avg_wer = np.mean(wer_scores)

    # Return metrics
    return {
        "cer": avg_cer,
        "wer": avg_wer,
    }

def create_train_val_split(page_transcriptions, val_ratio=0.2, seed=42):
    """
    Split the page transcriptions into training and validation sets.

    Args:
        page_transcriptions: Dictionary mapping (doc_name, page_num) to page data
        val_ratio: Ratio of validation data
        seed: Random seed for reproducibility

    Returns:
        Tuple of (train_data, val_data)
    """
    # Set random seed
    random.seed(seed)

    # Get all keys (document_name, page_number) tuples
    keys = list(page_transcriptions.keys())

    # Shuffle the keys
    random.shuffle(keys)

    # Calculate split index
    split_idx = int(len(keys) * (1 - val_ratio))

    # Split keys
    train_keys = keys[:split_idx]
    val_keys = keys[split_idx:]

    # Create dictionaries
    train_data = {k: page_transcriptions[k] for k in train_keys}
    val_data = {k: page_transcriptions[k] for k in val_keys}

    logger.info(f"Train set: {len(train_data)} pages")
    logger.info(f"Validation set: {len(val_data)} pages")

    return train_data, val_data

#---------------------------------------------------------------------------
# Transcription Loader Classes
#---------------------------------------------------------------------------

class TranscriptionLoader:
    """
    A class for loading transcriptions from various file formats (.txt, .docx)
    and mapping them to corresponding document images.
    """

    def __init__(self, transcription_dir, page_transcription_dir):
        """
        Initialize the transcription loader.

        Args:
            transcription_dir: Directory containing original transcription files
            page_transcription_dir: Directory to store page-level transcriptions
        """
        self.transcription_dir = transcription_dir
        self.page_transcription_dir = page_transcription_dir
        os.makedirs(page_transcription_dir, exist_ok=True)

        # Mapping from document names to transcription files
        self.doc_transcription_map = {}
        # List of available transcription basenames (without extensions)
        self.available_transcriptions = []

        # Format handlers
        self.format_handlers = {
            '.txt': self._read_txt_file,
            '.docx': self._read_docx_file
        }

    def find_transcription_files(self):
        """
        Find all transcription files in the transcription directory.

        Returns:
            Dictionary mapping document names to transcription file paths
        """
        # Clear existing maps
        self.doc_transcription_map = {}
        self.available_transcriptions = []

        for file in os.listdir(self.transcription_dir):
            file_path = os.path.join(self.transcription_dir, file)
            if not os.path.isfile(file_path):
                continue

            file_ext = os.path.splitext(file)[1].lower()
            if file_ext in self.format_handlers:
                # Get document name without extension
                doc_name = os.path.splitext(file)[0]
                self.doc_transcription_map[doc_name] = file_path
                self.available_transcriptions.append(doc_name)

        if self.doc_transcription_map:
            logger.info(f"Found {len(self.doc_transcription_map)} transcription files")
            logger.info(f"Available transcriptions: {', '.join(self.available_transcriptions)}")
        else:
            logger.warning("No transcription files found in directory: " + self.transcription_dir)

        return self.doc_transcription_map

    def _read_txt_file(self, file_path):
        """
        Read text content from a .txt file.

        Args:
            file_path: Path to the .txt file

        Returns:
            Text content as a string
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            # Try different encodings if utf-8 fails
            encodings = ['latin-1', 'iso-8859-1', 'cp1252']
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        logger.warning(f"File {file_path} decoded using {encoding} instead of utf-8")
                        return f.read()
                except UnicodeDecodeError:
                    continue

            # If all else fails, use binary mode and ignore errors
            logger.error(f"Could not decode {file_path} with common encodings, using binary mode")
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read()

    def _read_docx_file(self, file_path):
        """
        Read text content from a .docx file.

        Args:
            file_path: Path to the .docx file

        Returns:
            Text content as a string
        """
        try:
            # Try to import python-docx
            try:
                import docx
            except ImportError:
                logger.warning("python-docx not installed. Installing...")
                try:
                    import subprocess
                    subprocess.check_call(["pip", "install", "python-docx"])
                    import docx
                except Exception as e:
                    logger.error(f"Failed to install python-docx: {e}")
                    return f"ERROR: Could not read {file_path} (python-docx not available)"

            # Read the docx file
            doc = docx.Document(file_path)
            full_text = []

            # Extract text from paragraphs
            for para in doc.paragraphs:
                full_text.append(para.text)

            # Also check for tables, which might contain transcription data
            for table in doc.tables:
                for row in table.rows:
                    row_texts = [cell.text for cell in row.cells if cell.text.strip()]
                    if row_texts:
                        full_text.append(' | '.join(row_texts))

            return '\n'.join(full_text)
        except Exception as e:
            logger.error(f"Error reading .docx file {file_path}: {str(e)}")
            return f"ERROR: Could not read {file_path}"

    def read_transcription(self, doc_name):
        """
        Read transcription for a specific document, handling name variations.

        Args:
            doc_name: Document name (will be normalized and matched)

        Returns:
            Transcription content or None if not found
        """
        # Find best matching transcription
        best_match = get_best_transcription_match(doc_name, self.available_transcriptions)

        if best_match and best_match in self.doc_transcription_map:
            file_path = self.doc_transcription_map[best_match]
            file_ext = os.path.splitext(file_path)[1].lower()

            if file_ext in self.format_handlers:
                logger.info(f"Found transcription for '{doc_name}' -> '{best_match}'")
                return self.format_handlers[file_ext](file_path)

        logger.warning(f"No transcription found for document {doc_name}")
        return None

#---------------------------------------------------------------------------
# Image Finding and Processing Functions
#---------------------------------------------------------------------------

def extract_doc_info(image_path):
    """
    Extract document name and page number from image path.

    Args:
        image_path: Path to the image file

    Returns:
        Tuple of (document_name, page_number) or (None, None) if not parsable
    """
    # Extract filename from path
    filename = os.path.basename(image_path)
    # Remove extension
    filename_without_ext = os.path.splitext(filename)[0]

    # Try different patterns

    # Pattern 1: Document_Name_page_XXX.jpg
    match = re.match(r'(.+?)_page_(\d+)', filename_without_ext)
    if match:
        doc_name = match.group(1)
        page_num = int(match.group(2))
        return doc_name, page_num

    # Pattern 2: Detect document name and page number from directory structure
    parent_dir = os.path.basename(os.path.dirname(image_path))
    if parent_dir and filename_without_ext.endswith(('-1', '-2', '-3', '-4', '-5', '-6')):
        # Extract page number from the end of filename
        page_match = re.search(r'-(\d+)$', filename_without_ext)
        if page_match:
            page_num = int(page_match.group(1))
            # Use parent directory as document name
            return parent_dir, page_num

    # Try to extract from arbitrary filename with page number at the end
    page_match = re.search(r'[-_]p(?:age)?[-_]?(\d+)$', filename_without_ext, re.IGNORECASE)
    if page_match:
        page_num = int(page_match.group(1))
        # Remove page suffix from filename to get document name
        doc_name = re.sub(r'[-_]p(?:age)?[-_]?\d+$', '', filename_without_ext, flags=re.IGNORECASE)
        return doc_name, page_num

    logger.warning(f"Could not parse document name and page from: {image_path}")
    return None, None

def find_all_images(base_path='/content'):
    """
    Find all processed and binary images in the specified directories.

    Args:
        base_path: Base directory for all data

    Returns:
        Tuple of (processed_images, binary_images, image_mapping)
    """
    output_base_path = os.path.join(base_path, 'ocr_data')
    processed_dir = os.path.join(output_base_path, "processed_images")
    binary_dir = os.path.join(output_base_path, "binary_images")

    processed_images = []
    binary_images = []
    image_mapping = {}  # Maps (doc_name, page_num) to {'processed': path, 'binary': path}

    # Track document names to help with debugging
    found_doc_names = set()
    doc_page_counts = {}

    # Find processed images
    if os.path.exists(processed_dir):
        for doc_dir in os.listdir(processed_dir):
            doc_path = os.path.join(processed_dir, doc_dir)
            if os.path.isdir(doc_path):
                for img_file in os.listdir(doc_path):
                    if img_file.endswith(('.jpg', '.png')):
                        img_path = os.path.join(doc_path, img_file)
                        processed_images.append(img_path)

                        # Try to extract document name and page number
                        doc_name, page_num = extract_doc_info(img_path)

                        # If that fails, use the directory name as document name
                        # and generate a sequential page number
                        if doc_name is None or page_num is None:
                            doc_name = doc_dir
                            if doc_name not in doc_page_counts:
                                doc_page_counts[doc_name] = 0
                            doc_page_counts[doc_name] += 1
                            page_num = doc_page_counts[doc_name]

                        found_doc_names.add(doc_name)
                        if (doc_name, page_num) not in image_mapping:
                            image_mapping[(doc_name, page_num)] = {'processed': img_path}
    else:
        logger.warning(f"Processed images directory not found: {processed_dir}")

    # Find binary images
    if os.path.exists(binary_dir):
        for doc_dir in os.listdir(binary_dir):
            doc_path = os.path.join(binary_dir, doc_dir)
            if os.path.isdir(doc_path):
                for img_file in os.listdir(doc_path):
                    if img_file.endswith(('.jpg', '.png')):
                        img_path = os.path.join(doc_path, img_file)
                        binary_images.append(img_path)

                        # Extract document name and page number
                        doc_name, page_num = extract_doc_info(img_path)

                        # If that fails, use the directory name as document name
                        # and try to match with an existing processed image
                        if doc_name is None or page_num is None:
                            doc_name = doc_dir
                            # Try to find matching processed image
                            for (d, p), mapping in image_mapping.items():
                                if d == doc_name and os.path.basename(mapping.get('processed', '')) == img_file:
                                    page_num = p
                                    break

                            # If still can't find, generate a sequential page number
                            if page_num is None:
                                if doc_name not in doc_page_counts:
                                    doc_page_counts[doc_name] = 0
                                doc_page_counts[doc_name] += 1
                                page_num = doc_page_counts[doc_name]

                        found_doc_names.add(doc_name)
                        if (doc_name, page_num) in image_mapping:
                            image_mapping[(doc_name, page_num)]['binary'] = img_path
                        else:
                            image_mapping[(doc_name, page_num)] = {'binary': img_path}
    else:
        logger.warning(f"Binary images directory not found: {binary_dir}")

    # Log what we found
    logger.info(f"Found {len(processed_images)} processed images and {len(binary_images)} binary images")
    logger.info(f"Identified {len(found_doc_names)} unique documents: {', '.join(sorted(found_doc_names))}")
    logger.info(f"Created {len(image_mapping)} document-page mappings")

    return processed_images, binary_images, image_mapping

def create_page_transcriptions_modified(loader, image_mapping, max_pages_per_doc=None):
    """
    Create page-level transcriptions based on image mapping and full transcriptions.

    Args:
        loader: Instance of TranscriptionLoader
        image_mapping: Dictionary mapping (doc_name, page_num) to image paths
        max_pages_per_doc: Optional limit on pages per document

    Returns:
        Dictionary mapping (doc_name, page_num) to page data including transcription
    """
    page_transcriptions_output = {}
    doc_names = sorted(list(set(doc_name for doc_name, _ in image_mapping.keys())))
    logger.info(f"Attempting to create page transcriptions for {len(doc_names)} documents.")

    for doc_name in doc_names:
        full_transcription = loader.read_transcription(doc_name)
        if not full_transcription:
            logger.warning(f"No full transcription found for {doc_name}, skipping page creation.")
            continue

        # Find pages relevant to this document in the image mapping
        doc_pages = sorted([page_num for d, page_num in image_mapping.keys() if d == doc_name])
        if max_pages_per_doc:
            doc_pages = [p for p in doc_pages if p <= max_pages_per_doc]

        if not doc_pages:
            logger.warning(f"No image pages found for {doc_name} in mapping.")
            continue

        logger.info(f"Processing {doc_name}, found pages: {doc_pages}")

        # --- Page Splitting Logic ---
        # Try to find page markers like "PDF pX" or similar
        page_markers = list(re.finditer(r'(?:PDF p|Page |p)(\d+)', full_transcription, re.IGNORECASE))

        if not page_markers:
            logger.warning(f"No standard page markers found in transcription for {doc_name}. Attempting naive split.")
            # --- Naive Split (Fallback - Less Accurate) ---
            lines = full_transcription.split('\n')
            total_lines = len(lines)
            num_found_pages = len(doc_pages)
            lines_per_page = max(1, total_lines // num_found_pages if num_found_pages > 0 else total_lines)

            for i, page_num in enumerate(doc_pages):
                key = (doc_name, page_num)
                if key not in image_mapping: continue

                start_line = i * lines_per_page
                end_line = min((i + 1) * lines_per_page, total_lines) if i < num_found_pages - 1 else total_lines
                page_text = '\n'.join(lines[start_line:end_line]).strip()

                if not page_text:
                    logger.warning(f"Empty page text generated for {key} via naive split.")

                # Make sure we have a processed_image key in the mapping
                if 'processed' not in image_mapping[key]:
                    logger.warning(f"No processed image for {key}")
                    continue

                page_transcriptions_output[key] = {
                    'text': page_text,
                    'processed_image': image_mapping[key].get('processed'),
                    'binary_image': image_mapping[key].get('binary'),
                    'transcription_file': loader.doc_transcription_map.get(
                        get_best_transcription_match(doc_name, loader.available_transcriptions), '')
                }

        else:
            # --- Split by Markers ---
            current_page_num = 0
            start_index = 0
            marker_map = {}  # Map marker page num to text start/end

            # Assume first page starts at the beginning unless marker says otherwise
            first_marker_page = int(page_markers[0].group(1))
            if first_marker_page > 1:
                marker_map[1] = (0, page_markers[0].start())

            for i, marker in enumerate(page_markers):
                page_num_marker = int(marker.group(1))
                start_index = marker.end()  # Text starts after the marker usually

                if i + 1 < len(page_markers):
                    end_index = page_markers[i+1].start()
                else:
                    end_index = len(full_transcription)  # To the end of the document

                marker_map[page_num_marker] = (start_index, end_index)

            # Assign text based on mapped markers and found image pages
            for page_num in doc_pages:
                key = (doc_name, page_num)
                if key not in image_mapping: continue

                if page_num in marker_map:
                    start_idx, end_idx = marker_map[page_num]
                    page_text = full_transcription[start_idx:end_idx].strip()

                    # Basic cleanup: remove potential leading/trailing whitespace or markers
                    page_text = re.sub(r'^(?:PDF p|Page |p)\d+\s*', '', page_text, flags=re.IGNORECASE).strip()

                    if not page_text:
                        logger.warning(f"Empty page text generated for {key} using markers.")

                    # Make sure we have a processed_image key in the mapping
                    if 'processed' not in image_mapping[key]:
                        logger.warning(f"No processed image for {key}")
                        continue

                    page_transcriptions_output[key] = {
                        'text': page_text,
                        'processed_image': image_mapping[key].get('processed'),
                        'binary_image': image_mapping[key].get('binary'),
                        'transcription_file': loader.doc_transcription_map.get(
                            get_best_transcription_match(doc_name, loader.available_transcriptions), '')
                    }
                else:
                    logger.warning(f"No specific marker found for page {page_num} in {doc_name}. Skipping.")

    logger.info(f"Created {len(page_transcriptions_output)} page-level transcriptions entries.")
    return page_transcriptions_output

#---------------------------------------------------------------------------
# Data Augmentation and Dataset Classes
#---------------------------------------------------------------------------

def get_train_transforms():
    """
    Create augmentation transforms specifically designed for historical documents.
    """
    return transforms.Compose([
        transforms.RandomRotation(3),  # Slight rotation to simulate alignment variations
        transforms.RandomApply([
            transforms.ColorJitter(brightness=0.2, contrast=0.2)
        ], p=0.3),  # Simulate different document conditions
        transforms.RandomApply([
            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5))
        ], p=0.1),  # Simulate aged or blurry documents
        transforms.Resize((384, 384)),  # TrOCR expected size
        transforms.ToTensor(),
    ])

def get_val_transforms():
    """
    Simple transforms for validation data.
    """
    return transforms.Compose([
        transforms.Resize((384, 384)),  # TrOCR expected size
        transforms.ToTensor(),
    ])

class HistoricalDocumentDataset(Dataset):
    """
    Dataset class for historical Spanish documents with proper error handling
    and preprocessing for TrOCR fine-tuning.
    """
    def __init__(self, page_data, processor, max_length=512, transform=None, augment=False):
        """
        Initialize the dataset.

        Args:
            page_data: Dictionary mapping (doc_name, page_num) to page data
            processor: TrOCR processor for image and text processing
            max_length: Maximum sequence length for text
            transform: Image transforms to apply
            augment: Whether to apply data augmentation
        """
        self.page_keys = list(page_data.keys())
        self.page_data = page_data
        self.processor = processor
        self.max_length = max_length
        self.transform = transform
        self.augment = augment

    def __len__(self):
        return len(self.page_keys)

    def __getitem__(self, idx):
        # Get data for the specified index
        key = self.page_keys[idx]
        data = self.page_data[key]

        # Load and process image with robust error handling
        image_path = data['processed_image']
        try:
            image = Image.open(image_path).convert('RGB')

            # Apply augmentation if enabled
            if self.augment and self.transform:
                image = self.transform(image)
            elif self.transform:
                image = self.transform(image)

            # Process image with TrOCR processor
            pixel_values = self.processor(image, return_tensors="pt").pixel_values
            # Remove batch dimension
            pixel_values = pixel_values.squeeze(0)

        except Exception as e:
            logger.error(f"Error loading image {image_path}: {e}")
            # Create a dummy tensor as fallback
            pixel_values = torch.zeros((3, 384, 384))

        # Get and process text
        text = data.get('text', "")
        if len(text) > self.max_length:
            text = text[:self.max_length]

        # Tokenize text
        encodings = self.processor.tokenizer(
            text,
            padding="max_length",
            max_length=self.max_length,
            truncation=True,
            return_tensors="pt"
        )

        labels = encodings.input_ids.squeeze(0)

        # Replace padding token id with -100 so it's ignored in loss calculation
        labels[labels == self.processor.tokenizer.pad_token_id] = -100

        return {
            "pixel_values": pixel_values,
            "labels": labels,
            "text": text,
            "doc_name": key[0],
            "page_num": key[1],
            "image_path": image_path
        }

#---------------------------------------------------------------------------
# Model Initialization and Training Functions
#---------------------------------------------------------------------------

def initialize_model_for_fine_tuning():
    """
    Initialize and configure a TrOCR model specifically for historical printed documents.

    Returns:
        Tuple of (model, processor)
    """
    # For historical printed documents, the printed model variant works better than handwritten
    model_name = "microsoft/trocr-base-printed"
    logger.info(f"Loading TrOCR model: {model_name}")

    try:
        # Load processor and model
        processor = TrOCRProcessor.from_pretrained(model_name)
        model = VisionEncoderDecoderModel.from_pretrained(model_name)

        # Set essential model configuration
        model.config.decoder_start_token_id = processor.tokenizer.bos_token_id
        model.config.pad_token_id = processor.tokenizer.pad_token_id
        model.config.vocab_size = model.config.decoder.vocab_size

        # Set generation parameters
        model.config.max_length = 512  # Longer for historical documents
        model.config.early_stopping = True
        model.config.no_repeat_ngram_size = 3
        model.config.length_penalty = 2.0
        model.config.num_beams = 4

        # Move to GPU if available
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)

        logger.info(f"Model loaded successfully and moved to {device}")
        return model, processor

    except Exception as e:
        logger.error(f"Error loading model: {e}")
        raise

def get_training_args(output_dir, num_train_epochs=10, per_device_batch_size=4):
    """
    Create training arguments optimized for historical document OCR.

    Args:
        output_dir: Directory to save model checkpoints and logs
        num_train_epochs: Number of training epochs
        per_device_batch_size: Batch size per device

    Returns:
        Seq2SeqTrainingArguments object
    """
    return Seq2SeqTrainingArguments(
        output_dir=output_dir,
        evaluation_strategy="epoch",  # Evaluate after each epoch
        save_strategy="epoch",        # Save checkpoint after each epoch
        logging_strategy="steps",
        logging_steps=50,             # Log every 50 steps
        learning_rate=5e-5,           # Typical fine-tuning learning rate
        weight_decay=0.01,            # L2 regularization
        per_device_train_batch_size=per_device_batch_size,
        per_device_eval_batch_size=per_device_batch_size,
        gradient_accumulation_steps=4,  # Effective batch size = batch_size * gradient_accumulation_steps
        num_train_epochs=num_train_epochs,
        warmup_ratio=0.1,             # Warm up learning rate over 10% of steps
        fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available
        generation_max_length=512,    # Maximum generation length for evaluation
        predict_with_generate=True,   # Use generation for evaluation
        load_best_model_at_end=True,  # Load best model at end of training
        metric_for_best_model="cer",  # Use CER as metric for best model
        greater_is_better=False,      # Lower CER is better
        report_to="tensorboard",      # Report metrics to TensorBoard
        save_total_limit=2,           # Only keep 2 latest checkpoints
    )

class PredictionLoggerCallback(TrainerCallback):
    """
    Callback to log example predictions during training.
    """
    def __init__(self, dataset, processor, device):
        self.dataset = dataset
        self.processor = processor
        self.device = device

    def on_evaluate(self, args, state, control, **kwargs):
        # Only log predictions occasionally to avoid cluttering logs
        if state.epoch % 2 != 0:  # Log every other epoch
            return

        # Get model from trainer
        model = kwargs.get('model', None)
        if model is None:
            return

        # Log a few example predictions
        logger.info(f"Example predictions at epoch {state.epoch}:")

        # Use first 3 examples
        num_examples = min(3, len(self.dataset))
        for i in range(num_examples):
            sample = self.dataset[i]

            # Get image and text
            pixel_values = sample["pixel_values"].unsqueeze(0).to(self.device)
            text = sample["text"]

            # Generate prediction
            with torch.no_grad():
                generated_ids = model.generate(pixel_values, max_length=512)

            # Decode prediction
            pred = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

            # Log results
            doc_name = sample["doc_name"]
            page_num = sample["page_num"]
            logger.info(f"Document: {doc_name}, Page: {page_num}")
            logger.info(f"Ground truth: {text[:100]}...")
            logger.info(f"Prediction: {pred[:100]}...")
            logger.info(f"CER: {character_error_rate(text, pred):.4f}")
            logger.info("---")

def optimize_memory_usage(model, training_args):
    """
    Apply memory optimization techniques to the model and training arguments.

    Args:
        model: The TrOCR model
        training_args: Training arguments

    Returns:
        Updated model and training arguments
    """
    # 1. Enable gradient checkpointing (trades memory for computation)
    model.gradient_checkpointing_enable()

    # 2. Adjust batch size and gradient accumulation
    if torch.cuda.get_device_properties(0).total_memory < 10e9:  # Less than 10GB VRAM
        training_args.per_device_train_batch_size = 2  # Reduce batch size
        training_args.gradient_accumulation_steps = 8  # Increase gradient accumulation

    # 3. Use fp16 mixed precision training
    training_args.fp16 = torch.cuda.is_available()

    # 4. Reduce training precision
    # (only for highest memory pressure scenarios - may affect performance)
    if torch.cuda.get_device_properties(0).total_memory < 8e9:  # Less than 8GB VRAM
        training_args.fp16_full_eval = True

        # 5. Freeze encoder layers to reduce memory
        # Only fine-tune the decoder and last few encoder layers
        for param in model.encoder.parameters():
            param.requires_grad = False

        # Unfreeze only the last two encoder layers
        if hasattr(model.encoder, 'layer'):
            for param in model.encoder.layer[-2:].parameters():
                param.requires_grad = True

    return model, training_args

def fine_tune_trocr(train_data, val_data, output_dir, num_epochs=10, batch_size=4):
    """
    Fine-tune a TrOCR model on historical Spanish documents.

    Args:
        train_data: Dictionary with training data
        val_data: Dictionary with validation data
        output_dir: Directory to save model and results
        num_epochs: Number of training epochs
        batch_size: Batch size per device

    Returns:
        Tuple of (model, processor, evaluation_results)
    """
    global processor  # Make processor global for compute_metrics function

    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Initialize model and processor
    logger.info("Initializing model and processor...")
    model, processor = initialize_model_for_fine_tuning()

    # Create datasets with appropriate transformations
    logger.info("Creating datasets...")
    train_dataset = HistoricalDocumentDataset(
        train_data,
        processor,
        transform=get_train_transforms(),
        augment=True
    )

    val_dataset = HistoricalDocumentDataset(
        val_data,
        processor,
        transform=get_val_transforms(),
        augment=False
    )

    logger.info(f"Train dataset size: {len(train_dataset)}")
    logger.info(f"Validation dataset size: {len(val_dataset)}")

    # Set up training arguments
    logger.info("Setting up training arguments...")
    training_args = get_training_args(
        output_dir=output_dir,
        num_train_epochs=num_epochs,
        per_device_batch_size=batch_size
    )

    # Apply memory optimizations if needed
    if torch.cuda.is_available():
        logger.info("Applying memory optimizations...")
        model, training_args = optimize_memory_usage(model, training_args)

    # Set up data collator to properly batch the data
    def collate_fn(examples):
        # Extract pixel values and labels from examples
        pixel_values = torch.stack([example["pixel_values"] for example in examples])
        labels = torch.stack([example["labels"] for example in examples])

        return {
            "pixel_values": pixel_values,
            "labels": labels,
        }

    # Initialize trainer
    logger.info("Initializing trainer...")
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
        tokenizer=processor.tokenizer,
        data_collator=collate_fn
    )

    # Add prediction logger callback
    prediction_logger = PredictionLoggerCallback(
        dataset=val_dataset,
        processor=processor,
        device=device
    )
    trainer.add_callback(prediction_logger)

    # Train model with progress tracking
    logger.info("Starting fine-tuning...")
    try:
        trainer.train()
        logger.info("Fine-tuning completed successfully.")
    except Exception as e:
        logger.error(f"Error during fine-tuning: {e}")
        raise

    # Evaluate model on full validation set
    logger.info("Evaluating final model...")
    eval_results = trainer.evaluate()
    logger.info(f"Final evaluation results: {eval_results}")

    # Save final model
    final_output_dir = os.path.join(output_dir, "final-model")
    logger.info(f"Saving final model to {final_output_dir}")
    model.save_pretrained(final_output_dir)
    processor.save_pretrained(final_output_dir)

    return model, processor, eval_results

#---------------------------------------------------------------------------
# Main Function
#---------------------------------------------------------------------------

def main():
    """
    Main function to run the fine-tuning process.
    """
    # Define base paths
    base_path = '/content'
    output_base_path = os.path.join(base_path, 'ocr_data')
    transcriptions_path = os.path.join(base_path, 'transcriptions')
    fine_tuned_model_dir = os.path.join(output_base_path, "fine_tuned_model")
    page_transcription_dir = os.path.join(output_base_path, 'page_transcriptions')

    # Make sure directories exist
    os.makedirs(output_base_path, exist_ok=True)
    os.makedirs(transcriptions_path, exist_ok=True)
    os.makedirs(fine_tuned_model_dir, exist_ok=True)
    os.makedirs(page_transcription_dir, exist_ok=True)

    # 1. Find images
    logger.info("Finding images and transcriptions...")
    processed_images, binary_images, image_mapping = find_all_images(base_path=base_path)
    logger.info(f"Found {len(processed_images)} processed images and {len(binary_images)} binary images.")

    # 2. Find and load transcriptions
    transcription_loader = TranscriptionLoader(
        transcription_dir=transcriptions_path,
        page_transcription_dir=page_transcription_dir
    )
    transcription_loader.find_transcription_files()

    # 3. Create page transcriptions
    page_transcriptions = create_page_transcriptions_modified(
        transcription_loader,
        image_mapping,
        max_pages_per_doc=6
    )

    # Check if we have any page transcriptions
    if not page_transcriptions:
        logger.error("No page transcriptions were generated. Cannot proceed with fine-tuning.")
        return

    # 4. Create train/validation split
    logger.info("Creating train-validation split...")
    train_data, val_data = create_train_val_split(page_transcriptions)

    # 5. Fine-tune the model
    logger.info("Starting fine-tuning process...")
    model, processor, eval_results = fine_tune_trocr(
        train_data=train_data,
        val_data=val_data,
        output_dir=fine_tuned_model_dir,
        num_epochs=10,
        batch_size=4
    )

    logger.info("Fine-tuning completed!")
    logger.info(f"Final CER: {eval_results.get('cer', 'N/A')}")
    logger.info(f"Final WER: {eval_results.get('wer', 'N/A')}")

    # 6. Example of using the fine-tuned model
    logger.info("Example of using the fine-tuned model:")
    logger.info("""
    # Load fine-tuned model and processor
    from transformers import TrOCRProcessor, VisionEncoderDecoderModel

    model = VisionEncoderDecoderModel.from_pretrained('fine_tuned_model/final-model')
    processor = TrOCRProcessor.from_pretrained('fine_tuned_model/final-model')

    # Move to device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Process an image
    from PIL import Image
    image = Image.open('path/to/image.jpg').convert('RGB')
    pixel_values = processor(image, return_tensors="pt").pixel_values.to(device)

    # Generate text
    generated_ids = model.generate(pixel_values, max_length=512)
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

    print(f"OCR Text: {generated_text}")
    """)

    # 7. Integration with BETO post-processor (if used in your pipeline)
    logger.info("Note on integration with BETO post-processor:")
    logger.info("""
    To integrate with the BETO post-processor:

    1. Load your BETO model and tokenizer
    2. Create the SpanishHistoricalNormalizer
    3. Create the BETOPostProcessor
    4. Create your pipeline using both TrOCR and BETO components

    Example:

    beto_tokenizer = BertTokenizer.from_pretrained("dccuchile/bert-base-spanish-wwm-cased")
    beto_model = BertForMaskedLM.from_pretrained("dccuchile/bert-base-spanish-wwm-cased")

    normalizer = SpanishHistoricalNormalizer()
    normalizer.expand_lexicon_from_transcriptions(texts)

    post_processor = BETOPostProcessor(beto_model, beto_tokenizer, normalizer)

    # Your full pipeline using the fine-tuned TrOCR model and BETO post-processor
    pipeline = HybridOCRPipeline(
        trocr_model=model,  # Fine-tuned TrOCR model
        trocr_processor=processor,  # TrOCR processor
        post_processor=post_processor  # BETO post-processor
    )
    """)

if __name__ == "__main__":
    main()

"""8. Memory Optimization Tips"""

def optimize_memory_usage(model, training_args):
    """
    Apply memory optimization techniques to the model and training arguments.

    Args:
        model: The TrOCR model
        training_args: Training arguments

    Returns:
        Updated model and training arguments
    """
    # 1. Enable gradient checkpointing (trades memory for computation)
    model.gradient_checkpointing_enable()

    # 2. Adjust batch size and gradient accumulation
    training_args.per_device_train_batch_size = 2  # Reduce batch size
    training_args.gradient_accumulation_steps = 8  # Increase gradient accumulation

    # 3. Use fp16 mixed precision training
    training_args.fp16 = True

    # 4. Reduce training precision
    # (only for highest memory pressure scenarios - may affect performance)
    training_args.fp16_full_eval = True

    # 5. Freeze encoder layers to reduce memory
    # Only fine-tune the decoder and last few encoder layers
    for param in model.encoder.parameters():
        param.requires_grad = False

    # Unfreeze only the last two encoder layers
    for param in model.encoder.layer[-2:].parameters():
        param.requires_grad = True

    return model, training_args

"""# 9. Integration with BETO Post-Processing"""

def integrate_with_beto_postprocessor(trocr_model, trocr_processor, beto_model, beto_tokenizer):
    """
    Integrate the fine-tuned TrOCR model with the BETO post-processor.

    Args:
        trocr_model: Fine-tuned TrOCR model
        trocr_processor: TrOCR processor
        beto_model: BETO model for post-processing
        beto_tokenizer: BETO tokenizer

    Returns:
        Combined OCR pipeline
    """
    # Create the normalizer
    normalizer = SpanishHistoricalNormalizer()

    # Expand lexicon from your transcriptions
    texts = [data['text'] for data in page_transcriptions.values()]
    normalizer.expand_lexicon_from_transcriptions(texts)

    # Create BETO post-processor
    post_processor = BETOPostProcessor(beto_model, beto_tokenizer, normalizer)

    # Create the full OCR pipeline with correct models
    pipeline = HybridOCRPipeline(
        trocr_model=trocr_model,
        trocr_processor=trocr_processor,
        post_processor=post_processor
    )

    return pipeline