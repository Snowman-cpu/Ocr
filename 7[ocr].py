# -*- coding: utf-8 -*-
"""7[OCR].ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10RA8x5jMt0UjSF2x89gZWn0RULm5_BtY

# CELL 1: Extract the ZIP File and Explore Contents
"""

import os
import zipfile
from glob import glob
import pandas as pd

# Path to the original raw ZIP file (ensure the ZIP is in your Colab file system)
zip_path = "OneDrive_2025-03-13.zip"
extract_dir = "./extracted_docs"

if not os.path.exists(zip_path):
    print(f"Zip file not found: {zip_path}")
else:
    os.makedirs(extract_dir, exist_ok=True)
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)

    print("Extracted files and folders:")
    for root, dirs, files in os.walk(extract_dir):
        level = root.replace(extract_dir, '').count(os.sep)
        indent = ' ' * 4 * level
        print(f"{indent}{os.path.basename(root)}/")
        sub_indent = ' ' * 4 * (level + 1)
        for file in files:
            print(f"{sub_indent}{file}")

    # Count different file types
    docx_files = glob(os.path.join(extract_dir, "**", "*.docx"), recursive=True)
    pdf_files = glob(os.path.join(extract_dir, "**", "*.pdf"), recursive=True)
    other_files = []
    for root, dirs, files in os.walk(extract_dir):
        for file in files:
            if not file.endswith(('.docx', '.pdf')):
                other_files.append(os.path.join(root, file))

    print(f"\nFound {len(docx_files)} .docx files")
    print(f"Found {len(pdf_files)} .pdf files")
    print(f"Found {len(other_files)} other files")

"""# CELL 2: Organize Documents by Source"""

import os
import shutil
from collections import defaultdict

organized_dir = "./organized_docs"
os.makedirs(organized_dir, exist_ok=True)

source_docs = defaultdict(list)

for doc_path in docx_files:
    filename = os.path.basename(doc_path)
    parent_dir = os.path.basename(os.path.dirname(doc_path))
    # Use filename parts and parent folder as indicators
    source_indicators = [parent_dir] + filename.split('_')
    source = None
    for indicator in source_indicators:
        if indicator and not indicator.isdigit() and indicator.lower() not in ['docx', 'doc', 'document']:
            source = indicator
            break
    if not source:
        source = "unknown_source"
    source_docs[source].append(doc_path)

# Copy files to organized folder
for source, file_list in source_docs.items():
    source_dir = os.path.join(organized_dir, source)
    os.makedirs(source_dir, exist_ok=True)
    for file in file_list:
        shutil.copy2(file, source_dir)

print(f"Organized documents into {len(source_docs)} source categories:")
for source, files in source_docs.items():
    print(f"  - {source}: {len(files)} documents")

"""# CELL 3: Convert DOCX to PDF and PDFs to Images"""

# Install and import pdf2image
try:
    from pdf2image import convert_from_path
except ImportError:
    !pip install pdf2image
    from pdf2image import convert_from_path

# Install and import PyMuPDF (fitz)
try:
    import fitz
except ImportError:
    !pip install PyMuPDF
    import fitz

# Install and import python-docx
try:
    import docx
except ImportError:
    !pip install python-docx
    import docx

# Install and import reportlab components
try:
    from reportlab.lib.pagesizes import letter
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
    from reportlab.lib.styles import getSampleStyleSheet
except ImportError:
    !pip install reportlab
    from reportlab.lib.pagesizes import letter
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
    from reportlab.lib.styles import getSampleStyleSheet

import subprocess
import os
import glob

pdf_output_dir = "./pdf_files"
image_output_dir = "./image_files"
os.makedirs(pdf_output_dir, exist_ok=True)
os.makedirs(image_output_dir, exist_ok=True)

def convert_docx_to_pdf(docx_path, output_dir):
    filename = os.path.basename(docx_path)
    base_name = os.path.splitext(filename)[0]
    pdf_path = os.path.join(output_dir, f"{base_name}.pdf")
    try:
        cmd = ['libreoffice', '--headless', '--convert-to', 'pdf', '--outdir', output_dir, docx_path]
        process = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        if process.returncode != 0:
            print(f"Error converting {docx_path}: {process.stderr.decode('utf-8')}")
            return None
        if os.path.exists(pdf_path):
            return pdf_path
        else:
            print(f"PDF not created for {docx_path}")
            return None
    except Exception as e:
        print(f"Error converting {docx_path}: {e}")
        return None

def convert_docx_to_pdf_alternative(docx_path, output_dir):
    filename = os.path.basename(docx_path)
    base_name = os.path.splitext(filename)[0]
    pdf_path = os.path.join(output_dir, f"{base_name}.pdf")
    try:
        doc = docx.Document(docx_path)
        pdf = SimpleDocTemplate(pdf_path, pagesize=letter)
        styles = getSampleStyleSheet()
        content = []
        for para in doc.paragraphs:
            if para.text:
                content.append(Paragraph(para.text, styles["Normal"]))
                content.append(Spacer(1, 12))
        pdf.build(content)
        return pdf_path
    except Exception as e:
        print(f"Alternative conversion failed for {docx_path}: {e}")
        return None

def convert_pdf_to_images_pymupdf(pdf_path, output_dir, dpi=300):
    filename = os.path.basename(pdf_path)
    base_name = os.path.splitext(filename)[0]
    try:
        doc = fitz.open(pdf_path)
    except Exception as e:
        print(f"Error opening PDF {pdf_path}: {e}")
        return []
    images = []
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        pix = page.get_pixmap(matrix=fitz.Matrix(dpi/72, dpi/72))
        output_path = os.path.join(output_dir, f"{base_name}_page_{page_num+1}.jpg")
        pix.save(output_path)
        images.append(output_path)
    doc.close()
    return images

# Process organized DOCX files
all_organized_docx = []
for source_dir in os.listdir(organized_dir):
    source_path = os.path.join(organized_dir, source_dir)
    if os.path.isdir(source_path):
        docs = glob.glob(os.path.join(source_path, "*.docx"))
        all_organized_docx.extend(docs)

print(f"Converting {len(all_organized_docx)} DOCX files to PDF using LibreOffice...")
pdf_paths = []
for docx_path in all_organized_docx:
    pdf_path = convert_docx_to_pdf(docx_path, pdf_output_dir)
    if not pdf_path:
        print(f"Trying alternative conversion method for {docx_path}...")
        pdf_path = convert_docx_to_pdf_alternative(docx_path, pdf_output_dir)
    if pdf_path:
        pdf_paths.append(pdf_path)

print(f"Successfully converted {len(pdf_paths)} files to PDF")

# Fallback: if no PDFs were created, attempt conversion using textract and FPDF
if len(pdf_paths) == 0:
    try:
        from fpdf import FPDF
    except ImportError:
        !pip install fpdf
        from fpdf import FPDF
    try:
        import textract
    except ImportError:
        !pip install textract
        import textract

    for docx_path in all_organized_docx:
        try:
            text = textract.process(docx_path).decode('utf-8')
            filename = os.path.basename(docx_path)
            base_name = os.path.splitext(filename)[0]
            pdf_path = os.path.join(pdf_output_dir, f"{base_name}.pdf")
            pdf = FPDF()
            pdf.add_page()
            pdf.set_font("Arial", size=12)
            for line in text.split('\n'):
                pdf.multi_cell(0, 10, line)
            pdf.output(pdf_path)
            pdf_paths.append(pdf_path)
            print(f"Created PDF for {base_name} using textract and FPDF")
        except Exception as e:
            print(f"Failed to convert {docx_path} using textract: {e}")

print(f"Total PDFs created: {len(pdf_paths)}")

# Convert PDFs to images (or fallback to DOCX image extraction)
if len(pdf_paths) > 0:
    print("Converting PDFs to images...")
    image_paths = []
    for pdf_path in pdf_paths:
        doc_images = convert_pdf_to_images_pymupdf(pdf_path, image_output_dir)
        image_paths.extend(doc_images)
    print(f"Generated {len(image_paths)} images from {len(pdf_paths)} PDFs")
else:
    try:
        from PIL import Image, ImageDraw, ImageFont
    except ImportError:
        !pip install Pillow
        from PIL import Image, ImageDraw, ImageFont
    image_paths = []
    for docx_path in all_organized_docx:
        try:
            doc = docx.Document(docx_path)
            base_name = os.path.splitext(os.path.basename(docx_path))[0]
            text = "\n".join([para.text for para in doc.paragraphs])
            img = Image.new('RGB', (2100, 2970), color=(255, 255, 255))
            d = ImageDraw.Draw(img)
            try:
                font = ImageFont.truetype("DejaVuSans.ttf", 36)
            except:
                font = ImageFont.load_default()
            d.text((50, 50), text, fill=(0, 0, 0), font=font)
            img_path = os.path.join(image_output_dir, f"{base_name}_page_1.jpg")
            img.save(img_path)
            image_paths.append(img_path)
            print(f"Created image for {base_name}")
        except Exception as e:
            print(f"Failed to process {docx_path} directly: {e}")
    print(f"Total images created: {len(image_paths)}")

"""# CELL 4: Image Preprocessing"""

# Install OpenCV and scikit-image if missing
try:
    import cv2
except ImportError:
    !pip install opencv-python
    import cv2

try:
    from skimage import exposure, transform, morphology
except ImportError:
    !pip install scikit-image
    from skimage import exposure, transform, morphology

import numpy as np
from concurrent.futures import ThreadPoolExecutor
import matplotlib.pyplot as plt

preprocessed_dir = "./preprocessed_images"
os.makedirs(preprocessed_dir, exist_ok=True)

def preprocess_image(image_path):
    image = cv2.imread(image_path)
    if image is None:
        print(f"Could not read image: {image_path}")
        return None

    base_name = os.path.splitext(os.path.basename(image_path))[0]
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    denoised = cv2.GaussianBlur(gray, (3, 3), 0)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    enhanced = clahe.apply(denoised)
    edges = cv2.Canny(enhanced, 50, 150, apertureSize=3)
    lines = cv2.HoughLines(edges, 1, np.pi/180, 200)

    angle = 0
    if lines is not None:
        angles = [line[0][1] for line in lines if line[0][1] < np.pi/4 or line[0][1] > 3*np.pi/4]
        if angles:
            angle = np.median(angles) - np.pi/2

    if abs(angle) > 0.01:
        (h, w) = enhanced.shape[:2]
        center = (w // 2, h // 2)
        M = cv2.getRotationMatrix2D(center, angle * 180 / np.pi, 1.0)
        rotated = cv2.warpAffine(enhanced, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
    else:
        rotated = enhanced

    binary = cv2.adaptiveThreshold(rotated, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                   cv2.THRESH_BINARY, 11, 2)
    kernel = np.ones((1, 1), np.uint8)
    cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)

    output_path = os.path.join(preprocessed_dir, f"{base_name}_preprocessed.jpg")
    cv2.imwrite(output_path, cleaned)

    # Create a visualization for about 10% of images
    if np.random.random() < 0.1:
        fig, ax = plt.subplots(2, 3, figsize=(15, 10))
        ax[0, 0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        ax[0, 0].set_title('Original')
        ax[0, 1].imshow(gray, cmap='gray')
        ax[0, 1].set_title('Grayscale')
        ax[0, 2].imshow(denoised, cmap='gray')
        ax[0, 2].set_title('Denoised')
        ax[1, 0].imshow(enhanced, cmap='gray')
        ax[1, 0].set_title('Enhanced Contrast')
        ax[1, 1].imshow(rotated, cmap='gray')
        ax[1, 1].set_title(f'Deskewed (angle: {angle:.2f})')
        ax[1, 2].imshow(cleaned, cmap='gray')
        ax[1, 2].set_title('Binarized & Cleaned')
        plt.tight_layout()
        viz_path = os.path.join(preprocessed_dir, f"{base_name}_visualization.jpg")
        plt.savefig(viz_path)
        plt.close()

    return output_path

print(f"Preprocessing {len(image_paths)} images...")
with ThreadPoolExecutor(max_workers=4) as executor:
    processed_images = list(filter(None, executor.map(preprocess_image, image_paths)))
print(f"Successfully preprocessed {len(processed_images)} images")

"""# CELL 5: Generate a Summary Report"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

def generate_report():
    if 'source_docs' in globals() and hasattr(source_docs, 'items'):
        organized_by_source = {k: len(v) for k, v in source_docs.items()}
    else:
        organized_by_source = {"all_sources": len(docx_files)}

    report = {
        "original_docx_files": len(docx_files),
        "organized_by_source": organized_by_source,
        "generated_pdfs": len(pdf_paths) if 'pdf_paths' in globals() else 0,
        "generated_images": len(image_paths) if 'image_paths' in globals() else 0,
        "preprocessed_images": len(processed_images) if 'processed_images' in globals() else 0
    }

    summary_df = pd.DataFrame([
        {"Stage": "Original .docx files", "Count": report["original_docx_files"]},
        {"Stage": "Generated PDFs", "Count": report["generated_pdfs"]},
        {"Stage": "Generated Images", "Count": report["generated_images"]},
        {"Stage": "Preprocessed Images", "Count": report["preprocessed_images"]}
    ])

    sample_size = min(10, len(processed_images))
    if sample_size > 0:
        sample_indices = np.random.choice(range(len(processed_images)), sample_size, replace=False)
        for idx in sample_indices:
            proc_img_path = processed_images[idx]
            original_img_name = os.path.basename(proc_img_path).replace('_preprocessed.jpg', '.jpg')
            original_img_path = os.path.join(image_output_dir, original_img_name)
            if os.path.exists(original_img_path):
                orig_size = os.path.getsize(original_img_path)
                proc_size = os.path.getsize(proc_img_path)
                reduction = (1 - proc_size/orig_size) * 100
                print(f"Size reduction for {os.path.basename(proc_img_path)}: {reduction:.1f}%")

    return report, summary_df

report, summary_df = generate_report()
print("\nProcessing Summary:")
print(summary_df)

plt.figure(figsize=(10, 6))
plt.bar(summary_df['Stage'], summary_df['Count'])
plt.title('Document Processing Pipeline')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('processing_summary.png')
plt.show()

print("\nData preparation and preprocessing complete!")

"""# CELL 6: Data Alignment – Matching Transcriptions with Images"""

# Ensure python-docx is installed
try:
    from docx import Document
except ImportError:
    !pip install python-docx
    from docx import Document

import re
from difflib import SequenceMatcher
import os
import pandas as pd
import matplotlib.pyplot as plt
import random
import matplotlib.gridspec as gridspec

aligned_data_dir = "./aligned_data"
os.makedirs(aligned_data_dir, exist_ok=True)

def extract_text_from_docx(docx_path):
    try:
        doc = Document(docx_path)
        full_text = [para.text for para in doc.paragraphs]
        return "\n".join(full_text)
    except Exception as e:
        print(f"Error extracting text from {docx_path}: {e}")
        return ""

def string_similarity(a, b):
    return SequenceMatcher(None, a, b).ratio()

def align_transcriptions_with_images(docx_files, image_files):
    alignment_data = []
    common_patterns = [r'Buendia', r'Mendo', r'Ezcaray', r'Paredes', r'Constituciones', r'PORCONES']

    for docx_path in docx_files:
        docx_name = os.path.basename(docx_path)
        docx_basename = os.path.splitext(docx_name)[0]
        document_text = extract_text_from_docx(docx_path)
        document_first_100_chars = document_text[:100].lower() if document_text else ""

        document_type = "unknown"
        for pattern in common_patterns:
            if re.search(pattern, docx_name, re.IGNORECASE):
                document_type = pattern
                break

        matching_images = []
        for img_path in image_files:
            img_name = os.path.basename(img_path)
            if document_type != "unknown" and re.search(document_type, img_name, re.IGNORECASE):
                if re.search(r'page_[1-3]\.jpg$', img_name):
                    matching_images.append(img_path)
            elif string_similarity(docx_basename, os.path.splitext(img_name)[0]) > 0.6:
                if re.search(r'page_[1-3]\.jpg$', img_name):
                    matching_images.append(img_path)

        matching_images.sort(key=lambda x: int(re.search(r'page_(\d+)\.jpg$', x).group(1)) if re.search(r'page_(\d+)\.jpg$', x) else 0)

        if matching_images:
            num_pages = len(matching_images)
            if document_text:
                chars_per_page = len(document_text) // num_pages
                text_pages = [document_text[i:i+chars_per_page] for i in range(0, len(document_text), chars_per_page)]
                for i, img_path in enumerate(matching_images):
                    if i < len(text_pages):
                        page_text = text_pages[i]
                        page_num = i + 1
                        alignment_data.append({
                            'document_name': docx_basename,
                            'document_type': document_type,
                            'image_path': img_path,
                            'page_number': page_num,
                            'transcription': page_text[:500] + "..." if len(page_text) > 500 else page_text,
                            'full_transcription': page_text
                        })
                        pair_id = f"{docx_basename}_page_{page_num}"
                        metadata_path = os.path.join(aligned_data_dir, f"{pair_id}_metadata.txt")
                        with open(metadata_path, 'w', encoding='utf-8') as f:
                            f.write(f"Document: {docx_basename}\n")
                            f.write(f"Document Type: {document_type}\n")
                            f.write(f"Page: {page_num}\n")
                            f.write(f"Image: {os.path.basename(img_path)}\n")
                            f.write("--- Transcription ---\n")
                            f.write(page_text)
        else:
            print(f"No matching images found for {docx_name}")

    return alignment_data

def analyze_text_variations(alignment_data):
    irregularities = {'diacritics': [], 'spelling_variations': [], 'layout_notes': [], 'abbreviations': []}
    diacritic_pattern = re.compile(r'[áéíóúàèìòùäëïöüÁÉÍÓÚÀÈÌÒÙÄËÏÖÜñÑ]')
    abbrev_pattern = re.compile(r'\b[A-Za-z]{1,3}\.')

    for item in alignment_data:
        text = item['full_transcription']
        document_name = item['document_name']
        diacritics = diacritic_pattern.findall(text)
        if diacritics:
            irregularities['diacritics'].append({
                'document': document_name,
                'page': item['page_number'],
                'examples': diacritics[:10]
            })
        abbreviations = abbrev_pattern.findall(text)
        if abbreviations:
            irregularities['abbreviations'].append({
                'document': document_name,
                'page': item['page_number'],
                'examples': list(set(abbreviations))[:10]
            })
        para_count = text.count('\n\n')
        if para_count > 5:
            irregularities['layout_notes'].append({
                'document': document_name,
                'page': item['page_number'],
                'note': f"Contains {para_count} paragraph breaks"
            })
    return irregularities

print("Aligning transcriptions with images...")
alignment_data = align_transcriptions_with_images(docx_files, image_paths)
print(f"Found {len(alignment_data)} document-image alignments")
irregularities = analyze_text_variations(alignment_data)
print("\nDocument text irregularities found:")
for category, items in irregularities.items():
    print(f"  - {category}: {len(items)} instances")

alignment_df = pd.DataFrame(alignment_data)
alignment_csv_path = os.path.join(aligned_data_dir, "document_image_alignment.csv")
alignment_df.to_csv(alignment_csv_path, index=False)
print(f"Alignment data saved to {alignment_csv_path}")

if alignment_data:
    sample_size = min(3, len(alignment_data))
    samples = random.sample(alignment_data, sample_size)
    for i, sample in enumerate(samples):
        fig = plt.figure(figsize=(15, 10))
        gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])
        ax0 = plt.subplot(gs[0])
        img = plt.imread(sample['image_path'])
        ax0.imshow(img)
        ax0.set_title(f"Image: {os.path.basename(sample['image_path'])}")
        ax0.axis('off')
        ax1 = plt.subplot(gs[1])
        ax1.text(0.05, 0.95, f"Document: {sample['document_name']}\nPage: {sample['page_number']}",
                 transform=ax1.transAxes, fontsize=12, verticalalignment='top')
        ax1.text(0.05, 0.85, sample['transcription'][:300] + "...",
                 transform=ax1.transAxes, fontsize=10, verticalalignment='top', wrap=True)
        ax1.axis('off')
        plt.tight_layout()
        plt.savefig(os.path.join(aligned_data_dir, f"alignment_sample_{i+1}.png"))
        plt.close()
    print(f"Created {sample_size} sample alignment visualizations in {aligned_data_dir}")

"""# CELL 7: Data Augmentation"""

# Install and import scikit-image and OpenCV
try:
    from skimage import transform, exposure, util
except ImportError:
    !pip install scikit-image
    from skimage import transform, exposure, util

try:
    import cv2
except ImportError:
    !pip install opencv-python
    import cv2

import numpy as np
import matplotlib.pyplot as plt
from concurrent.futures import ThreadPoolExecutor
import os

augmented_dir = "./augmented_images"
os.makedirs(augmented_dir, exist_ok=True)

class ImageAugmenter:
    def __init__(self, output_dir):
        self.output_dir = output_dir

    def _rotate(self, image, angle):
        return transform.rotate(image, angle, resize=True, preserve_range=True).astype(np.uint8)

    def _scale(self, image, factor):
        h, w = image.shape[:2]
        new_h, new_w = int(h * factor), int(w * factor)
        return cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)

    def _adjust_brightness(self, image, factor):
        return exposure.adjust_gamma(image, factor)

    def _add_noise(self, image, amount=0.05):
        return (util.random_noise(image, var=amount, clip=True) * 255).astype(np.uint8)

    def _add_blur(self, image, sigma=1):
        return cv2.GaussianBlur(image, (5, 5), sigma)

    def _add_shadow(self, image):
        h, w = image.shape[:2]
        x = np.linspace(0, 1, w)
        y = np.linspace(0, 1, h)
        xx, yy = np.meshgrid(x, y)
        direction = np.random.rand() * 2 * np.pi
        gradient = np.sin(direction) * xx + np.cos(direction) * yy
        shadow = 0.7 + 0.3 * gradient
        if len(image.shape) == 3:
            for c in range(image.shape[2]):
                image[:, :, c] = np.clip(image[:, :, c] * shadow, 0, 255).astype(np.uint8)
        else:
            image = np.clip(image * shadow, 0, 255).astype(np.uint8)
        return image

    def _simulate_fold(self, image):
        h, w = image.shape[:2]
        is_vertical = np.random.rand() > 0.5
        if is_vertical:
            fold_pos = int(np.random.uniform(w * 0.3, w * 0.7))
            fold_width = int(np.random.uniform(5, 15))
            for i in range(fold_width):
                factor = 0.7 + 0.3 * (i / fold_width)
                pos = max(0, min(w-1, fold_pos - fold_width//2 + i))
                if len(image.shape) == 3:
                    image[:, pos, :] = (image[:, pos, :] * factor).astype(np.uint8)
                else:
                    image[:, pos] = (image[:, pos] * factor).astype(np.uint8)
        else:
            fold_pos = int(np.random.uniform(h * 0.3, h * 0.7))
            fold_width = int(np.random.uniform(5, 15))
            for i in range(fold_width):
                factor = 0.7 + 0.3 * (i / fold_width)
                pos = max(0, min(h-1, fold_pos - fold_width//2 + i))
                if len(image.shape) == 3:
                    image[pos, :, :] = (image[pos, :, :] * factor).astype(np.uint8)
                else:
                    image[pos, :] = (image[pos, :] * factor).astype(np.uint8)
        return image

    def augment_image(self, image_path):
        base_name = os.path.splitext(os.path.basename(image_path))[0]
        img = cv2.imread(image_path)
        if img is None:
            print(f"Error reading image {image_path}")
            return []
        augmented_paths = []
        # Rotation augmentations
        for angle in [-5, -3, 3, 5]:
            rotated = self._rotate(img, angle)
            output_path = os.path.join(self.output_dir, f"{base_name}_rot{angle}.jpg")
            cv2.imwrite(output_path, rotated)
            augmented_paths.append(output_path)
        # Scaling augmentations
        for scale in [0.9, 1.1]:
            scaled = self._scale(img, scale)
            output_path = os.path.join(self.output_dir, f"{base_name}_scale{scale:.1f}.jpg")
            cv2.imwrite(output_path, scaled)
            augmented_paths.append(output_path)
        # Brightness adjustments
        for factor in [0.8, 1.2]:
            brightened = (self._adjust_brightness(img/255.0, factor) * 255).astype(np.uint8)
            output_path = os.path.join(self.output_dir, f"{base_name}_bright{factor:.1f}.jpg")
            cv2.imwrite(output_path, brightened)
            augmented_paths.append(output_path)
        # Noise addition
        for amount in [0.01, 0.03]:
            noisy = self._add_noise(img, amount)
            output_path = os.path.join(self.output_dir, f"{base_name}_noise{amount:.2f}.jpg")
            cv2.imwrite(output_path, noisy)
            augmented_paths.append(output_path)
        # Blur
        for sigma in [1.0, 2.0]:
            blurry = self._add_blur(img, sigma)
            output_path = os.path.join(self.output_dir, f"{base_name}_blur{sigma:.1f}.jpg")
            cv2.imwrite(output_path, blurry)
            augmented_paths.append(output_path)
        # Shadow/gradient
        shadow = self._add_shadow(img.copy())
        output_path = os.path.join(self.output_dir, f"{base_name}_shadow.jpg")
        cv2.imwrite(output_path, shadow)
        augmented_paths.append(output_path)
        # Simulate fold/crease
        fold = self._simulate_fold(img.copy())
        output_path = os.path.join(self.output_dir, f"{base_name}_fold.jpg")
        cv2.imwrite(output_path, fold)
        augmented_paths.append(output_path)
        return augmented_paths

    def augment_dataset(self, image_dir):
        image_paths = []
        for root, _, files in os.walk(image_dir):
            for file in files:
                if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    image_paths.append(os.path.join(root, file))
        print(f"Found {len(image_paths)} images to augment")
        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
            results = list(executor.map(self.augment_image, image_paths))
        augmented_paths = [path for sublist in results for path in sublist]
        print(f"Created {len(augmented_paths)} augmented images")
        return augmented_paths

def run_augmentation(input_dir, output_dir="./augmented_images"):
    os.makedirs(output_dir, exist_ok=True)
    augmenter = ImageAugmenter(output_dir)
    augmented_paths = augmenter.augment_dataset(input_dir)
    print(f"Augmentation complete. {len(augmented_paths)} augmented images saved to {output_dir}")
    return augmented_paths

input_directory = './preprocessed_images'
output_directory = './augmented_images'
run_augmentation(input_directory, output_directory)