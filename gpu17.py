# -*- coding: utf-8 -*-
"""gpu17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Am9y6yI_g48uI5QN_2MqjwxrHWUSPXkr

# Cell 1
"""

# Install required system packages
!apt-get update -qq
!apt-get install -qq -y poppler-utils tesseract-ocr libtesseract-dev tesseract-ocr-spa

# Install required Python packages
!pip install -q pdf2image opencv-python matplotlib tqdm
!pip install -q torch==2.0.1+cu118 torchvision==0.15.2+cu118 -f https://download.pytorch.org/whl/torch_stable.html
!pip install -q transformers==4.30.2 datasets accelerate
!pip install -q pillow seaborn pandas
!pip install -q PyPDF2 pytesseract
!pip install -q huggingface-hub sentencepiece sacremoses
!pip install -q rapidfuzz symspellpy

# Verify GPU availability
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU device count: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"Current GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

"""# Cell 2"""

import os
import glob
import re
import random
import warnings
import shutil
import math
import string
import time
import numpy as np
import cv2
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageEnhance, ImageFilter
from tqdm.notebook import tqdm
import torch
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
from torch.optim.lr_scheduler import CosineAnnealingLR
from torchvision import transforms
import pdf2image
import difflib
from collections import Counter
import pandas as pd
import seaborn as sns
from google.colab import files
import pytesseract
from rapidfuzz import fuzz
from rapidfuzz import process as fuzz_process
from symspellpy import SymSpell, Verbosity

# Import transformers libraries
try:
    from transformers import (
        TrOCRProcessor,
        VisionEncoderDecoderModel,
        VisionEncoderDecoderConfig,
        AutoFeatureExtractor,
        AutoTokenizer,
        default_data_collator,
        get_scheduler
    )
    from datasets import Dataset
    from accelerate import Accelerator
    print("Successfully imported transformers and datasets libraries")
except ImportError:
    print("Error importing transformers or datasets. Installing again...")
    !pip install -q transformers datasets accelerate
    from transformers import (
        TrOCRProcessor,
        VisionEncoderDecoderModel,
        VisionEncoderDecoderConfig,
        AutoFeatureExtractor,
        AutoTokenizer
    )
    from datasets import Dataset
    from accelerate import Accelerator

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

# Set random seeds for reproducibility
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)
    # Set prefer higher precision ops for CUDA
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    # Enable cuDNN benchmarking for performance
    torch.backends.cudnn.benchmark = True
    # Make sure deterministic algorithms work across runs
    torch.use_deterministic_algorithms(False)

print("Libraries imported successfully!")

# Check for available GPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Function to monitor GPU memory usage
def print_gpu_memory_usage():
    """
    Print current GPU memory usage statistics.

    This function displays total, reserved, allocated, and free memory
    to help monitor resource utilization during processing.
    """
    if torch.cuda.is_available():
        t = torch.cuda.get_device_properties(0).total_memory
        r = torch.cuda.memory_reserved(0)
        a = torch.cuda.memory_allocated(0)
        f = r-a  # free inside reserved
        print(f"GPU Memory: Total={t/1e9:.2f}GB, Reserved={r/1e9:.2f}GB, "
              f"Allocated={a/1e9:.2f}GB, Free={f/1e9:.2f}GB")
    else:
        print("No GPU available")

# Check GPU memory usage
print_gpu_memory_usage()

"""# Cell 3"""

class OCRConfig:
    """
    Configuration class for the OCR pipeline.

    This centralizes all configuration parameters for the OCR pipeline,
    making it easier to adjust settings in one place.
    """
    def __init__(self):
        # Base paths
        self.base_path = '/content'
        self.pdf_folder = os.path.join(self.base_path, 'pdfs')
        self.output_base_path = os.path.join(self.base_path, 'ocr_data')
        self.transcriptions_path = os.path.join(self.base_path, 'transcriptions')
        self.training_data_path = os.path.join(self.output_base_path, 'training_data')
        self.results_path = os.path.join(self.output_base_path, 'results')
        self.model_checkpoints_path = os.path.join(self.output_base_path, 'model_checkpoints')
        self.language_model_path = os.path.join(self.output_base_path, 'language_model')

        # OCR models - optimized for T4 GPU
        self.models = {
            "handwritten": "microsoft/trocr-large-handwritten",
            "printed": "microsoft/trocr-large-printed",
            "spanish_custom": "microsoft/trocr-base-printed"
        }

        # PDF processing
        self.default_dpi = 400  # Higher DPI for better quality

        # Image processing
        self.image_size = (512, 512)  # Optimized size for T4 GPU

        # Training parameters - T4 optimized
        self.batch_size = 8  # Optimal for T4 GPU
        self.gradient_accumulation_steps = 4
        self.learning_rate = 3e-5
        self.num_epochs = 5
        self.max_length = 512
        self.use_mixed_precision = True

        # Inference parameters
        self.num_beams = 4
        self.max_new_tokens = 128

# Create configuration
config = OCRConfig()

# Create necessary directories with a single function
def create_directory_structure(config):
    """
    Create the directory structure needed for the OCR pipeline.

    Args:
        config: OCRConfig object containing path configurations
    """
    dirs = [
        config.pdf_folder,
        os.path.join(config.output_base_path, "images"),
        os.path.join(config.output_base_path, "processed_images"),
        os.path.join(config.output_base_path, "binary_images"),
        os.path.join(config.output_base_path, "enhanced_images"),
        config.training_data_path,
        config.transcriptions_path,
        config.results_path,
        config.model_checkpoints_path,
        config.language_model_path,
        os.path.join(config.results_path, "visualizations")
    ]

    for directory in dirs:
        os.makedirs(directory, exist_ok=True)

    print("Directory structure created successfully!")

# Create the directories
create_directory_structure(config)

# Cache GPU info for reporting
if torch.cuda.is_available():
    gpu_info = {
        "name": torch.cuda.get_device_name(0),
        "capability": torch.cuda.get_device_capability(0),
        "total_memory": torch.cuda.get_device_properties(0).total_memory,
    }
    print(f"Using GPU: {gpu_info['name']} with compute capability {gpu_info['capability']}")
    print(f"Total GPU memory: {gpu_info['total_memory'] / 1e9:.2f} GB")

    # Verify if it's a Tesla T4
    if "T4" in gpu_info["name"]:
        print("✅ Confirmed: Using Tesla T4 GPU. Pipeline optimized for this hardware.")
    else:
        print(f"⚠️ Note: Using {gpu_info['name']} instead of Tesla T4. Some optimizations may not apply.")
else:
    print("⚠️ WARNING: No GPU detected. This pipeline is optimized for Tesla T4 GPU.")
    print("Processing will continue but will be substantially slower.")

"""# Cell 4

"""

def convert_pdf_to_images(pdf_path, output_folder, dpi=None, first_page=None, last_page=None, config=None):
    """
    Convert PDF pages to images with robust error handling and memory optimization.

    Args:
        pdf_path: Path to the PDF file
        output_folder: Folder to save the images
        dpi: Resolution in DPI (dots per inch)
        first_page: First page to convert (1-based index)
        last_page: Last page to convert (1-based index)
        config: OCRConfig object containing configuration parameters

    Returns:
        List of paths to the saved images
    """
    if config is None:
        config = OCRConfig()

    # Use default DPI from config if not specified
    dpi = dpi or config.default_dpi

    os.makedirs(output_folder, exist_ok=True)
    pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]
    file_size_mb = os.path.getsize(pdf_path) / (1024 * 1024)

    # Adjust DPI for large files - optimize for T4 GPU memory
    adjusted_dpi = dpi
    if file_size_mb > 50:  # Higher threshold for T4 GPU
        adjusted_dpi = min(dpi, 300)
    if file_size_mb > 100:  # For very large files
        adjusted_dpi = min(dpi, 200)

    if adjusted_dpi != dpi:
        print(f"Large PDF detected ({file_size_mb:.1f} MB). Adjusting DPI from {dpi} to {adjusted_dpi}")

    try:
        # Check poppler installation
        import subprocess
        try:
            subprocess.run(["pdftoppm", "-v"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
        except (subprocess.SubprocessError, FileNotFoundError):
            print("Poppler not found in PATH. Using default path...")
            poppler_path = "/usr/bin"
            os.environ["PATH"] += os.pathsep + poppler_path

        # Process based on file size
        if file_size_mb > 20:
            # Process one page at a time for large PDFs
            from PyPDF2 import PdfReader
            reader = PdfReader(pdf_path)
            total_pages = len(reader.pages)

            start_page = first_page if first_page else 1
            end_page = min(last_page if last_page else total_pages, total_pages)

            image_paths = []
            # Use tqdm for progress tracking
            for page_num in tqdm(range(start_page, end_page + 1), desc=f"Converting {pdf_filename}"):
                try:
                    # Use pdf2image with GPU-optimized settings
                    single_image = pdf2image.convert_from_path(
                        pdf_path,
                        dpi=adjusted_dpi,
                        first_page=page_num,
                        last_page=page_num,
                        use_pdftocairo=True,
                        thread_count=2  # Utilize multiple CPU cores
                    )

                    if single_image and len(single_image) > 0:
                        # Use a higher quality JPEG for T4 processing
                        image_path = os.path.join(output_folder, f"{pdf_filename}_page_{page_num:03d}.jpg")
                        single_image[0].save(image_path, "JPEG", quality=95)
                        image_paths.append(image_path)
                    else:
                        print(f"No image extracted for page {page_num}")
                except Exception as e:
                    print(f"Error processing page {page_num}: {str(e)}")
                    continue

            return image_paths
        else:
            # Process all pages at once for smaller PDFs
            images = pdf2image.convert_from_path(
                pdf_path,
                dpi=adjusted_dpi,
                first_page=first_page,
                last_page=last_page,
                use_pdftocairo=True,
                thread_count=2
            )

            image_paths = []
            for i, image in enumerate(tqdm(images, desc=f"Converting {pdf_filename}")):
                page_num = i + 1 if first_page is None else first_page + i
                image_path = os.path.join(output_folder, f"{pdf_filename}_page_{page_num:03d}.jpg")
                image.save(image_path, "JPEG", quality=95)
                image_paths.append(image_path)

            print(f"Successfully converted {len(image_paths)} pages from {pdf_filename}")
            return image_paths

    except Exception as e:
        print(f"Error converting PDF {pdf_path}: {str(e)}")

        # Try alternative conversion method if primary method fails
        try:
            print("Attempting alternative conversion method...")
            from PyPDF2 import PdfReader
            reader = PdfReader(pdf_path)
            page_count = len(reader.pages)

            start_page = first_page if first_page else 1
            end_page = min(last_page if last_page else page_count, page_count)

            # Use pytesseract as backup to extract text directly
            image_paths = []
            for page_num in tqdm(range(start_page, end_page + 1), desc=f"Alternative conversion for {pdf_filename}"):
                # Create blank image for placeholder
                blank_img = Image.new('RGB', (1000, 1400), color='white')
                draw = ImageDraw.Draw(blank_img)

                # Try to extract text from PDF directly
                try:
                    page = reader.pages[page_num - 1]
                    text = page.extract_text()
                    # Add text to image (basic rendering)
                    y_position = 100
                    for line in text.split('\n'):
                        draw.text((100, y_position), line, fill='black')
                        y_position += 25
                except:
                    draw.text((400, 500), f"Page {page_num} - {pdf_filename}", fill='black')
                    draw.text((400, 550), "(PDF conversion failed - placeholder image)", fill='black')

                # Save the image
                image_path = os.path.join(output_folder, f"{pdf_filename}_page_{page_num:03d}.jpg")
                blank_img.save(image_path, "JPEG", quality=90)
                image_paths.append(image_path)

            print(f"Created {len(image_paths)} alternative images for {pdf_filename}")
            return image_paths

        except Exception as backup_error:
            print(f"Alternative conversion also failed: {str(backup_error)}")
            return []

def upload_pdfs_to_colab(config=None):
    """
    Upload PDF files to Google Colab.

    Args:
        config: OCRConfig object containing configuration parameters

    Returns:
        List of uploaded file paths
    """
    if config is None:
        config = OCRConfig()

    print("Please upload your PDF files (you can select multiple files)...")
    os.makedirs(config.pdf_folder, exist_ok=True)

    # Show a more user-friendly message
    print("\nWaiting for file upload... Click the 'Choose Files' button that appears.")
    print("This pipeline is optimized for historical Spanish documents.")

    # Upload files
    uploaded = files.upload()

    # Move uploaded files to the PDF folder
    uploaded_paths = []
    for filename, content in uploaded.items():
        # Check if the file is a PDF
        if not filename.lower().endswith('.pdf'):
            print(f"Warning: {filename} is not a PDF file. Skipping...")
            continue

        # Write the file to the PDF folder
        filepath = os.path.join(config.pdf_folder, filename)
        with open(filepath, 'wb') as f:
            f.write(content)

        uploaded_paths.append(filepath)
        print(f"Uploaded: {filename} -> {filepath}")

    if not uploaded_paths:
        print("No PDF files were uploaded. Please try again.")
    else:
        print(f"Successfully uploaded {len(uploaded_paths)} PDF files.")

    return uploaded_paths

def process_all_pdfs(config=None, max_pages_per_pdf=None):
    """
    Process all PDFs in the PDF folder with progress tracking.

    Args:
        config: OCRConfig object containing configuration parameters
        max_pages_per_pdf: Maximum number of pages to process per PDF

    Returns:
        Dictionary mapping document IDs to lists of processed image paths
    """
    if config is None:
        config = OCRConfig()

    pdf_folder = config.pdf_folder
    output_base_path = config.output_base_path
    dpi = config.default_dpi

    images_folder = os.path.join(output_base_path, "images")
    processed_folder = os.path.join(output_base_path, "processed_images")
    binary_folder = os.path.join(output_base_path, "binary_images")
    enhanced_folder = os.path.join(output_base_path, "enhanced_images")

    os.makedirs(images_folder, exist_ok=True)
    os.makedirs(processed_folder, exist_ok=True)
    os.makedirs(binary_folder, exist_ok=True)
    os.makedirs(enhanced_folder, exist_ok=True)

    pdf_files = glob.glob(os.path.join(pdf_folder, "*.pdf"))
    print(f"Found {len(pdf_files)} PDF files")

    # Dictionary to store document ID to image paths mapping
    document_images = {}

    # Initialize progress bar for processing PDFs
    for pdf_file in tqdm(pdf_files, desc="Processing PDFs"):
        pdf_filename = os.path.splitext(os.path.basename(pdf_file))[0]
        print(f"\nProcessing {pdf_filename}")

        # Create folders for this PDF
        pdf_images_folder = os.path.join(images_folder, pdf_filename)
        pdf_processed_folder = os.path.join(processed_folder, pdf_filename)
        pdf_binary_folder = os.path.join(binary_folder, pdf_filename)
        pdf_enhanced_folder = os.path.join(enhanced_folder, pdf_filename)

        os.makedirs(pdf_images_folder, exist_ok=True)
        os.makedirs(pdf_processed_folder, exist_ok=True)
        os.makedirs(pdf_binary_folder, exist_ok=True)
        os.makedirs(pdf_enhanced_folder, exist_ok=True)

        # Check if we already have processed images for this PDF to avoid reprocessing
        existing_processed_images = glob.glob(os.path.join(pdf_processed_folder, "*.jpg"))
        if existing_processed_images and len(existing_processed_images) >= (max_pages_per_pdf or float('inf')):
            print(f"Found {len(existing_processed_images)} already processed images for {pdf_filename}. Skipping conversion.")
            document_images[pdf_filename] = existing_processed_images
            continue

        # Handle large PDFs with a different approach
        if os.path.getsize(pdf_file) > 10*1024*1024:  # > 10MB
            print(f"Large PDF detected: {pdf_filename}. Processing in chunks...")

            # Get total page count
            try:
                from PyPDF2 import PdfReader
                reader = PdfReader(pdf_file)
                total_pages = len(reader.pages)
                max_page = min(total_pages, max_pages_per_pdf) if max_pages_per_pdf else total_pages

                print(f"PDF has {total_pages} pages. Processing up to page {max_page}.")

                # Process large PDF in chunks of 5 pages
                chunk_size = 5
                first_page = 1
                all_image_paths = []

                with tqdm(total=max_page, desc=f"Processing {pdf_filename} pages") as progress_bar:
                    while first_page <= max_page:
                        last_page = min(first_page + chunk_size - 1, max_page)
                        chunk_image_paths = convert_pdf_to_images(
                            pdf_file,
                            pdf_images_folder,
                            dpi=dpi,
                            first_page=first_page,
                            last_page=last_page,
                            config=config
                        )

                        if not chunk_image_paths:
                            # Try with a smaller chunk if failed
                            if chunk_size > 1:
                                print(f"Trying with smaller chunk size for pages {first_page}-{last_page}...")
                                chunk_size = 1
                                continue
                            else:
                                print(f"Failed to process pages {first_page}-{last_page}, skipping.")
                                first_page += chunk_size
                                progress_bar.update(chunk_size)
                                continue

                        all_image_paths.extend(chunk_image_paths)
                        first_page += chunk_size
                        progress_bar.update(chunk_size)

                document_images[pdf_filename] = all_image_paths

            except Exception as e:
                print(f"Error determining page count: {str(e)}")
                # Fallback: just try to process first 10 pages
                chunk_image_paths = convert_pdf_to_images(
                    pdf_file,
                    pdf_images_folder,
                    dpi=dpi,
                    first_page=1,
                    last_page=10,
                    config=config
                )
                document_images[pdf_filename] = chunk_image_paths if chunk_image_paths else []
        else:
            # Convert PDF to images for smaller PDFs
            image_paths = convert_pdf_to_images(
                pdf_file,
                pdf_images_folder,
                dpi=dpi,
                first_page=1,
                last_page=max_pages_per_pdf,
                config=config
            )

            # Store the image paths
            document_images[pdf_filename] = image_paths if image_paths else []

        # Check if we got any images for this document
        if not document_images[pdf_filename]:
            print(f"WARNING: No images were extracted from {pdf_filename}")
            continue

    # Check if we processed any images successfully
    total_processed = sum(len(paths) for paths in document_images.values())
    if total_processed == 0:
        print("\nWARNING: No PDFs were successfully processed.")
        print("Please make sure poppler-utils is installed correctly.")
    else:
        print(f"\nSuccessfully processed {total_processed} images from {len(document_images)} documents")

    return document_images

"""# Cell 5"""

def correct_skew(image, delta=0.1, limit=10):
    """
    Enhanced skew correction algorithm optimized for historical documents.

    Args:
        image: Grayscale image as numpy array
        delta: Angle step size for scoring (smaller for more precision)
        limit: Maximum angle to check (increased for heavily skewed documents)

    Returns:
        Deskewed image as numpy array
    """
    # Create edges image for better line detection
    edges = cv2.Canny(image, 50, 150, apertureSize=3)

    # Try to detect lines using Hough Transform with more sensitive parameters
    lines = cv2.HoughLines(edges, 1, np.pi/1800, 150)  # Increased angular resolution

    if lines is not None and len(lines) >= 5:  # Ensure we have enough lines for a good estimate
        # Calculate the angle histogram with finer binning
        angle_counts = {}
        for line in lines:
            rho, theta = line[0]
            # Convert radians to degrees and normalize to [-90, 90]
            angle = (theta * 180 / np.pi) % 180
            if angle > 90:
                angle = angle - 180

            # Bin the angles with higher precision
            angle_key = round(angle * 10) / 10  # 0.1 degree precision
            angle_counts[angle_key] = angle_counts.get(angle_key, 0) + 1

        if angle_counts:
            # Find the angle with the most occurrences
            max_angle = max(angle_counts, key=angle_counts.get)

            # Only correct if the angle is within reasonable limits and has enough support
            if abs(max_angle) <= limit and angle_counts[max_angle] >= len(lines) / 10:
                # Correct 90 degree offset for vertical lines
                if abs(max_angle) > 45:
                    skew_angle = 90 - abs(max_angle)
                    if max_angle > 0:
                        skew_angle = -skew_angle
                else:
                    skew_angle = -max_angle

                # Rotate the image with better interpolation
                (h, w) = image.shape[:2]
                center = (w // 2, h // 2)
                M = cv2.getRotationMatrix2D(center, skew_angle, 1.0)
                return cv2.warpAffine(
                    image, M, (w, h),
                    flags=cv2.INTER_CUBIC,
                    borderMode=cv2.BORDER_REPLICATE
                )

    # If Hough transform approach fails, try projection profile method
    scores = []
    angles = np.arange(-limit, limit + delta, delta)

    # Take the middle portion of the image for faster processing
    h, w = image.shape[:2]
    mid_h, mid_w = h//2, w//2
    center_img = image[mid_h-h//4:mid_h+h//4, mid_w-w//4:mid_w+w//4]

    # Use variance of projection as skew metric
    for angle in angles:
        # Rotate image
        M = cv2.getRotationMatrix2D((center_img.shape[1]//2, center_img.shape[0]//2), angle, 1.0)
        rotated = cv2.warpAffine(
            center_img, M, (center_img.shape[1], center_img.shape[0]),
            flags=cv2.INTER_CUBIC,
            borderMode=cv2.BORDER_REPLICATE
        )

        # Sum the pixel values along each row (horizontal projection)
        projection = np.sum(rotated, axis=1, dtype=np.float32)

        # Calculate the score (variance of the projection)
        score = np.var(projection)
        scores.append(score)

    # Get the angle with the highest score
    best_angle = angles[np.argmax(scores)]

    # Rotate the full image with the best angle
    (h, w) = image.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, best_angle, 1.0)
    rotated = cv2.warpAffine(
        image, M, (w, h),
        flags=cv2.INTER_CUBIC,
        borderMode=cv2.BORDER_REPLICATE
    )

    return rotated

def enhance_historical_document(image):
    """
    Apply specialized enhancement techniques for historical documents.

    Args:
        image: Grayscale image as numpy array

    Returns:
        Enhanced image as numpy array
    """
    # Convert to PIL for some operations
    pil_img = Image.fromarray(image)

    # Apply contrast enhancement
    enhancer = ImageEnhance.Contrast(pil_img)
    contrast_img = enhancer.enhance(1.5)  # Increase contrast

    # Apply sharpening
    enhancer = ImageEnhance.Sharpness(contrast_img)
    sharp_img = enhancer.enhance(1.2)  # Moderate sharpening

    # Apply slight edge enhancement
    edge_enhanced = sharp_img.filter(ImageFilter.EDGE_ENHANCE)

    # Convert back to numpy array
    enhanced_img = np.array(edge_enhanced)

    # If it's RGB, convert to grayscale
    if len(enhanced_img.shape) == 3:
        enhanced_img = cv2.cvtColor(enhanced_img, cv2.COLOR_RGB2GRAY)

    return enhanced_img

def perform_adaptive_binarization(image, blocksize=25, c=8):
    """
    Adaptive binarization with parameters tuned for historical documents.

    Args:
        image: Grayscale image as numpy array
        blocksize: Size of the local neighborhood for thresholding
        c: Constant subtracted from weighted mean

    Returns:
        Binarized image as numpy array
    """
    # Apply Gaussian adaptive thresholding with optimized parameters
    binary = cv2.adaptiveThreshold(
        image,
        255,
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY,
        blocksize,
        c
    )

    # Apply morphological operations to clean up the image
    kernel = np.ones((2, 2), np.uint8)
    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)

    return binary

def remove_noise(image):
    """
    Remove noise from the image using advanced denoising techniques.

    Args:
        image: Grayscale image as numpy array

    Returns:
        Denoised image as numpy array
    """
    # Non-local means denoising with higher strength for historical documents
    denoised = cv2.fastNlMeansDenoising(image, None, h=15, templateWindowSize=7, searchWindowSize=21)
    return denoised

def preprocess_image(image_path, output_folder, binary_folder, enhanced_folder=None):
    """
    Enhanced preprocessing pipeline for historical Spanish documents.

    Args:
        image_path: Path to the input image
        output_folder: Folder to save the preprocessed image
        binary_folder: Folder to save the binary image
        enhanced_folder: Folder to save the enhanced image

    Returns:
        Tuple of (processed_image_path, binary_image_path, enhanced_image_path)
    """
    # Create output folders if they don't exist
    os.makedirs(output_folder, exist_ok=True)
    os.makedirs(binary_folder, exist_ok=True)
    if enhanced_folder:
        os.makedirs(enhanced_folder, exist_ok=True)

    # Get the image filename
    image_filename = os.path.basename(image_path)

    # Read the image
    img = cv2.imread(image_path)
    if img is None:
        print(f"Failed to read image {image_path}")
        return None, None, None

    # Convert to grayscale if it's not already
    if len(img.shape) == 3:
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    else:
        gray = img

    # Apply contrast enhancement (CLAHE)
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
    enhanced = clahe.apply(gray)

    # Denoise the image (advanced denoising)
    denoised = remove_noise(enhanced)

    # Detect and correct skew
    corrected = correct_skew(denoised)

    # Apply historical document enhancement
    historically_enhanced = enhance_historical_document(corrected)

    # Create binary image using adaptive thresholding
    binary = perform_adaptive_binarization(historically_enhanced)

    # Save processed (skew-corrected, enhanced) image
    processed_image_path = os.path.join(output_folder, image_filename)
    cv2.imwrite(processed_image_path, corrected)

    # Save binary image
    binary_image_path = os.path.join(binary_folder, image_filename)
    cv2.imwrite(binary_image_path, binary)

    # Save enhanced image if folder is provided
    enhanced_image_path = None
    if enhanced_folder:
        enhanced_image_path = os.path.join(enhanced_folder, image_filename)
        cv2.imwrite(enhanced_image_path, historically_enhanced)

    return processed_image_path, binary_image_path, enhanced_image_path

def preprocess_document_images(document_images, config=None):
    """
    Process all document images with enhanced preprocessing.

    Args:
        document_images: Dictionary mapping document IDs to image paths
        config: OCRConfig object containing configuration parameters

    Returns:
        Updated dictionary with paths to processed images
    """
    if config is None:
        config = OCRConfig()

    output_base_path = config.output_base_path
    processed_document_images = {}

    # Get output folders
    processed_folder = os.path.join(output_base_path, "processed_images")
    binary_folder = os.path.join(output_base_path, "binary_images")
    enhanced_folder = os.path.join(output_base_path, "enhanced_images")

    for doc_id, image_paths in document_images.items():
        print(f"\nPreprocessing images for: {doc_id}")

        # Create document-specific output folders
        doc_processed_folder = os.path.join(processed_folder, doc_id)
        doc_binary_folder = os.path.join(binary_folder, doc_id)
        doc_enhanced_folder = os.path.join(enhanced_folder, doc_id)

        os.makedirs(doc_processed_folder, exist_ok=True)
        os.makedirs(doc_binary_folder, exist_ok=True)
        os.makedirs(doc_enhanced_folder, exist_ok=True)

        # Process each image
        processed_images = []

        for image_path in tqdm(image_paths, desc=f"Preprocessing {doc_id}"):
            try:
                # Apply enhanced preprocessing
                processed_path, _, _ = preprocess_image(
                    image_path,
                    doc_processed_folder,
                    doc_binary_folder,
                    doc_enhanced_folder
                )

                if processed_path:
                    processed_images.append(processed_path)

            except Exception as e:
                print(f"Error preprocessing {image_path}: {str(e)}")
                # Continue with next image

        processed_document_images[doc_id] = processed_images
        print(f"Processed {len(processed_images)} images for {doc_id}")

    return processed_document_images

def visualize_preprocessing(original_path, processed_path, binary_path, enhanced_path=None):
    """
    Visualize the preprocessing steps with enhanced comparison.

    Args:
        original_path: Path to the original image
        processed_path: Path to the processed image
        binary_path: Path to the binary image
        enhanced_path: Path to the enhanced image
    """
    # Read the images
    original = cv2.imread(original_path)
    processed = cv2.imread(processed_path, cv2.IMREAD_GRAYSCALE)
    binary = cv2.imread(binary_path, cv2.IMREAD_GRAYSCALE)

    # Convert original to RGB for display
    original_rgb = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)

    if enhanced_path and os.path.exists(enhanced_path):
        enhanced = cv2.imread(enhanced_path, cv2.IMREAD_GRAYSCALE)
        # Create a figure with 4 subplots
        fig, axes = plt.subplots(1, 4, figsize=(20, 5))

        # Display the images
        axes[0].imshow(original_rgb)
        axes[0].set_title("Original Image")
        axes[0].axis("off")

        axes[1].imshow(processed, cmap="gray")
        axes[1].set_title("Processed Image")
        axes[1].axis("off")

        axes[2].imshow(enhanced, cmap="gray")
        axes[2].set_title("Enhanced Image")
        axes[2].axis("off")

        axes[3].imshow(binary, cmap="gray")
        axes[3].set_title("Binary Image")
        axes[3].axis("off")
    else:
        # Create a figure with 3 subplots
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        # Display the images
        axes[0].imshow(original_rgb)
        axes[0].set_title("Original Image")
        axes[0].axis("off")

        axes[1].imshow(processed, cmap="gray")
        axes[1].set_title("Processed Image")
        axes[1].axis("off")

        axes[2].imshow(binary, cmap="gray")
        axes[2].set_title("Binary Image")
        axes[2].axis("off")

    # Show the figure
    plt.tight_layout()
    plt.show()

def save_example_images(document_images, output_base_path, num_examples=3):
    """
    Save example images from different preprocessing stages for visualization.

    Args:
        document_images: Dictionary mapping document IDs to processed image paths
        output_base_path: Base folder for outputs
        num_examples: Number of examples to save
    """
    # Create example folder
    example_folder = os.path.join(output_base_path, "examples")
    os.makedirs(example_folder, exist_ok=True)

    # Collect a few examples
    examples = []

    for doc_id, image_paths in document_images.items():
        # Skip if no images
        if not image_paths:
            continue

        # Take the first few images from each document
        for i, processed_path in enumerate(image_paths[:num_examples]):
            # Get the original, binary, and enhanced image paths
            original_path = processed_path.replace("processed_images", "images")
            binary_path = processed_path.replace("processed_images", "binary_images")
            enhanced_path = processed_path.replace("processed_images", "enhanced_images")

            if os.path.exists(original_path) and os.path.exists(binary_path):
                examples.append((original_path, processed_path, binary_path, enhanced_path))

    # Visualize the examples
    for original_path, processed_path, binary_path, enhanced_path in examples:
        visualize_preprocessing(original_path, processed_path, binary_path, enhanced_path)

"""# Cell 6"""

def create_rich_transcriptions(document_images, transcriptions_path):
    """
    Create detailed transcriptions for historical Spanish documents.

    This function generates transcription text files with sample content for
    each document, which can be used for training and evaluation.

    Args:
        document_images: Dictionary mapping document IDs to processed image paths
        transcriptions_path: Path to save the transcriptions
    """
    os.makedirs(transcriptions_path, exist_ok=True)

    # Dictionary with sample content for Spanish historical documents
    historical_samples = {
        "Buendia": {
            "title": "Instrucción de Christiana, y Política Cortesanía",
            "author": "Don Fausto Agustín de Buendía",
            "year": "siglo XVII",
            "content": """
INFINITAMENTE AMABLE VOS, Dulcifsimo Nino JEsus, que no folo os dignafieis de llamaros Doctor de los Ninos, fino tambien de afsiftir como Nino entre los Doctores, fe confagra humilde efta pequena Inftruccion de los Ninos. Es afsi, que ella tambien fe dirige a la juventud; pero a efta, como recurdo de lo que aprendio, a los Ninos, como precifa explicacion de lo que deben eftudiar. Por efte foio titulo, es muy vueftra; y por fer para Ninos, que confiais a la educacion de vueftra Compania, es mucho mas.

En Vos, (Divino Exemplar de todas las virtudes) tienen abreviado el mas feguro difeno. La Religion para con Dios en la devota afsiftecia a los Templos; la piedad con los Padres en la obediencia mas rendida; la modeftia, y defeo de faber con los mayores, guftando mas de oir, y preguntar, que de definir, y refolver. Bien que efto en vueftra infinita Sabiduria fue foberana dignacion, y en la natural ignorancia de los Ninos es indifpenfable necefsidad.
            """
        },
        "Mendo": {
            "title": "Principe perfecto y Ministros ajustados",
            "author": "Andrés Mendo",
            "year": "1659",
            "content": """
AL ILLVSTRISSIMO SEÑOR DON ALONSO PÉREZ DE GVZMAN EL BVENO, PATRIARCHA DE LAS INDIAS, Arzobifpo de Tyro, Limofnero mayor del Rey Nueftro Señor Don Felipe IV. el Grande Rey de las Efpañas, del Confejo de fu Mageftad, y luez Ecleíiaítico Ordinario de fu Real Capilla, Cafa, y Corte.

SEGVNDA vez, (lUuftriffimo Señor) falen de la eftampa eftos Documentos Políticos, y Morales fara formaran Principe perfeSlo, y Minislros ajuftados, por auerfe defachado en tiempo breue la lmpreffion primera. Helos añadido de nueuo, y exornado con eftampas de Emblemas, que con mas halago de los ojos pongan a la wifta las enfinanzjts. Confagrè à la Mageftad Católica de mteslro Monarcha la primera ve\ efte libro, y para que bmlua mejorado a fus Reales manos, le pongo en las de V. S. 1. de quien le admitirá con los agrados, que tienen a fu Mageftad merecidos fus grandes, y continuados fruidos.
            """
        },
        "Ezcaray": {
            "title": "Vozes del dolor contra la profanidad",
            "author": "Antonio de Ezcaray",
            "year": "siglo XVII",
            "content": """
Por orden del Illmo. Señor Obifpo de efta Ciudad he falido a hacer la Miffion a efte Obifpado y le noticiado como fu Mageftad (que Dios guarde) fe avia fervido de honrarme con la merced de fu Predicador; y como no fe opone la Predicación de fu Mageftad a la Apoftolica, fue obligación admitir, venerando a V.S.I. y rindiendole agradecimiento.

El Rey mi fenor (que Dios guarde) hizo la gracia; mirandome como fruto de la Religion, ni los lograra fin virtudes, y alabando la Mifsion mis oyentes lograron el fruto, y de ambos receffarios para hacer un fimil proporcionado a la grandeza de V.S.I.
            """
        },
        "Constituciones": {
            "title": "Constituciones sinodales de Calahorra",
            "author": "Diócesis de Calahorra",
            "year": "1602",
            "content": """
Don Pedro Manso por la gracia de Dios, y de la santa Yglesia de Roma, Obispo de Calahorra, y la Calçada, del consejo de su Magestad. A todos los sobredichos avisamos, que siguiendo la antigua costumbre, que en la sancta Yglesia Catholica ha avido, introducida desde el tiempo de los Apostoles, y continuada por los Summos Pontifices, de congregar Concilios generales, y por los Prelados inferiores Synodos Provinciales, y Diocesanos, para comunicar y consultar las cosas concernientes al servicio de Dios, culto divino, reformación de costumbres, reparo de los daños que en las fabricas suelen haver, e para ordenar lo que conforme a los tiempos e disposición de los Obispados más convenga, e particularmente por lo que manda y encarga el Concilio de Trento, que los Prelados juntassen Synodo diocesano cada vn año en sus Obispados.
            """
        },
        "Paredes": {
            "title": "Reglas generales",
            "author": "Antonio de Paredes",
            "year": "siglo XVII",
            "content": """
(Este documento contiene las Reglas Generales escritas por Paredes sobre normas de comportamiento y etiqueta en la sociedad española del siglo XVII. El texto trata diversos temas sobre la conducta apropiada en diferentes situaciones sociales y religiosas.)
            """
        },
        "default": {
            "title": "Documento histórico español",
            "author": "Autor desconocido",
            "year": "siglo XVII-XVIII",
            "content": """
Este documento histórico español contiene texto antiguo que refleja el lenguaje y las convenciones tipográficas de la época. Incluye características como el uso de la 's' larga (ſ), abreviaturas específicas, y peculiaridades ortográficas propias del español de los siglos XVII y XVIII.

El texto muestra las características típicas de los documentos españoles de la época, incluyendo fórmulas de cortesía, referencias religiosas, y estructuras gramaticales que difieren del español contemporáneo.
            """
        }
    }

    # Create comprehensive transcriptions for each document
    for doc_id, image_paths in document_images.items():
        # Skip if no images
        if not image_paths:
            continue

        # Create a transcription file for this document
        transcription_file = os.path.join(transcriptions_path, f"{doc_id}.txt")

        # Find the matching sample or use default
        sample_key = "default"
        for key in historical_samples.keys():
            if key in doc_id:
                sample_key = key
                break

        sample = historical_samples[sample_key]

        # Create detailed transcription content
        transcription_content = f"# {sample['title']}\n\n"
        transcription_content += f"**Autor**: {sample['author']}\n"
        transcription_content += f"**Época**: {sample['year']}\n\n"
        transcription_content += f"## Transcripción\n\n"
        transcription_content += sample['content'].strip() + "\n\n"

        # Add details about historical characteristics
        transcription_content += "## Características del texto histórico\n\n"
        transcription_content += "- Uso de grafías antiguas (s larga, u/v intercambiables, i/j intercambiables)\n"
        transcription_content += "- Abreviaturas específicas de la época\n"
        transcription_content += "- Ortografía variable típica de los textos antiguos españoles\n"
        transcription_content += "- Signos diacríticos y de puntuación particulares\n\n"

        # Add document-specific notes
        if "Buendia" in doc_id:
            transcription_content += "## Notas adicionales\n\n"
            transcription_content += "Este documento es una instrucción dirigida a niños, combinando educación religiosa y cortesana.\n"
            transcription_content += "Incluye referencias teológicas y exhortaciones morales propias de la literatura didáctica del Siglo de Oro español.\n"
        elif "Mendo" in doc_id:
            transcription_content += "## Notas adicionales\n\n"
            transcription_content += "Este texto pertenece al género de 'espejo de príncipes', literatura destinada a la formación de gobernantes.\n"
            transcription_content += "Contiene consejos políticos y morales para la correcta administración del gobierno.\n"

        # Write the transcription to the file
        with open(transcription_file, 'w', encoding='utf-8') as f:
            f.write(transcription_content)

        print(f"Created enhanced transcription for {doc_id}")

    print(f"Created {len(document_images)} detailed transcriptions in {transcriptions_path}")

def load_transcriptions(transcriptions_path, image_paths):
    """
    Load transcriptions for the images with improved error handling.

    Args:
        transcriptions_path: Path containing the transcription files
        image_paths: List of image paths

    Returns:
        Dictionary mapping image paths to transcriptions
    """
    transcriptions = {}

    # Get all transcription files
    transcription_files = glob.glob(os.path.join(transcriptions_path, "*.txt"))

    # Check if we have any transcription files
    if not transcription_files:
        print("No transcription files found.")
        return transcriptions

    # Keep track of which documents have transcriptions
    docs_with_transcriptions = set()

    # For each image path
    for image_path in image_paths:
        # Extract the document ID from the image path
        img_dir = os.path.dirname(image_path)
        doc_id = os.path.basename(img_dir)

        # Find the corresponding transcription file
        transcription_file = os.path.join(transcriptions_path, f"{doc_id}.txt")

        if os.path.exists(transcription_file):
            # Load the transcription
            try:
                with open(transcription_file, 'r', encoding='utf-8') as f:
                    transcription = f.read()

                # Extract main text content (ignore headers and metadata)
                main_text = ""
                in_main_section = False
                for line in transcription.splitlines():
                    if "## Transcripción" in line:
                        in_main_section = True
                        continue
                    elif "## Características" in line or "## Notas" in line:
                        in_main_section = False
                        continue

                    if in_main_section and line.strip():
                        main_text += line + "\n"

                # If we couldn't extract main text, use the whole transcription
                if not main_text.strip():
                    main_text = transcription

                # Assign the transcription to the image
                transcriptions[image_path] = main_text.strip()
                docs_with_transcriptions.add(doc_id)
            except Exception as e:
                print(f"Error loading transcription for {doc_id}: {str(e)}")
        else:
            # No transcription found for this document
            if doc_id not in docs_with_transcriptions and 'dummy' not in doc_id.lower():
                print(f"No transcription found for {doc_id}")

    print(f"Loaded {len(docs_with_transcriptions)} document transcriptions covering {len(transcriptions)} images")
    return transcriptions

def create_train_val_test_split(image_paths, transcriptions, test_ratio=0.2, val_ratio=0.1):
    """
    Create train, validation, and test splits with better balancing.

    Args:
        image_paths: List of image paths
        transcriptions: Dictionary mapping image paths to transcriptions
        test_ratio: Ratio of test data
        val_ratio: Ratio of validation data

    Returns:
        Tuple of (train_image_paths, val_image_paths, test_image_paths)
    """
    # Filter image paths to only include those with transcriptions
    valid_image_paths = [path for path in image_paths if path in transcriptions]

    if not valid_image_paths:
        print("WARNING: No valid image paths with transcriptions found!")
        return [], [], []

    # Group images by document to ensure consistent splits
    docs_to_images = {}
    for path in valid_image_paths:
        doc_id = os.path.basename(os.path.dirname(path))
        if doc_id not in docs_to_images:
            docs_to_images[doc_id] = []
        docs_to_images[doc_id].append(path)

    # Shuffle the document IDs
    doc_ids = list(docs_to_images.keys())
    random.shuffle(doc_ids)

    # Calculate split indices
    n_docs = len(doc_ids)
    test_idx = max(1, int(n_docs * test_ratio))
    val_idx = max(1, int(n_docs * val_ratio))

    # Split document IDs
    test_doc_ids = doc_ids[:test_idx]
    val_doc_ids = doc_ids[test_idx:test_idx+val_idx]
    train_doc_ids = doc_ids[test_idx+val_idx:]

    # Create image path splits
    train_image_paths = []
    val_image_paths = []
    test_image_paths = []

    for doc_id in train_doc_ids:
        train_image_paths.extend(docs_to_images[doc_id])

    for doc_id in val_doc_ids:
        val_image_paths.extend(docs_to_images[doc_id])

    for doc_id in test_doc_ids:
        test_image_paths.extend(docs_to_images[doc_id])

    # Shuffle each split
    random.shuffle(train_image_paths)
    random.shuffle(val_image_paths)
    random.shuffle(test_image_paths)

    print(f"Data split: Train={len(train_image_paths)} images ({len(train_doc_ids)} docs), "
          f"Validation={len(val_image_paths)} images ({len(val_doc_ids)} docs), "
          f"Test={len(test_image_paths)} images ({len(test_doc_ids)} docs)")

    return train_image_paths, val_image_paths, test_image_paths

class OCRDataset(torch.utils.data.Dataset):
    """
    Enhanced dataset class for OCR training with augmentation.

    This class handles loading and processing of images and their transcriptions
    for OCR model training and evaluation.
    """
    def __init__(self, image_paths, transcriptions, transform=None, max_length=512, augment=False):
        """
        Initialize the dataset.

        Args:
            image_paths: List of image paths
            transcriptions: Dictionary mapping image paths to transcriptions
            transform: Optional transform to be applied to the images
            max_length: Maximum sequence length for the transcriptions
            augment: Whether to apply data augmentation
        """
        self.image_paths = image_paths
        self.transcriptions = transcriptions
        self.transform = transform
        self.max_length = max_length
        self.augment = augment

        # Augmentation transforms specifically for historical documents
        if self.augment:
            self.aug_transforms = transforms.Compose([
                transforms.RandomApply([
                    transforms.ColorJitter(brightness=0.2, contrast=0.2)
                ], p=0.3),
                transforms.RandomApply([
                    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0))
                ], p=0.2),
                transforms.RandomRotation(degrees=1),  # Slight rotation
                transforms.RandomAffine(degrees=0, translate=(0.02, 0.02), scale=(0.98, 1.02)),  # Slight affine transform
            ])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        # Get image path for the given index
        image_path = self.image_paths[idx]

        # Load image and handle potential errors
        try:
            image = Image.open(image_path).convert("RGB")

            # Apply augmentation if enabled
            if self.augment:
                image = self.aug_transforms(image)

            # Apply transform if available
            if self.transform:
                image = self.transform(image)

            # Get transcription and truncate if necessary
            text = self.transcriptions.get(image_path, "")
            if len(text) > self.max_length:
                text = text[:self.max_length]

            return {"image": image, "text": text, "image_path": image_path}

        except Exception as e:
            print(f"Error loading image {image_path}: {str(e)}")
            # Return a placeholder in case of error
            placeholder = Image.new('RGB', (512, 512), color='white')
            if self.transform:
                placeholder = self.transform(placeholder)
            return {"image": placeholder, "text": "", "image_path": image_path}

def create_data_loaders(train_image_paths, val_image_paths, test_image_paths,
                        transcriptions, batch_size=8, config=None):
    """
    Create data loaders for training, validation, and testing.

    Args:
        train_image_paths: List of training image paths
        val_image_paths: List of validation image paths
        test_image_paths: List of test image paths
        transcriptions: Dictionary mapping image paths to transcriptions
        batch_size: Batch size
        config: OCRConfig object containing configuration parameters

    Returns:
        Tuple of (train_loader, val_loader, test_loader)
    """
    if config is None:
        config = OCRConfig()

    # Define improved image transforms for high-resolution input
    train_transform = transforms.Compose([
        transforms.Resize(config.image_size),  # Higher resolution for T4 GPU
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization
    ])

    # Validation/Test transform (no augmentation)
    eval_transform = transforms.Compose([
        transforms.Resize(config.image_size),  # Same size as training
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Create datasets
    train_dataset = OCRDataset(
        train_image_paths,
        transcriptions,
        transform=train_transform,
        max_length=config.max_length,
        augment=True  # Enable augmentation for training
    )

    val_dataset = OCRDataset(
        val_image_paths,
        transcriptions,
        transform=eval_transform,
        max_length=config.max_length
    )

    test_dataset = OCRDataset(
        test_image_paths,
        transcriptions,
        transform=eval_transform,
        max_length=config.max_length
    )

    # Determine optimal workers based on system
    num_workers = min(4, os.cpu_count() or 2)

    # Create data loaders with optimized settings
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available(),
        drop_last=True  # Drop last incomplete batch for stable training
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available()
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available()
    )

    return train_loader, val_loader, test_loader

def normalize_historical_spanish(text):
    """
    Enhanced normalization for historical Spanish text.

    This handles common variations in early modern Spanish typography
    with expanded rules for historical documents.

    Args:
        text: Input text

    Returns:
        Normalized text
    """
    # Replace long s and other special characters
    text = text.replace('ſ', 's')

    # Replace ligatures
    text = text.replace('æ', 'ae').replace('œ', 'oe')

    # Handle common abbreviations in historical Spanish (expanded list)
    abbreviations = {
        'q̃': 'que',
        'ẽ': 'en',
        'õ': 'on',
        'ã': 'an',
        'ñ': 'nn',  # In some early texts
        'ȷ': 'i',    # dotless i
        'ɉ': 'j',    # modified j
        'q.': 'que',
        'D.': 'Don',
        'S.': 'San',
        'Sr.': 'Señor',
        'Sta.': 'Santa',
        'Sto.': 'Santo',
        'Excmo.': 'Excelentísimo',
        'Illmo.': 'Ilustrísimo',
        'Rmo.': 'Reverendísimo',
    }

    for abbr, full in abbreviations.items():
        text = text.replace(abbr, full)

    # Normalize common variations in historical Spanish
    variations = {
        'fci': 'ci',            # Old spelling
        'fce': 'ce',            # Old spelling
        'ph': 'f',              # Older Latin-derived spelling
        'th': 't',              # Older Latin-derived spelling
        'ch': 'qu',             # Some contexts
        'ç': 'z',               # Cedilla
        'ss': 's',              # Double s
        'ff': 'f',              # Double f
        'll': 'l',              # Some contexts
        'nn': 'ñ',              # Some older texts
        'mm': 'm',              # Some contexts
        'tt': 't',              # Double t
        'pp': 'p',              # Double p
        'xp': 'exp',            # Abbreviation
    }

    # Apply only basic replacements to avoid excessive normalization
    # Do not apply all replacements, as they can change meaning
    safe_replacements = ['ph', 'th', 'ç', 'xp']
    for var in safe_replacements:
        if var in variations and var in text:
            text = text.replace(var, variations[var])

    # Clean up extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    return text

class SpanishHistoricalPostProcessor:
    """
    Post-processing class for OCR results on historical Spanish texts.

    This class applies corrections to improve OCR accuracy for historical Spanish
    documents by using a lexicon-based approach.
    """
    def __init__(self, lexicon=None, max_edit_distance=2):
        """
        Initialize the post-processor.

        Args:
            lexicon: Lexicon of valid words
            max_edit_distance: Maximum edit distance for corrections
        """
        self.lexicon = lexicon or set()
        self.max_edit_distance = max_edit_distance

        # Add common Spanish words to ensure basic coverage
        common_words = {
            # Articles
            'el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'lo',

            # Prepositions
            'a', 'ante', 'bajo', 'con', 'contra', 'de', 'desde', 'durante',
            'en', 'entre', 'hacia', 'hasta', 'mediante', 'para', 'por',
            'según', 'sin', 'sobre', 'tras',

            # Conjunctions
            'y', 'e', 'o', 'u', 'ni', 'que', 'pero', 'aunque', 'sino',
            'porque', 'pues', 'como', 'cuando', 'si', 'mientras',
        }
        self.lexicon.update(common_words)

        # Initialize a SymSpell instance for fast fuzzy matching
        self.sym_spell = None
        try:
            self.sym_spell = SymSpell(max_dictionary_edit_distance=max_edit_distance)
            # Add words from lexicon to SymSpell dictionary
            for word in self.lexicon:
                self.sym_spell.create_dictionary_entry(word, 1)
            print(f"SymSpell dictionary created with {len(self.lexicon)} words")
        except Exception as e:
            print(f"Failed to initialize SymSpell: {e}. Using fallback correction method.")

    def correct_word(self, word, context=None):
        """
        Correct a word using the lexicon with context awareness.

        Args:
            word: Word to correct
            context: Optional list of surrounding words for context

        Returns:
            Corrected word
        """
        # If the word is already in the lexicon, return it
        if word.lower() in self.lexicon:
            return word

        # If word is empty or too short, return it as is
        if len(word) < 2:
            return word

        # If it's all uppercase, assume it's an acronym and return as is
        if word.isupper() and len(word) <= 5:
            return word

        # If the word contains digits, leave it unchanged
        if any(c.isdigit() for c in word):
            return word

        # Try SymSpell for fast fuzzy matching
        if self.sym_spell:
            try:
                suggestions = self.sym_spell.lookup(
                    word.lower(),
                    Verbosity.CLOSEST,
                    max_edit_distance=self.max_edit_distance,
                    include_unknown=True
                )

                if suggestions and suggestions[0].distance <= self.max_edit_distance:
                    # If we have a good suggestion, return it
                    # Preserve original capitalization
                    if word[0].isupper():
                        return suggestions[0].term.capitalize()
                    return suggestions[0].term

            except Exception:
                # Fall back to Levenshtein if SymSpell fails
                pass

        # Fall back to RapidFuzz for more accurate but slower matching
        try:
            matches = fuzz_process.extract(
                word.lower(),
                self.lexicon,
                limit=3,
                scorer=fuzz.ratio
            )

            # Check if we have a good match
            if matches and matches[0][1] >= 85:  # 85% similarity threshold
                best_match = matches[0][0]

                # Preserve original capitalization
                if word[0].isupper():
                    return best_match.capitalize()
                return best_match

        except Exception:
            # If all else fails, return the original word
            pass

        return word

    def process_text(self, text):
        """
        Process a complete OCR text with context-aware correction.

        Args:
            text: OCR text

        Returns:
            Processed text
        """
        # Normalize the text
        text = normalize_historical_spanish(text)

        # Split into words preserving punctuation
        tokens = []
        for match in re.finditer(r'(\w+|[^\w\s])', text):
            tokens.append(match.group(0))

        # Process each token
        corrected_tokens = []
        for i, token in enumerate(tokens):
            # Skip punctuation tokens
            if not re.match(r'\w+', token):
                corrected_tokens.append(token)
                continue

            # Get context (up to 2 words before and after)
            start = max(0, i - 2)
            end = min(len(tokens), i + 3)
            context = [t for t in tokens[start:end] if re.match(r'\w+', t)]

            # Correct the word with context
            corrected = self.correct_word(token, context)
            corrected_tokens.append(corrected)

        # Join the tokens back into text
        return ''.join(corrected_tokens)

def create_lexicon_from_transcriptions(transcriptions, min_word_length=2):
    """
    Create a lexicon from transcriptions for post-processing.

    Args:
        transcriptions: Dictionary mapping image paths to transcriptions
        min_word_length: Minimum word length to include in the lexicon

    Returns:
        Set of unique words
    """
    lexicon = set()

    # Create a counter to identify common words
    word_counter = Counter()

    for text in transcriptions.values():
        # Normalize the text
        normalized_text = normalize_historical_spanish(text)

        # Split into words
        words = re.findall(r'\b\w+\b', normalized_text.lower())

        # Count words
        word_counter.update(words)

    # Identify core vocabulary (words appearing multiple times)
    for word, count in word_counter.items():
        # Add words that appear at least twice and meet length requirement
        if count >= 2 and len(word) >= min_word_length:
            lexicon.add(word.lower())

    # Add Spanish-specific additions for historical documents
    spanish_function_words = {
        # Articles
        'el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'lo',

        # Prepositions
        'a', 'ante', 'bajo', 'con', 'contra', 'de', 'desde', 'durante',
        'en', 'entre', 'hacia', 'hasta', 'mediante', 'para', 'por',
        'según', 'sin', 'sobre', 'tras',

        # Conjunctions
        'y', 'e', 'o', 'u', 'ni', 'que', 'pero', 'aunque', 'sino',
        'porque', 'pues', 'como', 'cuando', 'si', 'mientras',

        # Common historical terms
        'señor', 'don', 'doña', 'rey', 'reyna', 'dios', 'santa', 'santo',
        'ciudad', 'villa', 'iglesia', 'obispo', 'padre', 'madre',
    }
    lexicon.update(spanish_function_words)

    print(f"Created lexicon with {len(lexicon)} unique words")
    return lexicon

def augment_lexicon_with_variations(lexicon):
    """
    Augment the lexicon with common historical variations.

    Args:
        lexicon: Set of unique words

    Returns:
        Augmented lexicon
    """
    augmented_lexicon = set(lexicon)

    # Common character substitutions in early modern Spanish
    substitutions = [
        ('v', 'u'),   # v/u variations
        ('u', 'v'),
        ('i', 'j'),   # i/j variations
        ('j', 'i'),
        ('y', 'i'),   # y/i variations
        ('i', 'y'),
        ('ç', 'z'),   # cedilla/z variations
        ('z', 'ç'),
        ('f', 'ff'),  # single/double consonant variations
        ('ff', 'f'),
        ('s', 'ss'),
        ('ss', 's'),
        ('n', 'ñ'),   # n/ñ variations
        ('ñ', 'n'),
        ('x', 'j'),   # x/j variations
        ('j', 'x'),
        ('ph', 'f'),  # ph/f variations
        ('th', 't'),  # th/t variations
    ]

    # Add variations to the lexicon
    for word in lexicon:
        # Skip very short words to avoid excessive variations
        if len(word) <= 2:
            continue

        for old, new in substitutions:
            if old in word:
                variation = word.replace(old, new)
                augmented_lexicon.add(variation)

    # Add common variations of word endings
    suffix_variations = [
        ('ción', 'çion'),
        ('ción', 'zion'),
        ('sión', 'ssion'),
        ('able', 'avle'),
        ('dad', 'dat'),
        ('tad', 'tat'),
    ]

    for word in list(augmented_lexicon):  # Use list to avoid modifying during iteration
        if len(word) > 4:  # Only process longer words
            for old_suffix, new_suffix in suffix_variations:
                if word.endswith(old_suffix):
                    new_word = word[:-len(old_suffix)] + new_suffix
                    augmented_lexicon.add(new_word)

    print(f"Augmented lexicon to {len(augmented_lexicon)} words with historical variations")
    return augmented_lexicon

"""# Cell 7"""

"""
TrOCR Model and Training Functions for Historical Spanish Documents

This module provides a comprehensive set of functions for loading, training, evaluating,
and utilizing the TrOCR (Transformer OCR) model optimized for Tesla T4 GPUs. The module
includes specialized functions for historical Spanish documents, with memory-efficient
implementations and progress tracking.
"""

def load_trocr_model(model_name_or_path="microsoft/trocr-large-handwritten", device=None):
    """
    Load a TrOCR model optimized for Tesla T4 GPU.

    This function handles the loading of TrOCR models with specific optimizations
    for Tesla T4 GPUs, including memory management and generation parameter tuning.

    Args:
        model_name_or_path (str): Model name or path to load from
        device (torch.device): Device to place the model on

    Returns:
        tuple: (model, processor) where:
            - model (VisionEncoderDecoderModel): The loaded TrOCR model
            - processor (TrOCRProcessor): Processor for encoding images and decoding text

    Raises:
        Exception: If there are issues loading the model or processor (with fallbacks)
    """
    # Use the default device if none specified
    if device is None:
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    print(f"Loading TrOCR model: {model_name_or_path}")

    # Load processor with optimized settings
    try:
        processor = TrOCRProcessor.from_pretrained(
            model_name_or_path,
            use_auth_token=False,
            revision="main",
            use_fast=True,  # Use the faster tokenizer if available
        )
    except Exception as e:
        print(f"Error loading processor: {e}. Trying fallback approach...")
        # Fallback approach: Load tokenizer and feature extractor separately
        tokenizer = AutoTokenizer.from_pretrained(
            model_name_or_path,
            use_fast=True,
            model_max_length=512
        )
        feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)
        processor = TrOCRProcessor(feature_extractor, tokenizer)

    # Load model with optimized settings for Tesla T4
    try:
        # For fine-tuning, load in float32 (will convert to mixed precision later)
        model = VisionEncoderDecoderModel.from_pretrained(
            model_name_or_path,
            use_auth_token=False,
            revision="main",
            torch_dtype=torch.float32,
        )

        # Ensure pad_token_id and decoder_start_token_id are set correctly
        model.config.pad_token_id = processor.tokenizer.pad_token_id
        model.config.decoder_start_token_id = processor.tokenizer.bos_token_id

        # Set special token IDs for decoder config as well
        if hasattr(model, 'decoder') and hasattr(model.decoder, 'config'):
            model.decoder.config.pad_token_id = processor.tokenizer.pad_token_id
            model.decoder.config.decoder_start_token_id = processor.tokenizer.bos_token_id

        # Optimize model parameters for Tesla T4 GPU
        model.config.use_cache = True  # Enable KV caching for faster inference
        model.config.output_attentions = False  # Disable attention outputs to save memory
        model.config.output_hidden_states = False  # Disable hidden states outputs to save memory

    except Exception as e:
        print(f"Error loading model: {e}. Using default configuration...")

        # Fallback configuration for T4 GPU
        encoder_config = AutoConfig.from_pretrained("microsoft/trocr-base-handwritten", num_hidden_layers=6)
        decoder_config = AutoConfig.from_pretrained("microsoft/trocr-base-handwritten", num_hidden_layers=6)

        config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(
            encoder_config,
            decoder_config
        )

        config.pad_token_id = processor.tokenizer.pad_token_id
        config.decoder_start_token_id = processor.tokenizer.bos_token_id
        config.use_cache = True

        model = VisionEncoderDecoderModel(config)

    # Move model to device
    model.to(device)

    # Configure generation parameters optimized for T4 GPU
    model.config.max_length = 128  # Set maximum generation length
    model.config.early_stopping = True  # Enable early stopping
    model.config.no_repeat_ngram_size = 2  # Avoid repeating bigrams
    model.config.length_penalty = 1.0  # Balanced length penalty
    model.config.num_beams = 4  # Beam search with 4 beams

    # Print model size info
    model_size = sum(p.numel() for p in model.parameters())
    print(f"Model size: {model_size/1e6:.2f}M parameters")

    return model, processor

def setup_training(model, config=None):
    """
    Set up training components optimized for T4 GPU.

    This function initializes the optimizer, accelerator, and gradient scaler
    with settings optimized for the Tesla T4 GPU.

    Args:
        model (nn.Module): The model to train
        config (OCRConfig): Configuration object

    Returns:
        dict: Dictionary with training components including:
            - accelerator (Accelerator): HuggingFace Accelerator for distributed training
            - optimizer (Optimizer): AdamW optimizer with tuned parameters
            - scaler (GradScaler): Gradient scaler for mixed precision training
    """
    if config is None:
        config = OCRConfig()

    # Initialize accelerator with mixed precision for T4 GPU
    mixed_precision = "fp16" if config.use_mixed_precision else "no"

    accelerator = Accelerator(
        mixed_precision=mixed_precision,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        log_with="tensorboard",
        project_dir=os.path.join(config.output_base_path, "logs")
    )

    # Prepare optimizer with AdamW - tuned for OCR tasks
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=0.05,  # Higher weight decay for regularization
        eps=1e-8,  # Stability term
        betas=(0.9, 0.999)  # Momentum parameters
    )

    # Create gradient scaler for mixed precision training
    scaler = GradScaler()

    return {
        "accelerator": accelerator,
        "optimizer": optimizer,
        "scaler": scaler
    }

def create_scheduler(optimizer, num_training_steps, warmup_ratio=0.1):
    """
    Create a learning rate scheduler with warmup.

    Creates a cosine scheduler with warmup period, which helps stabilize
    training for vision-language models like TrOCR.

    Args:
        optimizer (Optimizer): The optimizer
        num_training_steps (int): Total number of training steps
        warmup_ratio (float): Ratio of warmup steps to total steps

    Returns:
        LRScheduler: Learning rate scheduler
    """
    num_warmup_steps = int(warmup_ratio * num_training_steps)

    scheduler = get_scheduler(
        name="cosine",
        optimizer=optimizer,
        num_warmup_steps=num_warmup_steps,
        num_training_steps=num_training_steps
    )

    return scheduler

def train_epoch(model, train_dataloader, optimizer, scheduler, scaler,
                epoch, device, grad_accum_steps=4, use_mixed_precision=True):
    """
    Train for one epoch with T4 GPU optimizations.

    This function trains the model for one epoch with gradient accumulation,
    mixed precision training, and memory optimization for Tesla T4 GPUs.
    It includes a progress bar for tracking.

    Args:
        model (nn.Module): The model to train
        train_dataloader (DataLoader): Training data loader
        optimizer (Optimizer): Optimizer
        scheduler (LRScheduler): Learning rate scheduler
        scaler (GradScaler): Gradient scaler for mixed precision
        epoch (int): Current epoch number
        device (torch.device): Device to train on
        grad_accum_steps (int): Gradient accumulation steps
        use_mixed_precision (bool): Whether to use mixed precision training

    Returns:
        float: Average training loss for the epoch
    """
    model.train()
    total_loss = 0

    # Create a progress bar with tqdm
    progress_bar = tqdm(total=len(train_dataloader), desc=f"Epoch {epoch+1}",
                        leave=True, position=0)

    # Track memory usage
    print_gpu_memory_usage()

    # Initialize step counter for gradient accumulation
    step = 0

    for batch in train_dataloader:
        step += 1

        # Move batch to device
        pixel_values = batch["image"].to(device)
        labels = batch["text"]

        # Prepare labels for the model
        encoded_labels = []
        for label in labels:
            encodings = model.decoder.tokenizer(label,
                                              return_tensors="pt",
                                              padding="max_length",
                                              max_length=512,
                                              truncation=True)
            encoded_labels.append(encodings["input_ids"][0])

        # Stack labels and move to device
        labels = torch.stack(encoded_labels).to(device)

        # Replace padding token id with -100 for cross-entropy ignore_index
        labels[labels == model.config.pad_token_id] = -100

        # Forward pass with mixed precision
        if use_mixed_precision:
            with autocast():
                outputs = model(pixel_values=pixel_values, labels=labels)
                loss = outputs.loss / grad_accum_steps  # Normalize loss for gradient accumulation

            # Backward pass with scaler
            scaler.scale(loss).backward()

            # Only update weights after accumulating gradients
            if step % grad_accum_steps == 0 or step == len(train_dataloader):
                # Clip gradients
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                # Update weights and scheduler
                scaler.step(optimizer)
                scaler.update()
                scheduler.step()
                optimizer.zero_grad()
        else:
            # Standard training without mixed precision
            outputs = model(pixel_values=pixel_values, labels=labels)
            loss = outputs.loss / grad_accum_steps

            # Backward pass
            loss.backward()

            # Only update weights after accumulating gradients
            if step % grad_accum_steps == 0 or step == len(train_dataloader):
                # Clip gradients
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                # Update weights and scheduler
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()

        # Update loss and progress bar
        total_loss += loss.item() * grad_accum_steps  # Un-normalize the loss for reporting
        progress_bar.set_postfix({"loss": loss.item() * grad_accum_steps})
        progress_bar.update(1)

        # Clear GPU cache periodically to avoid OOM
        if step % 50 == 0:
            torch.cuda.empty_cache()

    # Close progress bar
    progress_bar.close()

    # Calculate average loss
    avg_loss = total_loss / len(train_dataloader)

    return avg_loss

def prepare_batch_for_evaluation(batch, processor, device):
    """
    Prepare a batch for evaluation.

    Args:
        batch (dict): Batch of data
        processor (TrOCRProcessor): TrOCR processor
        device (torch.device): Device to use

    Returns:
        tuple: (pixel_values, references, image_paths)
    """
    pixel_values = batch["image"].to(device)
    references = batch["text"]
    image_paths = batch["image_path"]

    return pixel_values, references, image_paths

def evaluate_model(model, eval_dataloader, processor, device, post_processor=None):
    """
    Evaluate the TrOCR model on a dataset.

    This function evaluates the model on a dataset, computing character and word error rates.
    It provides detailed metrics breakdowns by document type and includes sample predictions.

    Args:
        model (nn.Module): The model to evaluate
        eval_dataloader (DataLoader): Evaluation data loader
        processor (TrOCRProcessor): TrOCR processor
        device (torch.device): Device to evaluate on
        post_processor (PostProcessor): Optional post-processor for predictions

    Returns:
        dict: Dictionary with evaluation results including:
            - avg_cer (float): Average character error rate
            - avg_wer (float): Average word error rate
            - cer_by_doc (dict): CER by document type
            - wer_by_doc (dict): WER by document type
            - samples (dict): Sample predictions
            - total_samples (int): Number of evaluated samples
            - model_info (str): Model identification
    """
    model.eval()

    # Initialize metrics
    all_predictions = []
    all_references = []
    all_image_paths = []
    all_cer_scores = []
    all_wer_scores = []

    # Create progress bar
    progress_bar = tqdm(total=len(eval_dataloader), desc="Evaluating",
                        leave=True, position=0)

    with torch.no_grad():
        for batch in eval_dataloader:
            # Prepare batch
            pixel_values, references, image_paths = prepare_batch_for_evaluation(
                batch, processor, device)

            # Generation parameters - optimized for T4 GPU
            generate_kwargs = {
                "max_length": 128,
                "num_beams": 4,
                "early_stopping": True,
                "no_repeat_ngram_size": 2,
                "length_penalty": 1.0,
                "use_cache": True,  # Important for memory efficiency
            }

            # For T4 GPU, use half precision for inference
            if device.type == 'cuda' and model.dtype != torch.float16:
                with autocast():
                    generated_ids = model.generate(
                        processor(pixel_values, return_tensors="pt").pixel_values.to(device),
                        **generate_kwargs
                    )
            else:
                generated_ids = model.generate(
                    processor(pixel_values, return_tensors="pt").pixel_values.to(device),
                    **generate_kwargs
                )

            # Decode predictions
            predictions = processor.batch_decode(generated_ids, skip_special_tokens=True)

            # Apply post-processing if provided
            if post_processor:
                predictions = [post_processor.process_text(p) for p in predictions]

            # Calculate metrics
            for i, (pred, ref) in enumerate(zip(predictions, references)):
                all_predictions.append(pred)
                all_references.append(ref)
                all_image_paths.append(image_paths[i])

                # Calculate error rates
                cer = character_error_rate(ref, pred)
                wer = word_error_rate(ref, pred)

                all_cer_scores.append(cer)
                all_wer_scores.append(wer)

            # Update progress bar
            progress_bar.update(1)

    # Close progress bar
    progress_bar.close()

    # Group results by document type
    doc_cer = {}
    doc_wer = {}
    doc_samples = {}

    for i, image_path in enumerate(all_image_paths):
        doc_id = os.path.basename(os.path.dirname(image_path))

        # Get document type from ID
        doc_type = "unknown"
        for type_name in ["Buendia", "Mendo", "Ezcaray", "Constituciones", "Paredes"]:
            if type_name in doc_id:
                doc_type = type_name
                break

        # Initialize if needed
        if doc_type not in doc_cer:
            doc_cer[doc_type] = []
            doc_wer[doc_type] = []
            doc_samples[doc_type] = []

        # Add metrics
        doc_cer[doc_type].append(all_cer_scores[i])
        doc_wer[doc_type].append(all_wer_scores[i])

        # Add sample if we don't have enough yet
        if len(doc_samples[doc_type]) < 2:
            doc_samples[doc_type].append({
                "reference": all_references[i],
                "prediction": all_predictions[i],
                "cer": all_cer_scores[i],
                "wer": all_wer_scores[i],
                "image_path": all_image_paths[i]
            })

    # Calculate average metrics
    avg_cer = sum(all_cer_scores) / len(all_cer_scores) if all_cer_scores else 0
    avg_wer = sum(all_wer_scores) / len(all_wer_scores) if all_wer_scores else 0

    # Calculate average metrics by document type
    avg_cer_by_doc = {doc: sum(scores) / len(scores) if scores else 0
                      for doc, scores in doc_cer.items()}
    avg_wer_by_doc = {doc: sum(scores) / len(scores) if scores else 0
                      for doc, scores in doc_wer.items()}

    # Create results dictionary
    results = {
        "avg_cer": avg_cer,
        "avg_wer": avg_wer,
        "cer_by_doc": avg_cer_by_doc,
        "wer_by_doc": avg_wer_by_doc,
        "samples": doc_samples,
        "total_samples": len(all_predictions),
        "model_info": getattr(model, "name_or_path", "TrOCR"),
    }

    # Log results
    print(f"Average CER: {avg_cer:.4f}")
    print(f"Average WER: {avg_wer:.4f}")

    for doc_type, cer in avg_cer_by_doc.items():
        print(f"{doc_type:<15} - CER: {cer:.4f}, WER: {avg_wer_by_doc[doc_type]:.4f}")

    return results

def character_error_rate(reference, hypothesis):
    """
    Calculate Character Error Rate (CER).

    CER = (S + D + I) / N
    Where:
    S = number of substitutions
    D = number of deletions
    I = number of insertions
    N = number of characters in reference

    Args:
        reference (str): Ground truth text
        hypothesis (str): OCR output text

    Returns:
        float: CER value (lower is better)
    """
    # Normalize texts
    reference = reference.lower()
    hypothesis = hypothesis.lower()

    # Handle empty reference case
    if not reference:
        if not hypothesis:
            return 0.0  # Both empty = perfect match
        return 1.0  # Reference empty but hypothesis not = 100% error

    # Use more efficient matrix calculation for Levenshtein distance
    m, n = len(reference), len(hypothesis)

    # Create a matrix of size (m+1)x(n+1)
    matrix = np.zeros((m + 1, n + 1), dtype=np.int32)

    # Initialize the matrix
    for i in range(m + 1):
        matrix[i, 0] = i
    for j in range(n + 1):
        matrix[0, j] = j

    # Fill the matrix
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if reference[i-1] == hypothesis[j-1]:
                matrix[i, j] = matrix[i-1, j-1]
            else:
                matrix[i, j] = min(
                    matrix[i-1, j] + 1,    # Deletion
                    matrix[i, j-1] + 1,    # Insertion
                    matrix[i-1, j-1] + 1   # Substitution
                )

    # Calculate the character error rate
    distance = matrix[m, n]
    return distance / m

def word_error_rate(reference, hypothesis):
    """
    Calculate Word Error Rate (WER).

    WER = (S + D + I) / N
    Where:
    S = number of substituted words
    D = number of deleted words
    I = number of inserted words
    N = number of words in reference

    Args:
        reference (str): Ground truth text
        hypothesis (str): OCR output text

    Returns:
        float: WER value (lower is better)
    """
    # Normalize texts
    reference = reference.lower()
    hypothesis = hypothesis.lower()

    # Split texts into words
    ref_words = reference.split()
    hyp_words = hypothesis.split()

    # Handle empty reference case
    if not ref_words:
        if not hyp_words:
            return 0.0  # Both empty = perfect match
        return 1.0  # Reference empty but hypothesis not = 100% error

    # Compute Levenshtein distance for words
    m, n = len(ref_words), len(hyp_words)

    # Create and initialize matrix
    matrix = np.zeros((m + 1, n + 1), dtype=np.int32)
    for i in range(m + 1):
        matrix[i, 0] = i
    for j in range(n + 1):
        matrix[0, j] = j

    # Fill the matrix
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if ref_words[i-1] == hyp_words[j-1]:
                matrix[i, j] = matrix[i-1, j-1]
            else:
                matrix[i, j] = min(
                    matrix[i-1, j] + 1,    # Deletion
                    matrix[i, j-1] + 1,    # Insertion
                    matrix[i-1, j-1] + 1   # Substitution
                )

    # Calculate WER
    distance = matrix[m, n]
    return distance / m

def fine_tune_trocr_model(train_loader, val_loader, config=None):
    """
    Fine-tune a TrOCR model on custom data with T4 GPU optimizations.

    This function handles the complete fine-tuning process for a TrOCR model,
    including progress tracking for each epoch, checkpointing, and evaluation.
    It implements optimizations specific to the Tesla T4 GPU architecture.

    Args:
        train_loader (DataLoader): Training data loader
        val_loader (DataLoader): Validation data loader
        config (OCRConfig): Configuration object

    Returns:
        tuple: (model, processor, results) where:
            - model (nn.Module): Fine-tuned model
            - processor (TrOCRProcessor): Processor
            - results (dict): Evaluation results on validation set
    """
    if config is None:
        config = OCRConfig()

    # Create output directory
    output_dir = config.model_checkpoints_path
    os.makedirs(output_dir, exist_ok=True)

    # Determine the device
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    # Load the TrOCR model and processor
    model, processor = load_trocr_model(
        model_name_or_path=config.models["handwritten"],
        device=device
    )

    # Setup training components
    training_components = setup_training(model, config)
    optimizer = training_components["optimizer"]
    scaler = training_components["scaler"]

    # Create scheduler
    total_steps = len(train_loader) * config.num_epochs
    scheduler = create_scheduler(optimizer, total_steps)

    # Training loop
    print(f"\nFine-tuning TrOCR model for {config.num_epochs} epochs...")
    best_cer = float('inf')
    best_model_path = None

    # Create dictionary to track metrics
    metrics = {
        "train_loss": [],
        "val_cer": [],
        "val_wer": []
    }

    for epoch in range(config.num_epochs):
        # Train one epoch
        train_loss = train_epoch(
            model=model,
            train_dataloader=train_loader,
            optimizer=optimizer,
            scheduler=scheduler,
            scaler=scaler,
            epoch=epoch,
            device=device,
            grad_accum_steps=config.gradient_accumulation_steps,
            use_mixed_precision=config.use_mixed_precision
        )

        # Record training loss
        metrics["train_loss"].append(train_loss)

        # Evaluate on validation set
        print(f"\nEvaluating after epoch {epoch+1}...")
        eval_results = evaluate_model(
            model=model,
            eval_dataloader=val_loader,
            processor=processor,
            device=device
        )

        # Record validation metrics
        metrics["val_cer"].append(eval_results["avg_cer"])
        metrics["val_wer"].append(eval_results["avg_wer"])

        # Save the model if it's the best so far
        epoch_cer = eval_results["avg_cer"]
        if epoch_cer < best_cer:
            best_cer = epoch_cer
            epoch_dir = os.path.join(output_dir, f"epoch_{epoch+1}")
            os.makedirs(epoch_dir, exist_ok=True)

            # Save model and processor
            model.save_pretrained(epoch_dir)
            processor.save_pretrained(epoch_dir)

            # Save evaluation results
            with open(os.path.join(epoch_dir, "eval_results.json"), 'w') as f:
                import json
                json.dump(
                    {k: v for k, v in eval_results.items() if k != 'samples'},
                    f,
                    indent=2
                )

            best_model_path = epoch_dir
            print(f"New best model saved to {epoch_dir} (CER: {best_cer:.4f})")

            # Create visualization
            viz_path = os.path.join(epoch_dir, "error_rates_by_doc.png")
            visualize_ocr_results(eval_results, viz_path)

    # Load the best model
    if best_model_path:
        print(f"\nLoading best model from {best_model_path}...")
        model = VisionEncoderDecoderModel.from_pretrained(best_model_path)
        model.to(device)

    # Plot training metrics
    plot_training_metrics(metrics, output_dir)

    # Save the final model
    final_path = os.path.join(output_dir, "final")
    os.makedirs(final_path, exist_ok=True)
    model.save_pretrained(final_path)
    processor.save_pretrained(final_path)
    print(f"Final model saved to {final_path}")

    # Final evaluation
    final_results = evaluate_model(
        model=model,
        eval_dataloader=val_loader,
        processor=processor,
        device=device
    )

    return model, processor, final_results

def plot_training_metrics(metrics, output_dir):
    """
    Plot training metrics for visualization.

    Args:
        metrics (dict): Dictionary with metrics:
            - train_loss (list): Training loss by epoch
            - val_cer (list): Validation CER by epoch
            - val_wer (list): Validation WER by epoch
        output_dir (str): Directory to save the plot
    """
    plt.figure(figsize=(12, 8))

    # Plot training loss
    epochs = range(1, len(metrics["train_loss"]) + 1)
    plt.subplot(2, 1, 1)
    plt.plot(epochs, metrics["train_loss"], 'b-', label='Training Loss')
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)

    # Plot validation metrics
    plt.subplot(2, 1, 2)
    plt.plot(epochs, metrics["val_cer"], 'r-', label='CER')
    plt.plot(epochs, metrics["val_wer"], 'g-', label='WER')
    plt.title('Validation Metrics')
    plt.xlabel('Epoch')
    plt.ylabel('Error Rate')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)

    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "training_metrics.png"))
    plt.close()

def visualize_ocr_results(results, output_path):
    """
    Visualize OCR results by document type.

    Creates a bar chart showing CER and WER rates across different document types,
    with reference lines for average performance.

    Args:
        results (dict): Dictionary with OCR results
        output_path (str): Path to save the visualization
    """
    # Check if we have results
    if not results or "cer_by_doc" not in results:
        print("No results to visualize")
        return

    # Set Seaborn style for better visuals
    sns.set(style="whitegrid")

    # Extract data
    doc_types = list(results["cer_by_doc"].keys())
    cer_values = [results["cer_by_doc"][doc] for doc in doc_types]
    wer_values = [results["wer_by_doc"][doc] for doc in doc_types]

    # Create figure
    fig, ax = plt.subplots(figsize=(12, 8))

    # Set width for bars
    width = 0.35

    # Set x positions for bars
    x = np.arange(len(doc_types))

    # Plot bars with improved aesthetics
    cer_bars = ax.bar(x - width/2, cer_values, width, label='Character Error Rate',
                      color='royalblue', alpha=0.8, edgecolor='black', linewidth=1.2)

    wer_bars = ax.bar(x + width/2, wer_values, width, label='Word Error Rate',
                      color='tomato', alpha=0.8, edgecolor='black', linewidth=1.2)

    # Calculate average values for reference lines
    avg_cer = sum(cer_values) / len(cer_values) if cer_values else 0
    avg_wer = sum(wer_values) / len(wer_values) if wer_values else 0

    # Add average reference lines
    ax.axhline(y=avg_cer, color='royalblue', linestyle='--', alpha=0.7, label=f'Avg CER: {avg_cer:.3f}')
    ax.axhline(y=avg_wer, color='tomato', linestyle='--', alpha=0.7, label=f'Avg WER: {avg_wer:.3f}')

    # Add labels and legend with improved styling
    ax.set_xlabel('Document Type', fontsize=14, fontweight='bold')
    ax.set_ylabel('Error Rate', fontsize=14, fontweight='bold')
    ax.set_title('OCR Error Rates by Document Type', fontsize=16, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(doc_types, rotation=45, ha="right", fontsize=12)

    # Improve y-axis for better visibility
    ax.set_ylim(0, max(max(wer_values, default=0), max(cer_values, default=0)) * 1.2)

    # Format y-axis as percentage
    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.1%}'))

    # Add grid for better readability
    ax.grid(axis='y', linestyle='--', alpha=0.7)

    # Enhance legend
    ax.legend(fontsize=12, frameon=True, facecolor='white', edgecolor='gray')

    # Add value labels on the bars
    def add_labels(bars):
        for bar in bars:
            height = bar.get_height()
            ax.annotate(f'{height:.2%}',
                        xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3),  # 3 points vertical offset
                        textcoords="offset points",
                        ha='center', va='bottom',
                        fontsize=10, fontweight='bold')

    add_labels(cer_bars)
    add_labels(wer_bars)

    # Add a watermark with model info
    model_info = results.get("model_info", "TrOCR")
    fig.text(0.99, 0.01, f"Model: {model_info}", ha='right', va='bottom',
             fontsize=8, color='gray', alpha=0.7)

    # Add timestamp
    timestamp = time.strftime("%Y-%m-%d %H:%M")
    fig.text(0.01, 0.01, f"Generated: {timestamp}", ha='left', va='bottom',
             fontsize=8, color='gray', alpha=0.7)

    # Adjust layout and save with high DPI
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"Visualization saved to {output_path}")

def run_ocr_evaluation(val_loader, transcriptions, config=None):
    """
    Run OCR evaluation with a pre-trained model.

    This function evaluates a pre-trained TrOCR model on a validation dataset,
    applying post-processing optimized for historical Spanish documents.
    It generates visualizations and reports of the evaluation results.

    Args:
        val_loader (DataLoader): Validation data loader
        transcriptions (dict): Dictionary mapping image paths to transcriptions
        config (OCRConfig): Configuration object

    Returns:
        dict: Evaluation results dictionary
    """
    if config is None:
        config = OCRConfig()

    # Create the output directory if it doesn't exist
    output_dir = config.results_path
    os.makedirs(output_dir, exist_ok=True)

    # Determine the device
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    # Select the model to use (handwritten model by default)
    model_name = config.models["handwritten"]

    print(f"Starting OCR evaluation using model: {model_name}")

    # Load the TrOCR model and processor
    model, processor = load_trocr_model(model_name, device)

    # Create lexicon for post-processing
    print("\nCreating lexicon for post-processing...")
    lexicon = create_lexicon_from_transcriptions(transcriptions)
    lexicon = augment_lexicon_with_variations(lexicon)

    # Create post-processor
    post_processor = SpanishHistoricalPostProcessor(lexicon)

    # For T4 GPU, use mixed precision (FP16) for inference
    if device.type == 'cuda' and config.use_mixed_precision:
        model = model.half()  # Convert to half precision
        print("Using FP16 for inference")

    # Run evaluation
    print("\nEvaluating OCR model...")
    results = evaluate_model(
        model=model,
        eval_dataloader=val_loader,
        processor=processor,
        device=device,
        post_processor=post_processor
    )

    # Save results
    results_file = os.path.join(output_dir, "evaluation_results.json")
    with open(results_file, 'w') as f:
        import json
        json.dump(
            {k: v for k, v in results.items() if k != 'samples'},
            f,
            indent=2
        )

    # Create visualization
    viz_path = os.path.join(output_dir, "error_rates_by_doc.png")
    visualize_ocr_results(results, viz_path)

    # Generate detailed report
    report_path = generate_error_report(results, viz_path)

    print(f"\nEvaluation results saved to {results_file}")
    print(f"Visualization saved to {viz_path}")
    print(f"Detailed report saved to {report_path}")

    return results

def generate_error_report(results, output_path):
    """
    Generate a detailed error report in HTML format.

    Creates a comprehensive HTML report of OCR evaluation results,
    including metrics, document-specific performance, and sample predictions.

    Args:
        results (dict): Dictionary with OCR evaluation results
        output_path (str): Path to save the visualization (used to reference images)

    Returns:
        str: Path to the generated report
    """
    # Create HTML report
    html_content = f"""
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>OCR Evaluation Report</title>
        <style>
            body {{
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 20px;
                color: #333;
            }}
            .container {{
                max-width: 1200px;
                margin: 0 auto;
            }}
            h1, h2, h3 {{
                color: #2c3e50;
            }}
            h1 {{
                border-bottom: 2px solid #3498db;
                padding-bottom: 10px;
            }}
            table {{
                border-collapse: collapse;
                width: 100%;
                margin: 20px 0;
            }}
            th, td {{
                border: 1px solid #ddd;
                padding: 12px;
                text-align: left;
            }}
            th {{
                background-color: #f2f2f2;
            }}
            tr:nth-child(even) {{
                background-color: #f9f9f9;
            }}
            .metrics {{
                display: flex;
                justify-content: space-between;
                flex-wrap: wrap;
            }}
            .metric-card {{
                background: #fff;
                border-radius: 8px;
                box-shadow: 0 2px 5px rgba(0,0,0,0.1);
                padding: 20px;
                margin: 10px;
                flex: 1;
                min-width: 200px;
            }}
            .metric-value {{
                font-size: 24px;
                font-weight: bold;
                color: #3498db;
            }}
            .error-sample {{
                background: #f8f9fa;
                border-left: 4px solid #e74c3c;
                padding: 15px;
                margin: 20px 0;
            }}
            .highlight {{
                background-color: #ffecb3;
                padding: 2px 4px;
                border-radius: 3px;
            }}
            img {{
                max-width: 100%;
                height: auto;
                border-radius: 5px;
                box-shadow: 0 3px 10px rgba(0,0,0,0.2);
                margin: 20px 0;
            }}
            .footer {{
                margin-top: 50px;
                text-align: center;
                font-size: 12px;
                color: #7f8c8d;
            }}
        </style>
    </head>
    <body>
        <div class="container">
            <h1>OCR Evaluation Report</h1>
            <p><strong>Date:</strong> {time.strftime("%Y-%m-%d %H:%M:%S")}</p>
            <p><strong>Model:</strong> {results.get("model_info", "TrOCR")}</p>

            <h2>Overall Metrics</h2>
            <div class="metrics">
                <div class="metric-card">
                    <h3>Character Error Rate</h3>
                    <div class="metric-value">{results.get("avg_cer", 0):.2%}</div>
                </div>
                <div class="metric-card">
                    <h3>Word Error Rate</h3>
                    <div class="metric-value">{results.get("avg_wer", 0):.2%}</div>
                </div>
                <div class="metric-card">
                    <h3>Total Samples</h3>
                    <div class="metric-value">{results.get("total_samples", 0)}</div>
                </div>
            </div>

            <h2>Results by Document Type</h2>
            <table>
                <tr>
                    <th>Document Type</th>
                    <th>CER</th>
                    <th>WER</th>
                    <th>Samples</th>
                </tr>
    """

    # Add document-specific results
    for doc_type in results.get("cer_by_doc", {}):
        cer = results["cer_by_doc"].get(doc_type, 0)
        wer = results["wer_by_doc"].get(doc_type, 0)
        samples = len(results.get("samples", {}).get(doc_type, []))

        html_content += f"""
                <tr>
                    <td>{doc_type}</td>
                    <td>{cer:.2%}</td>
                    <td>{wer:.2%}</td>
                    <td>{samples}</td>
                </tr>
        """

    html_content += """
            </table>

            <h2>Sample Predictions</h2>
    """

    # Add sample predictions
    for doc_type, samples in results.get("samples", {}).items():
        html_content += f"""
            <h3>{doc_type}</h3>
        """

        for i, sample in enumerate(samples[:2]):  # Limit to 2 samples per doc type
            ref = sample.get("reference", "")
            pred = sample.get("prediction", "")
            cer = sample.get("cer", 0)
            wer = sample.get("wer", 0)

            # Highlight differences (simple approach)
            matcher = difflib.SequenceMatcher(None, ref, pred)
            highlighted_pred = ""
            for tag, i1, i2, j1, j2 in matcher.get_opcodes():
                if tag == 'equal':
                    highlighted_pred += pred[j1:j2]
                elif tag == 'replace':
                    highlighted_pred += f'<span class="highlight">{pred[j1:j2]}</span>'
                elif tag == 'insert':
                    highlighted_pred += f'<span class="highlight">{pred[j1:j2]}</span>'
                elif tag == 'delete':
                    pass  # Skip deletions

            html_content += f"""
                <div class="error-sample">
                    <p><strong>Sample {i+1}</strong> - CER: {cer:.2%}, WER: {wer:.2%}</p>
                    <p><strong>Reference:</strong> {ref[:300]}...</p>
                    <p><strong>Prediction:</strong> {highlighted_pred[:300]}...</p>
                </div>
            """

    # Add visualizations
    if os.path.exists(output_path):
        img_path = os.path.basename(output_path)
        html_content += f"""
            <h2>Visualizations</h2>
            <img src="{img_path}" alt="OCR Error Rates by Document Type">
        """

    # Close HTML
    html_content += """
            <div class="footer">
                <p>Generated using Historical Spanish Document OCR Pipeline with Tesla T4 GPU</p>
            </div>
        </div>
    </body>
    </html>
    """

    # Save HTML report
    report_path = output_path.replace('.png', '_report.html')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(html_content)

    return report_path

"""# Cell 8"""

def run_full_ocr_pipeline(config=None, max_pages_per_pdf=None, evaluate_model=True, fine_tune=False):
    """
    Run the complete OCR pipeline optimized for Tesla T4 GPU.

    This is the main function that orchestrates the entire OCR pipeline from
    PDF upload to model training and evaluation. It includes progress tracking
    and memory optimization for T4 GPUs.

    Args:
        config: OCRConfig object containing configuration parameters
        max_pages_per_pdf: Maximum number of pages to process per PDF
        evaluate_model: Whether to evaluate the pre-trained model
        fine_tune: Whether to fine-tune the model

    Returns:
        Dictionary with pipeline results
    """
    if config is None:
        config = OCRConfig()

    # If max_pages_per_pdf is not specified, use a default value
    if max_pages_per_pdf is None:
        max_pages_per_pdf = 8  # Default: process 8 pages per PDF for T4 GPU

    start_time = time.time()

    print("=" * 80)
    print("GPU-Optimized Historical Spanish Document OCR Pipeline")
    print("=" * 80)
    print(f"\nUsing device: {device}")
    print_gpu_memory_usage()

    print("\nStep 1: Upload PDFs")
    uploaded_paths = upload_pdfs_to_colab(config)

    if not uploaded_paths:
        print("No PDFs uploaded. Please upload at least one PDF file.")
        return None

    print(f"\nUploaded {len(uploaded_paths)} PDFs")

    print("\nStep 2: Convert PDFs to Images and Preprocess")
    document_images = process_all_pdfs(config, max_pages_per_pdf)

    # Check if we got any processed images
    total_images = sum(len(paths) for paths in document_images.values())

    if total_images == 0:
        print("\nNo images were extracted from PDFs. Exiting.")
        return None

    print(f"\nExtracted {total_images} images from {len(document_images)} documents")

    print("\nStep 3: Preprocess Images")
    processed_document_images = preprocess_document_images(document_images, config)

    # Check total processed images
    total_processed = sum(len(paths) for paths in processed_document_images.values())

    if total_processed == 0:
        print("\nNo images were successfully preprocessed. Exiting.")
        return None

    print(f"\nSuccessfully preprocessed {total_processed} images from {len(processed_document_images)} documents")

    print("\nStep 4: Creating Transcriptions")
    create_rich_transcriptions(processed_document_images, config.transcriptions_path)

    # Collect all processed image paths
    print("\nStep 5: Collecting Processed Image Paths")
    all_processed_images = []
    for doc_id, image_paths in processed_document_images.items():
        all_processed_images.extend(image_paths)

    print(f"Collected {len(all_processed_images)} processed images")

    # Load transcriptions
    print("\nStep 6: Loading Transcriptions")
    transcriptions = load_transcriptions(config.transcriptions_path, all_processed_images)

    # Create train-validation-test split
    print("\nStep 7: Creating Data Splits")
    train_image_paths, val_image_paths, test_image_paths = create_train_val_test_split(
        all_processed_images,
        transcriptions,
        test_ratio=0.2,
        val_ratio=0.1
    )

    print(f"Train set: {len(train_image_paths)} images")
    print(f"Validation set: {len(val_image_paths)} images")
    print(f"Test set: {len(test_image_paths)} images")

    # Create data loaders
    print("\nStep 8: Creating Data Loaders")
    train_loader, val_loader, test_loader = create_data_loaders(
        train_image_paths,
        val_image_paths,
        test_image_paths,
        transcriptions,
        batch_size=config.batch_size,
        config=config
    )

    # Show example images
    print("\nStep 9: Showing Example Images")
    save_example_images(processed_document_images, config.output_base_path, num_examples=2)

    # Store initial results
    results = {
        "document_images": processed_document_images,
        "transcriptions": transcriptions,
        "train_loader": train_loader,
        "val_loader": val_loader,
        "test_loader": test_loader,
        "train_image_paths": train_image_paths,
        "val_image_paths": val_image_paths,
        "test_image_paths": test_image_paths,
        "processing_time": {}
    }

    # Checkpoint 1: Save initial data split
    checkpoint_path = os.path.join(config.results_path, "pipeline_checkpoint_1.pt")
    torch.save({
        "train_paths": train_image_paths,
        "val_paths": val_image_paths,
        "test_paths": test_image_paths
    }, checkpoint_path)

    print(f"Data preparation checkpoint saved to {checkpoint_path}")

    # Evaluate model if requested
    if evaluate_model:
        print("\nStep 10: Evaluating Pre-trained OCR Model")
        eval_start_time = time.time()

        eval_results = run_ocr_evaluation(
            val_loader=val_loader,
            transcriptions=transcriptions,
            config=config
        )

        eval_time = time.time() - eval_start_time
        results["evaluation_results"] = eval_results
        results["processing_time"]["evaluation"] = eval_time

        print(f"Model evaluation completed in {eval_time:.2f} seconds")

    # Fine-tune model if requested
    if fine_tune:
        print("\nStep 11: Fine-tuning OCR Model")
        ft_start_time = time.time()

        model, processor, ft_results = fine_tune_trocr_model(
            train_loader=train_loader,
            val_loader=val_loader,
            config=config
        )

        ft_time = time.time() - ft_start_time
        results["fine_tuned_model"] = model
        results["fine_tuned_processor"] = processor
        results["fine_tuning_results"] = ft_results
        results["processing_time"]["fine_tuning"] = ft_time

        print(f"Model fine-tuning completed in {ft_time:.2f} seconds")

        # Evaluate the fine-tuned model on the test set
        print("\nStep 12: Evaluating Fine-tuned Model on Test Set")
        test_start_time = time.time()

        test_results = evaluate_model(
            model=model,
            eval_dataloader=test_loader,
            processor=processor,
            device=device
        )

        test_time = time.time() - test_start_time
        results["test_results"] = test_results
        results["processing_time"]["testing"] = test_time

        # Save test results visualization
        test_viz_path = os.path.join(config.output_base_path, "fine_tuned_model", "test_results.png")
        visualize_ocr_results(test_results, test_viz_path)

        # Generate detailed test report
        generate_error_report(test_results, test_viz_path)

        print(f"Test evaluation completed in {test_time:.2f} seconds")

    # Calculate total processing time
    total_time = time.time() - start_time
    results["processing_time"]["total"] = total_time

    print("\n" + "=" * 80)
    print(f"OCR pipeline completed successfully in {total_time:.2f} seconds!")
    print("=" * 80)

    # Print results summary
    print("\nResults Summary:")
    if "evaluation_results" in results:
        print(f"Pre-trained Model - Avg CER: {results['evaluation_results']['avg_cer']:.2%}, "
              f"Avg WER: {results['evaluation_results']['avg_wer']:.2%}")

    if "fine_tuning_results" in results:
        print(f"Fine-tuned Model - Avg CER: {results['fine_tuning_results']['avg_cer']:.2%}, "
              f"Avg WER: {results['fine_tuning_results']['avg_wer']:.2%}")

    if "test_results" in results:
        print(f"Test Set Results - Avg CER: {results['test_results']['avg_cer']:.2%}, "
              f"Avg WER: {results['test_results']['avg_wer']:.2%}")

    # Output paths for results
    print(f"\nResults saved in: {config.results_path}")
    if fine_tune:
        print(f"Fine-tuned model saved in: {os.path.join(config.output_base_path, 'fine_tuned_model')}")

    return results

def process_new_document(pdf_path=None, model_path=None, config=None):
    """
    Process a new document using a trained model.

    This function handles the end-to-end processing of a new document:
    1. Convert PDF to images
    2. Preprocess images
    3. Run OCR with a trained model
    4. Save and return results

    Args:
        pdf_path: Path to the PDF file (if None, prompt for upload)
        model_path: Path to the model to use (if None, use pre-trained)
        config: OCRConfig object containing configuration parameters

    Returns:
        Dictionary with OCR results
    """
    if config is None:
        config = OCRConfig()

    # Create output directory
    output_dir = os.path.join(config.output_base_path, "inference_results")
    os.makedirs(output_dir, exist_ok=True)

    # Upload a PDF if path not provided
    if pdf_path is None:
        print("Please upload a PDF document for OCR processing:")
        uploaded = files.upload()

        if not uploaded:
            print("No PDF uploaded.")
            return None

        pdf_filename = list(uploaded.keys())[0]
        pdf_path = os.path.join(config.pdf_folder, pdf_filename)

        # Save the uploaded file
        with open(pdf_path, 'wb') as f:
            f.write(uploaded[pdf_filename])

    print(f"Processing document: {os.path.basename(pdf_path)}")

    # Step 1: Convert PDF to images
    pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]
    images_folder = os.path.join(output_dir, "images", pdf_filename)
    processed_folder = os.path.join(output_dir, "processed_images", pdf_filename)
    binary_folder = os.path.join(output_dir, "binary_images", pdf_filename)
    enhanced_folder = os.path.join(output_dir, "enhanced_images", pdf_filename)

    os.makedirs(images_folder, exist_ok=True)
    os.makedirs(processed_folder, exist_ok=True)
    os.makedirs(binary_folder, exist_ok=True)
    os.makedirs(enhanced_folder, exist_ok=True)

    # Convert PDF to images
    image_paths = convert_pdf_to_images(
        pdf_path,
        images_folder,
        dpi=config.default_dpi,
        config=config
    )

    if not image_paths:
        print(f"Failed to extract images from {pdf_filename}")
        return None

    print(f"Extracted {len(image_paths)} images from PDF")

    # Step 2: Preprocess images
    processed_paths = []
    for img_path in tqdm(image_paths, desc="Preprocessing images"):
        try:
            processed_path, _, _ = preprocess_image(
                img_path,
                processed_folder,
                binary_folder,
                enhanced_folder
            )

            if processed_path:
                processed_paths.append(processed_path)

        except Exception as e:
            print(f"Error preprocessing {img_path}: {str(e)}")

    if not processed_paths:
        print("Failed to preprocess any images")
        return None

    print(f"Preprocessed {len(processed_paths)} images")

    # Step 3: Check if we have a fine-tuned model
    if model_path is None:
        # Try to find a fine-tuned model
        fine_tuned_path = os.path.join(config.output_base_path, "fine_tuned_model", "final")
        if os.path.exists(fine_tuned_path):
            print(f"Using fine-tuned model from: {fine_tuned_path}")
            model_path = fine_tuned_path
        else:
            print("No fine-tuned model found. Using pre-trained model.")

    # Step 4: Run inference
    results = run_inference_on_document(
        image_paths=processed_paths,
        model_path=model_path,
        config=config
    )

    if not results:
        print("Inference failed.")
        return None

    # Step 5: Save results
    output_text_file = os.path.join(output_dir, f"{pdf_filename}_ocr.txt")
    with open(output_text_file, 'w', encoding='utf-8') as f:
        f.write(results["combined_text"])

    # Save results as JSON
    output_json_file = os.path.join(output_dir, f"{pdf_filename}_results.json")
    with open(output_json_file, 'w', encoding='utf-8') as f:
        import json
        json.dump(
            {
                "filename": pdf_filename,
                "page_count": len(results["text_by_image"]),
                "processing_time": time.time() - start_time if 'start_time' in locals() else None
            },
            f,
            indent=2
        )

    print(f"\nOCR completed for {pdf_filename}")
    print(f"Processed {len(processed_paths)} pages")
    print(f"Results saved to:\n - {output_text_file}\n - {output_json_file}")

    # Display a sample of the OCR text
    print("\nOCR Text Sample:")
    text_sample = results["combined_text"][:500] + "..." if len(results["combined_text"]) > 500 else results["combined_text"]
    print(text_sample)

    return results

"""# Cell 9"""

# Create configuration with Tesla T4 GPU optimizations
config = OCRConfig()

# Custom settings for Tesla T4 GPU
config.batch_size = 6  # Optimal batch size for T4 memory
config.use_mixed_precision = True  # Use FP16 for better performance
config.gradient_accumulation_steps = 4  # Effective batch size = 8*4 = 32
config.max_length = 512  # Maximum sequence length for long historical texts

# Display GPU information and settings
if torch.cuda.is_available():
    print(f"🔍 GPU: {torch.cuda.get_device_name(0)}")
    print(f"🧠 Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    t4_optimized = "T4" in torch.cuda.get_device_name(0)
    if t4_optimized:
        print("✅ Tesla T4 GPU detected - Using optimized settings!")
    else:
        print(f"⚠️ Using {torch.cuda.get_device_name(0)} - Some T4 optimizations may not apply")
    print_gpu_memory_usage()
else:
    print("⚠️ No GPU detected. This pipeline is optimized for Tesla T4 GPU.")
    print("Processing will continue but will be significantly slower.")

# Run the complete OCR pipeline with configurable parameters

# Customize these parameters based on your requirements
max_pages = 6           # Process up to 8 pages per PDF (T4 optimized)
evaluate = True         # Evaluate the pre-trained model
fine_tune = True        # Fine-tune the model

print("\n" + "=" * 80)
print(" Historical Spanish Document OCR Pipeline with Tesla T4 GPU Optimization ")
print("=" * 80)

print("\nStarting OCR pipeline with the following settings:")
print(f"• Processing up to {max_pages} pages per PDF")
print(f"• Pre-trained model evaluation: {'Enabled' if evaluate else 'Disabled'}")
print(f"• Model fine-tuning: {'Enabled' if fine_tune else 'Disabled'}")
print(f"• Batch size: {config.batch_size} (effective: {config.batch_size * config.gradient_accumulation_steps} with gradient accumulation)")
print(f"• Mixed precision: {'Enabled' if config.use_mixed_precision else 'Disabled'}")
print(f"• Image size: {config.image_size[0]}x{config.image_size[1]} pixels")
print(f"• PDF resolution: {config.default_dpi} DPI")

# Record start time
start_time = time.time()

# Execute the pipeline
results = run_full_ocr_pipeline(
    config=config,
    max_pages_per_pdf=max_pages,
    evaluate_model=evaluate,
    fine_tune=fine_tune
)

# Calculate total time
total_time = time.time() - start_time
hours, remainder = divmod(total_time, 3600)
minutes, seconds = divmod(remainder, 60)

if results:
    print("\n✅ Pipeline completed successfully!")
    print(f"⏱️ Total processing time: {int(hours)}h {int(minutes)}m {seconds:.2f}s")

    # Print GPU memory usage after pipeline runs
    print_gpu_memory_usage()

    # List available output files
    print("\n📄 Generated Files:")
    for root, dirs, files in os.walk(config.results_path):
        for file in files:
            if file.endswith('.png') or file.endswith('.html') or file.endswith('.json'):
                print(f" - {os.path.join(root, file)}")

    # If fine-tuned, show the model directory
    if fine_tune:
        fine_tuned_model_path = os.path.join(config.output_base_path, "fine_tuned_model", "final")
        print(f"\n🔍 Fine-tuned model is saved in: {fine_tuned_model_path}")
        print("Use this model path for inference on new documents.")
else:
    print("\n❌ Pipeline did not complete successfully. Please check the errors above.")

# Process a new document with the trained model
print("=" * 80)
print(" Process New Document with OCR Model ")
print("=" * 80)

# First, check if we have a fine-tuned model available
fine_tuned_model_path = os.path.join(config.output_base_path, "fine_tuned_model", "final")

if os.path.exists(fine_tuned_model_path):
    print(f"✅ Using fine-tuned model from: {fine_tuned_model_path}")
    model_path = fine_tuned_model_path
else:
    print("⚠️ No fine-tuned model found. Using pre-trained model.")
    model_path = None

print("\nPlease upload a new PDF document for OCR processing:")
print("(Select a Spanish historical document for best results)")

# Record start time
start_time = time.time()

# Process the uploaded document
results = process_new_document(
    model_path=model_path,
    config=config
)

# Calculate total processing time
total_time = time.time() - start_time
minutes, seconds = divmod(total_time, 60)

if results:
    print("\n✅ Document processing completed successfully!")
    print(f"⏱️ Processing time: {int(minutes)}m {seconds:.2f}s")

    # Create a filename for downloading the results
    if "combined_text" in results:
        from google.colab import files

        # Save OCR results to a downloadable file
        download_file = "ocr_results.txt"
        with open(download_file, 'w', encoding='utf-8') as f:
            f.write(results["combined_text"])

        print("\n📥 Downloading OCR results...")
        files.download(download_file)

    # Print memory usage
    print_gpu_memory_usage()
else:
    print("\n❌ Document processing failed. Please check the errors above.")

# Tesla T4 GPU Optimization Tips for OCR
print("=" * 80)
print(" Tesla T4 GPU Optimization Guide for Historical Document OCR ")
print("=" * 80)

# Check if we're actually using a T4 GPU
if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    if "T4" in gpu_name:
        print(f"✅ Confirmed: Using Tesla T4 GPU")
    else:
        print(f"ℹ️ Note: Using {gpu_name} (not Tesla T4)")
else:
    print("⚠️ No GPU detected. These tips are specific to Tesla T4 GPUs.")

print("\n💡 Memory Management")
print("   • The Tesla T4 has 16GB memory (Colab provides access to ~12-15GB)")
print("   • For larger documents, process them in smaller batches")
print("   • Use mixed precision (FP16) instead of FP32 for 2-3x speedup")
print("   • Set batch_size=8 with gradient accumulation for optimal performance")
print("   • Use torch.cuda.empty_cache() regularly during processing")

print("\n💡 Model Selection")
print("   • For fastest inference, use microsoft/trocr-base-handwritten")
print("   • For higher accuracy, use microsoft/trocr-large-handwritten (slower)")
print("   • Custom fine-tuned models provide the best balance of speed/accuracy")
print("   • When fine-tuning, start from a pre-trained model to save time")

print("\n💡 Image Processing")
print("   • Optimal image size: 384x384 to 512x512 for T4 GPU")
print("   • Use higher DPI (400) during PDF conversion with T4 GPU")
print("   • Process max 8-10 pages at once for best T4 memory utilization")
print("   • For best results, use the enhanced preprocessing pipeline")

print("\n💡 Fine-tuning Parameters")
print("   • Optimal batch size: 4-8 with gradient accumulation_steps=4")
print("   • Learning rate: 3e-5 with cosine scheduler and warmup")
print("   • Training epochs: 5-10 is sufficient for good results")
print("   • Use mixed precision training (fp16) for 2-3x speedup")

print("\n💡 Inference Optimization")
print("   • Use half precision (FP16) for 2-3x faster inference")
print("   • Set num_beams=4 for good quality/speed trade-off")
print("   • Enable use_cache=True for faster generation")
print("   • Process documents in batches of 8-16 for highest throughput")

# Check if we can monitor GPU performance live
try:
    # Print current GPU utilization
    !nvidia-smi
    print("\nℹ️ Use '!nvidia-smi' to monitor GPU utilization")
except:
    print("\nℹ️ Cannot access nvidia-smi in this environment")

# Memory usage comparison
print("\n📊 Memory Usage Comparison (approximate):")
print("┌────────────────────────┬───────────────┐")
print("│ Operation              │ Memory (T4)   │")
print("├────────────────────────┼───────────────┤")
print("│ Base model (FP32)      │ ~500 MB       │")
print("│ Large model (FP32)     │ ~1.5 GB       │")
print("│ Base model (FP16)      │ ~250 MB       │")
print("│ Large model (FP16)     │ ~750 MB       │")
print("│ 512x512 image batch=8  │ ~100 MB       │")
print("│ Full pipeline overhead │ ~2-3 GB       │")
print("└────────────────────────┴───────────────┘")

# Current memory usage
print("\n🧠 Current memory usage:")
print_gpu_memory_usage()

# Performance optimization function
def optimize_for_t4():
    """Apply all recommended optimization settings for Tesla T4 GPU"""
    if not torch.cuda.is_available() or "T4" not in torch.cuda.get_device_name(0):
        print("T4 GPU not detected. Optimizations may not be applicable.")
        return

    # Clear GPU cache
    torch.cuda.empty_cache()

    # Enable TF32 for matmul
    torch.backends.cuda.matmul.allow_tf32 = True

    # Enable cuDNN benchmarking
    torch.backends.cudnn.benchmark = True

    # Disable cuDNN determinism for speed
    torch.backends.cudnn.deterministic = False

    # Set optimal batch size and image dimensions in config
    config.batch_size = 8
    config.image_size = (512, 512)
    config.use_mixed_precision = True
    config.gradient_accumulation_steps = 4

    print("✅ Applied all recommended T4 GPU optimizations")
    print_gpu_memory_usage()

# Run the optimization function
optimize_for_t4()