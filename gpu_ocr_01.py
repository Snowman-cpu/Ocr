# -*- coding: utf-8 -*-
"""gpu_ocr_01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ghByaEPbvXYEBmA1aX5Li5TxBCZt29FT

# Cell 1: Install Required Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# Install required system packages
!apt-get update
!apt-get install -y poppler-utils tesseract-ocr libtesseract-dev tesseract-ocr-spa

# Install required Python packages
!pip install pdf2image opencv-python matplotlib tqdm
!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 -f https://download.pytorch.org/whl/torch_stable.html
!pip install transformers==4.30.2 datasets accelerate
!pip install pillow seaborn pandas
!pip install PyPDF2 pytesseract
!pip install huggingface-hub sentencepiece sacremoses
!pip install rapidfuzz symspellpy

# Install APEX for mixed precision training (optimized for NVIDIA GPUs)
!git clone https://github.com/NVIDIA/apex
# %cd apex
!pip install -v --disable-pip-version-check --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./
# %cd ..

# Verify GPU availability
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU device count: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"Current GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9} GB")

"""# Cell 2"""

import os
import glob
import re
import random
import warnings
import shutil
import math
import string
import time
import numpy as np
import cv2
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageEnhance, ImageFilter
from tqdm.notebook import tqdm
import torch
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
from torch.optim.lr_scheduler import CosineAnnealingLR
from torchvision import transforms
import pdf2image
import difflib
from collections import Counter
import pandas as pd
import seaborn as sns
from google.colab import files
import pytesseract
from rapidfuzz import fuzz
from rapidfuzz import process as fuzz_process
from symspellpy import SymSpell, Verbosity

# Import transformers libraries
try:
    from transformers import (
        TrOCRProcessor,
        VisionEncoderDecoderModel,
        VisionEncoderDecoderConfig,
        AutoFeatureExtractor,
        AutoTokenizer,
        default_data_collator,
        get_scheduler
    )
    from datasets import Dataset
    from accelerate import Accelerator
    print("Successfully imported transformers and datasets libraries")
except ImportError:
    print("Error importing transformers or datasets. Installing again...")
    !pip install transformers datasets accelerate
    from transformers import (
        TrOCRProcessor,
        VisionEncoderDecoderModel,
        VisionEncoderDecoderConfig,
        AutoFeatureExtractor,
        AutoTokenizer
    )
    from datasets import Dataset
    from accelerate import Accelerator

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

# Set random seeds for reproducibility
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)
    # Set prefer higher precision ops for CUDA
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    # Enable cuDNN benchmarking for performance
    torch.backends.cudnn.benchmark = True
    # Make sure deterministic algorithms work across runs
    torch.use_deterministic_algorithms(False)

print("Libraries imported successfully!")

# Check for available GPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Function to monitor GPU memory usage
def print_gpu_memory_usage():
    if torch.cuda.is_available():
        t = torch.cuda.get_device_properties(0).total_memory
        r = torch.cuda.memory_reserved(0)
        a = torch.cuda.memory_allocated(0)
        f = r-a  # free inside reserved
        print(f"GPU Memory: Total={t/1e9:.2f}GB, Reserved={r/1e9:.2f}GB, Allocated={a/1e9:.2f}GB, Free={f/1e9:.2f}GB")
    else:
        print("No GPU available")

# Check GPU memory usage
print_gpu_memory_usage()

"""# Cell 3"""

# Define base paths and create necessary directories
base_path = '/content'
pdf_folder = os.path.join(base_path, 'pdfs')
output_base_path = os.path.join(base_path, 'ocr_data')
transcriptions_path = os.path.join(base_path, 'transcriptions')
training_data_path = os.path.join(output_base_path, 'training_data')
results_path = os.path.join(output_base_path, 'results')
model_checkpoints_path = os.path.join(output_base_path, 'model_checkpoints')
language_model_path = os.path.join(output_base_path, 'language_model')

# Define constants for the OCR pipeline
OCR_MODELS = {
    "handwritten": "microsoft/trocr-large-handwritten",  # Upgraded from base to large
    "printed": "microsoft/trocr-large-printed",          # Upgraded from base to large
    "spanish_custom": "microsoft/trocr-base-printed"     # Default for Spanish if no custom model available
}

# Higher DPI for better quality images
DEFAULT_DPI = 400  # Increased from 300 for better quality

# Higher resolution inputs for the TrOCR model
IMAGE_SIZE = (512, 512)  # Increased from 384x384 for more detail

# Create necessary directories with a single function
def create_directory_structure():
    dirs = [
        pdf_folder,
        os.path.join(output_base_path, "images"),
        os.path.join(output_base_path, "processed_images"),
        os.path.join(output_base_path, "binary_images"),
        os.path.join(output_base_path, "enhanced_images"),  # New folder for enhanced preprocessing
        training_data_path,
        transcriptions_path,
        results_path,
        model_checkpoints_path,
        language_model_path,
        os.path.join(results_path, "visualizations")  # For saving visualizations
    ]

    for directory in dirs:
        os.makedirs(directory, exist_ok=True)

    print("Directory structure created successfully!")

# Create the directories
create_directory_structure()

# Cache GPU info for reporting
if torch.cuda.is_available():
    gpu_info = {
        "name": torch.cuda.get_device_name(0),
        "capability": torch.cuda.get_device_capability(0),
        "total_memory": torch.cuda.get_device_properties(0).total_memory,
    }
    print(f"Using GPU: {gpu_info['name']} with compute capability {gpu_info['capability']}")
    print(f"Total GPU memory: {gpu_info['total_memory'] / 1e9:.2f} GB")
else:
    print("WARNING: No GPU detected. This pipeline is optimized for Tesla T4 GPU.")
    print("Processing will continue but will be slower.")

"""# Cell 4

"""

def convert_pdf_to_images(pdf_path, output_folder, dpi=DEFAULT_DPI, first_page=None, last_page=None):
    """
    Convert PDF pages to images with robust error handling and memory management

    Args:
        pdf_path: Path to the PDF file
        output_folder: Folder to save the images
        dpi: Resolution in DPI (dots per inch)
        first_page: First page to convert (1-based index)
        last_page: Last page to convert (1-based index)

    Returns:
        List of paths to the saved images
    """
    os.makedirs(output_folder, exist_ok=True)
    pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]
    file_size_mb = os.path.getsize(pdf_path) / (1024 * 1024)

    # Adjust DPI for large files but keep higher quality for T4 GPU
    adjusted_dpi = dpi
    if file_size_mb > 50:  # Higher threshold for T4 GPU
        adjusted_dpi = min(dpi, 300)
    if file_size_mb > 100:  # For very large files
        adjusted_dpi = min(dpi, 200)

    if adjusted_dpi != dpi:
        print(f"Large PDF detected ({file_size_mb:.1f} MB). Adjusting DPI from {dpi} to {adjusted_dpi}")

    try:
        # Check poppler installation
        import subprocess
        try:
            subprocess.run(["pdftoppm", "-v"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
        except (subprocess.SubprocessError, FileNotFoundError):
            print("Poppler not found in PATH. Using default path...")
            poppler_path = "/usr/bin"
            os.environ["PATH"] += os.pathsep + poppler_path

        # Process based on file size
        if file_size_mb > 20:
            # Process one page at a time for large PDFs
            from PyPDF2 import PdfReader
            reader = PdfReader(pdf_path)
            total_pages = len(reader.pages)

            start_page = first_page if first_page else 1
            end_page = min(last_page if last_page else total_pages, total_pages)

            image_paths = []
            for page_num in range(start_page, end_page + 1):
                try:
                    # Use pdf2image with GPU-optimized settings
                    single_image = pdf2image.convert_from_path(
                        pdf_path,
                        dpi=adjusted_dpi,
                        first_page=page_num,
                        last_page=page_num,
                        use_pdftocairo=True,
                        thread_count=2  # Utilize multiple CPU cores
                    )

                    if single_image and len(single_image) > 0:
                        # Use a higher quality JPEG for T4 processing
                        image_path = os.path.join(output_folder, f"{pdf_filename}_page_{page_num:03d}.jpg")
                        single_image[0].save(image_path, "JPEG", quality=95)
                        image_paths.append(image_path)
                    else:
                        print(f"No image extracted for page {page_num}")
                except Exception as e:
                    print(f"Error processing page {page_num}: {str(e)}")
                    continue

            return image_paths
        else:
            # Process all pages at once for smaller PDFs
            images = pdf2image.convert_from_path(
                pdf_path,
                dpi=adjusted_dpi,
                first_page=first_page,
                last_page=last_page,
                use_pdftocairo=True,
                thread_count=2
            )

            image_paths = []
            for i, image in enumerate(images):
                page_num = i + 1 if first_page is None else first_page + i
                image_path = os.path.join(output_folder, f"{pdf_filename}_page_{page_num:03d}.jpg")
                image.save(image_path, "JPEG", quality=95)
                image_paths.append(image_path)

            print(f"Successfully converted {len(image_paths)} pages from {pdf_filename}")
            return image_paths

    except Exception as e:
        print(f"Error converting PDF {pdf_path}: {str(e)}")

        # Try alternative conversion method if primary method fails
        try:
            print("Attempting alternative conversion method...")
            from PyPDF2 import PdfReader
            reader = PdfReader(pdf_path)
            page_count = len(reader.pages)

            start_page = first_page if first_page else 1
            end_page = min(last_page if last_page else page_count, page_count)

            # Use pytesseract as backup to extract text directly
            image_paths = []
            for page_num in range(start_page, end_page + 1):
                # Create blank image for placeholder
                blank_img = Image.new('RGB', (1000, 1400), color='white')
                draw = ImageDraw.Draw(blank_img)

                # Try to extract text from PDF directly
                try:
                    page = reader.pages[page_num - 1]
                    text = page.extract_text()
                    # Add text to image (basic rendering)
                    y_position = 100
                    for line in text.split('\n'):
                        draw.text((100, y_position), line, fill='black')
                        y_position += 25
                except:
                    draw.text((400, 500), f"Page {page_num} - {pdf_filename}", fill='black')
                    draw.text((400, 550), "(PDF conversion failed - placeholder image)", fill='black')

                # Save the image
                image_path = os.path.join(output_folder, f"{pdf_filename}_page_{page_num:03d}.jpg")
                blank_img.save(image_path, "JPEG", quality=90)
                image_paths.append(image_path)

            print(f"Created {len(image_paths)} alternative images for {pdf_filename}")
            return image_paths

        except Exception as backup_error:
            print(f"Alternative conversion also failed: {str(backup_error)}")
            return []

def upload_pdfs_to_colab():
    """
    Upload PDF files to Google Colab

    Returns:
        List of uploaded file paths
    """
    print("Please upload your PDF files (you can select multiple files)...")
    os.makedirs(pdf_folder, exist_ok=True)

    # Show a more user-friendly message
    print("\nWaiting for file upload... Click the 'Choose Files' button that appears.")
    print("This pipeline is optimized for historical Spanish documents.")

    # Upload files
    uploaded = files.upload()

    # Move uploaded files to the PDF folder
    uploaded_paths = []
    for filename, content in uploaded.items():
        # Check if the file is a PDF
        if not filename.lower().endswith('.pdf'):
            print(f"Warning: {filename} is not a PDF file. Skipping...")
            continue

        # Write the file to the PDF folder
        filepath = os.path.join(pdf_folder, filename)
        with open(filepath, 'wb') as f:
            f.write(content)

        uploaded_paths.append(filepath)
        print(f"Uploaded: {filename} -> {filepath}")

    if not uploaded_paths:
        print("No PDF files were uploaded. Please try again.")
    else:
        print(f"Successfully uploaded {len(uploaded_paths)} PDF files.")

    return uploaded_paths

def process_all_pdfs(pdf_folder, output_base_path, dpi=DEFAULT_DPI, max_pages_per_pdf=None):
    """
    Process all PDFs in a folder with improved error handling and recovery

    Args:
        pdf_folder: Folder containing the PDFs
        output_base_path: Base folder to save the output
        dpi: Resolution in DPI (dots per inch)
        max_pages_per_pdf: Maximum number of pages to process per PDF

    Returns:
        Dictionary mapping document IDs to lists of processed image paths
    """
    images_folder = os.path.join(output_base_path, "images")
    processed_folder = os.path.join(output_base_path, "processed_images")
    binary_folder = os.path.join(output_base_path, "binary_images")
    enhanced_folder = os.path.join(output_base_path, "enhanced_images")

    os.makedirs(images_folder, exist_ok=True)
    os.makedirs(processed_folder, exist_ok=True)
    os.makedirs(binary_folder, exist_ok=True)
    os.makedirs(enhanced_folder, exist_ok=True)

    pdf_files = glob.glob(os.path.join(pdf_folder, "*.pdf"))
    print(f"Found {len(pdf_files)} PDF files")

    # Dictionary to store document ID to image paths mapping
    document_images = {}

    # Initialize progress bar for processing PDFs
    for pdf_file in tqdm(pdf_files, desc="Processing PDFs"):
        pdf_filename = os.path.splitext(os.path.basename(pdf_file))[0]
        print(f"\nProcessing {pdf_filename}")

        # Create folders for this PDF
        pdf_images_folder = os.path.join(images_folder, pdf_filename)
        pdf_processed_folder = os.path.join(processed_folder, pdf_filename)
        pdf_binary_folder = os.path.join(binary_folder, pdf_filename)
        pdf_enhanced_folder = os.path.join(enhanced_folder, pdf_filename)

        os.makedirs(pdf_images_folder, exist_ok=True)
        os.makedirs(pdf_processed_folder, exist_ok=True)
        os.makedirs(pdf_binary_folder, exist_ok=True)
        os.makedirs(pdf_enhanced_folder, exist_ok=True)

        # Check if we already have processed images for this PDF to avoid reprocessing
        existing_processed_images = glob.glob(os.path.join(pdf_processed_folder, "*.jpg"))
        if existing_processed_images and len(existing_processed_images) >= (max_pages_per_pdf or float('inf')):
            print(f"Found {len(existing_processed_images)} already processed images for {pdf_filename}. Skipping conversion.")
            document_images[pdf_filename] = existing_processed_images
            continue

        # Handle large PDFs with a different approach
        if os.path.getsize(pdf_file) > 10*1024*1024:  # > 10MB
            print(f"Large PDF detected: {pdf_filename}. Processing in chunks...")

            # Get total page count
            try:
                from PyPDF2 import PdfReader
                reader = PdfReader(pdf_file)
                total_pages = len(reader.pages)
                max_page = min(total_pages, max_pages_per_pdf) if max_pages_per_pdf else total_pages

                print(f"PDF has {total_pages} pages. Processing up to page {max_page}.")

                # Process large PDF in chunks of 5 pages
                chunk_size = 5
                first_page = 1
                all_image_paths = []

                while first_page <= max_page:
                    last_page = min(first_page + chunk_size - 1, max_page)
                    chunk_image_paths = convert_pdf_to_images(
                        pdf_file,
                        pdf_images_folder,
                        dpi=dpi,
                        first_page=first_page,
                        last_page=last_page
                    )

                    if not chunk_image_paths:
                        # Try with a smaller chunk if failed
                        if chunk_size > 1:
                            print(f"Trying with smaller chunk size for pages {first_page}-{last_page}...")
                            chunk_size = 1
                            continue
                        else:
                            print(f"Failed to process pages {first_page}-{last_page}, skipping.")
                            first_page += chunk_size
                            continue

                    all_image_paths.extend(chunk_image_paths)
                    first_page += chunk_size

                document_images[pdf_filename] = all_image_paths

            except Exception as e:
                print(f"Error determining page count: {str(e)}")
                # Fallback: just try to process first 10 pages
                chunk_image_paths = convert_pdf_to_images(
                    pdf_file,
                    pdf_images_folder,
                    dpi=dpi,
                    first_page=1,
                    last_page=10
                )
                document_images[pdf_filename] = chunk_image_paths if chunk_image_paths else []
        else:
            # Convert PDF to images for smaller PDFs
            image_paths = convert_pdf_to_images(
                pdf_file,
                pdf_images_folder,
                dpi=dpi,
                first_page=1,
                last_page=max_pages_per_pdf
            )

            # Store the image paths
            document_images[pdf_filename] = image_paths if image_paths else []

        # Check if we got any images for this document
        if not document_images[pdf_filename]:
            print(f"WARNING: No images were extracted from {pdf_filename}")
            continue

        # The preprocessing will be handled in the next functions
        # This keeps this function focused only on PDF to image conversion

    # Check if we processed any images successfully
    total_processed = sum(len(paths) for paths in document_images.values())
    if total_processed == 0:
        print("\nWARNING: No PDFs were successfully processed.")
        print("Please make sure poppler-utils is installed correctly.")
    else:
        print(f"\nSuccessfully processed {total_processed} images from {len(document_images)} documents")

    return document_images

"""# Cell 5"""

def correct_skew(image, delta=0.1, limit=10):
    """
    Enhanced skew correction algorithm optimized for historical documents

    Args:
        image: Grayscale image
        delta: Angle step size for scoring (smaller for more precision)
        limit: Maximum angle to check (increased for heavily skewed documents)

    Returns:
        Deskewed image
    """
    # Create edges image for better line detection
    edges = cv2.Canny(image, 50, 150, apertureSize=3)

    # Try to detect lines using Hough Transform with more sensitive parameters
    lines = cv2.HoughLines(edges, 1, np.pi/1800, 150)  # Increased angular resolution

    if lines is not None and len(lines) >= 5:  # Ensure we have enough lines for a good estimate
        # Calculate the angle histogram with finer binning
        angle_counts = {}
        for line in lines:
            rho, theta = line[0]
            # Convert radians to degrees and normalize to [-90, 90]
            angle = (theta * 180 / np.pi) % 180
            if angle > 90:
                angle = angle - 180

            # Bin the angles with higher precision
            angle_key = round(angle * 10) / 10  # 0.1 degree precision
            angle_counts[angle_key] = angle_counts.get(angle_key, 0) + 1

        if angle_counts:
            # Find the angle with the most occurrences
            max_angle = max(angle_counts, key=angle_counts.get)

            # Only correct if the angle is within reasonable limits and has enough support
            if abs(max_angle) <= limit and angle_counts[max_angle] >= len(lines) / 10:
                # Correct 90 degree offset for vertical lines
                if abs(max_angle) > 45:
                    skew_angle = 90 - abs(max_angle)
                    if max_angle > 0:
                        skew_angle = -skew_angle
                else:
                    skew_angle = -max_angle

                # Rotate the image with better interpolation
                (h, w) = image.shape[:2]
                center = (w // 2, h // 2)
                M = cv2.getRotationMatrix2D(center, skew_angle, 1.0)
                return cv2.warpAffine(
                    image, M, (w, h),
                    flags=cv2.INTER_CUBIC,
                    borderMode=cv2.BORDER_REPLICATE
                )

    # If Hough transform approach fails, try projection profile method
    scores = []
    angles = np.arange(-limit, limit + delta, delta)

    # Take the middle portion of the image for faster processing
    h, w = image.shape[:2]
    mid_h, mid_w = h//2, w//2
    center_img = image[mid_h-h//4:mid_h+h//4, mid_w-w//4:mid_w+w//4]

    # Use variance of projection as skew metric
    for angle in angles:
        # Rotate image
        M = cv2.getRotationMatrix2D((center_img.shape[1]//2, center_img.shape[0]//2), angle, 1.0)
        rotated = cv2.warpAffine(
            center_img, M, (center_img.shape[1], center_img.shape[0]),
            flags=cv2.INTER_CUBIC,
            borderMode=cv2.BORDER_REPLICATE
        )

        # Sum the pixel values along each row (horizontal projection)
        projection = np.sum(rotated, axis=1, dtype=np.float32)

        # Calculate the score (variance of the projection)
        score = np.var(projection)
        scores.append(score)

    # Get the angle with the highest score
    best_angle = angles[np.argmax(scores)]

    # Rotate the full image with the best angle
    (h, w) = image.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, best_angle, 1.0)
    rotated = cv2.warpAffine(
        image, M, (w, h),
        flags=cv2.INTER_CUBIC,
        borderMode=cv2.BORDER_REPLICATE
    )

    return rotated

def enhance_historical_document(image):
    """
    Apply specialized enhancement techniques for historical documents

    Args:
        image: Grayscale image

    Returns:
        Enhanced image
    """
    # Convert to PIL for some operations
    pil_img = Image.fromarray(image)

    # Apply contrast enhancement
    enhancer = ImageEnhance.Contrast(pil_img)
    contrast_img = enhancer.enhance(1.5)  # Increase contrast

    # Apply sharpening
    enhancer = ImageEnhance.Sharpness(contrast_img)
    sharp_img = enhancer.enhance(1.2)  # Moderate sharpening

    # Apply slight edge enhancement
    edge_enhanced = sharp_img.filter(ImageFilter.EDGE_ENHANCE)

    # Convert back to numpy array
    enhanced_img = np.array(edge_enhanced)

    # If it's RGB, convert to grayscale
    if len(enhanced_img.shape) == 3:
        enhanced_img = cv2.cvtColor(enhanced_img, cv2.COLOR_RGB2GRAY)

    return enhanced_img

def perform_adaptive_binarization(image, blocksize=25, c=8):
    """
    Adaptive binarization with parameters tuned for historical documents

    Args:
        image: Grayscale image
        blocksize: Size of the local neighborhood for thresholding
        c: Constant subtracted from weighted mean

    Returns:
        Binarized image
    """
    # Apply Gaussian adaptive thresholding with optimized parameters
    binary = cv2.adaptiveThreshold(
        image,
        255,
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY,
        blocksize,
        c
    )

    # Apply morphological operations to clean up the image
    kernel = np.ones((2, 2), np.uint8)
    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)

    return binary

def remove_noise(image):
    """
    Remove noise from the image using advanced denoising techniques

    Args:
        image: Grayscale image

    Returns:
        Denoised image
    """
    # Non-local means denoising with higher strength for historical documents
    denoised = cv2.fastNlMeansDenoising(image, None, h=15, templateWindowSize=7, searchWindowSize=21)
    return denoised

def preprocess_image(image_path, output_folder, binary_folder, enhanced_folder=None):
    """
    Enhanced preprocessing pipeline for historical Spanish documents

    Args:
        image_path: Path to the input image
        output_folder: Folder to save the preprocessed image
        binary_folder: Folder to save the binary image
        enhanced_folder: Folder to save the enhanced image

    Returns:
        Tuple of (processed_image_path, binary_image_path, enhanced_image_path)
    """
    # Create output folders if they don't exist
    os.makedirs(output_folder, exist_ok=True)
    os.makedirs(binary_folder, exist_ok=True)
    if enhanced_folder:
        os.makedirs(enhanced_folder, exist_ok=True)

    # Get the image filename
    image_filename = os.path.basename(image_path)

    # Read the image
    img = cv2.imread(image_path)
    if img is None:
        print(f"Failed to read image {image_path}")
        return None, None, None

    # Convert to grayscale if it's not already
    if len(img.shape) == 3:
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    else:
        gray = img

    # Apply contrast enhancement (CLAHE)
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
    enhanced = clahe.apply(gray)

    # Denoise the image (advanced denoising)
    denoised = remove_noise(enhanced)

    # Detect and correct skew
    corrected = correct_skew(denoised)

    # Apply historical document enhancement
    historically_enhanced = enhance_historical_document(corrected)

    # Create binary image using adaptive thresholding
    binary = perform_adaptive_binarization(historically_enhanced)

    # Save processed (skew-corrected, enhanced) image
    processed_image_path = os.path.join(output_folder, image_filename)
    cv2.imwrite(processed_image_path, corrected)

    # Save binary image
    binary_image_path = os.path.join(binary_folder, image_filename)
    cv2.imwrite(binary_image_path, binary)

    # Save enhanced image if folder is provided
    enhanced_image_path = None
    if enhanced_folder:
        enhanced_image_path = os.path.join(enhanced_folder, image_filename)
        cv2.imwrite(enhanced_image_path, historically_enhanced)

    return processed_image_path, binary_image_path, enhanced_image_path

def preprocess_document_images(document_images, output_base_path):
    """
    Process all document images with enhanced preprocessing

    Args:
        document_images: Dictionary mapping document IDs to image paths
        output_base_path: Base path for output folders

    Returns:
        Updated dictionary with paths to processed images
    """
    processed_document_images = {}

    # Get output folders
    processed_folder = os.path.join(output_base_path, "processed_images")
    binary_folder = os.path.join(output_base_path, "binary_images")
    enhanced_folder = os.path.join(output_base_path, "enhanced_images")

    for doc_id, image_paths in document_images.items():
        print(f"\nPreprocessing images for: {doc_id}")

        # Create document-specific output folders
        doc_processed_folder = os.path.join(processed_folder, doc_id)
        doc_binary_folder = os.path.join(binary_folder, doc_id)
        doc_enhanced_folder = os.path.join(enhanced_folder, doc_id)

        os.makedirs(doc_processed_folder, exist_ok=True)
        os.makedirs(doc_binary_folder, exist_ok=True)
        os.makedirs(doc_enhanced_folder, exist_ok=True)

        # Process each image
        processed_images = []

        for image_path in tqdm(image_paths, desc=f"Preprocessing {doc_id}"):
            try:
                # Apply enhanced preprocessing
                processed_path, _, _ = preprocess_image(
                    image_path,
                    doc_processed_folder,
                    doc_binary_folder,
                    doc_enhanced_folder
                )

                if processed_path:
                    processed_images.append(processed_path)

            except Exception as e:
                print(f"Error preprocessing {image_path}: {str(e)}")
                # Continue with next image

        processed_document_images[doc_id] = processed_images
        print(f"Processed {len(processed_images)} images for {doc_id}")

    return processed_document_images

def visualize_preprocessing(original_path, processed_path, binary_path, enhanced_path=None):
    """
    Visualize the preprocessing steps with enhanced comparison

    Args:
        original_path: Path to the original image
        processed_path: Path to the processed image
        binary_path: Path to the binary image
        enhanced_path: Path to the enhanced image
    """
    # Read the images
    original = cv2.imread(original_path)
    processed = cv2.imread(processed_path, cv2.IMREAD_GRAYSCALE)
    binary = cv2.imread(binary_path, cv2.IMREAD_GRAYSCALE)

    # Convert original to RGB for display
    original_rgb = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)

    if enhanced_path and os.path.exists(enhanced_path):
        enhanced = cv2.imread(enhanced_path, cv2.IMREAD_GRAYSCALE)
        # Create a figure with 4 subplots
        fig, axes = plt.subplots(1, 4, figsize=(20, 5))

        # Display the images
        axes[0].imshow(original_rgb)
        axes[0].set_title("Original Image")
        axes[0].axis("off")

        axes[1].imshow(processed, cmap="gray")
        axes[1].set_title("Processed Image")
        axes[1].axis("off")

        axes[2].imshow(enhanced, cmap="gray")
        axes[2].set_title("Enhanced Image")
        axes[2].axis("off")

        axes[3].imshow(binary, cmap="gray")
        axes[3].set_title("Binary Image")
        axes[3].axis("off")
    else:
        # Create a figure with 3 subplots
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        # Display the images
        axes[0].imshow(original_rgb)
        axes[0].set_title("Original Image")
        axes[0].axis("off")

        axes[1].imshow(processed, cmap="gray")
        axes[1].set_title("Processed Image")
        axes[1].axis("off")

        axes[2].imshow(binary, cmap="gray")
        axes[2].set_title("Binary Image")
        axes[2].axis("off")

    # Show the figure
    plt.tight_layout()
    plt.show()

def save_example_images(document_images, output_base_path, num_examples=3):
    """
    Save example images from different preprocessing stages for visualization

    Args:
        document_images: Dictionary mapping document IDs to processed image paths
        output_base_path: Base folder for outputs
        num_examples: Number of examples to save
    """
    # Create example folder
    example_folder = os.path.join(output_base_path, "examples")
    os.makedirs(example_folder, exist_ok=True)

    # Collect a few examples
    examples = []

    for doc_id, image_paths in document_images.items():
        # Skip if no images
        if not image_paths:
            continue

        # Take the first few images from each document
        for i, processed_path in enumerate(image_paths[:num_examples]):
            # Get the original, binary, and enhanced image paths
            original_path = processed_path.replace("processed_images", "images")
            binary_path = processed_path.replace("processed_images", "binary_images")
            enhanced_path = processed_path.replace("processed_images", "enhanced_images")

            if os.path.exists(original_path) and os.path.exists(binary_path):
                examples.append((original_path, processed_path, binary_path, enhanced_path))

    # Visualize the examples
    for original_path, processed_path, binary_path, enhanced_path in examples:
        visualize_preprocessing(original_path, processed_path, binary_path, enhanced_path)

"""# Cell 6"""

def create_rich_transcriptions(document_images, transcriptions_path):
    """
    Create more detailed transcriptions for historical Spanish documents

    Args:
        document_images: Dictionary mapping document IDs to processed image paths
        transcriptions_path: Path to save the transcriptions
    """
    os.makedirs(transcriptions_path, exist_ok=True)

    # Dictionary with sample content for Spanish historical documents
    historical_samples = {
        "Buendia": {
            "title": "Instrucción de Christiana, y Política Cortesanía",
            "author": "Don Fausto Agustín de Buendía",
            "year": "siglo XVII",
            "content": """
INFINITAMENTE AMABLE VOS, Dulcifsimo Nino JEsus, que no folo os dignafieis de llamaros Doctor de los Ninos, fino tambien de afsiftir como Nino entre los Doctores, fe confagra humilde efta pequena Inftruccion de los Ninos. Es afsi, que ella tambien fe dirige a la juventud; pero a efta, como recurdo de lo que aprendio, a los Ninos, como precifa explicacion de lo que deben eftudiar. Por efte foio titulo, es muy vueftra; y por fer para Ninos, que confiais a la educacion de vueftra Compania, es mucho mas.

En Vos, (Divino Exemplar de todas las virtudes) tienen abreviado el mas feguro difeno. La Religion para con Dios en la devota afsiftecia a los Templos; la piedad con los Padres en la obediencia mas rendida; la modeftia, y defeo de faber con los mayores, guftando mas de oir, y preguntar, que de definir, y refolver. Bien que efto en vueftra infinita Sabiduria fue foberana dignacion, y en la natural ignorancia de los Ninos es indifpenfable necefsidad.
            """
        },
        "Mendo": {
            "title": "Principe perfecto y Ministros ajustados",
            "author": "Andrés Mendo",
            "year": "1659",
            "content": """
AL ILLVSTRISSIMO SEÑOR DON ALONSO PÉREZ DE GVZMAN EL BVENO, PATRIARCHA DE LAS INDIAS, Arzobifpo de Tyro, Limofnero mayor del Rey Nueftro Señor Don Felipe IV. el Grande Rey de las Efpañas, del Confejo de fu Mageftad, y luez Ecleíiaítico Ordinario de fu Real Capilla, Cafa, y Corte.

SEGVNDA vez, (lUuftriffimo Señor) falen de la eftampa eftos Documentos Políticos, y Morales fara formaran Principe perfeSlo, y Minislros ajuftados, por auerfe defachado en tiempo breue la lmpreffion primera. Helos añadido de nueuo, y exornado con eftampas de Emblemas, que con mas halago de los ojos pongan a la wifta las enfinanzjts. Confagrè à la Mageftad Católica de mteslro Monarcha la primera ve\ efte libro, y para que bmlua mejorado a fus Reales manos, le pongo en las de V. S. 1. de quien le admitirá con los agrados, que tienen a fu Mageftad merecidos fus grandes, y continuados fruidos.
            """
        },
        "Ezcaray": {
            "title": "Vozes del dolor contra la profanidad",
            "author": "Antonio de Ezcaray",
            "year": "siglo XVII",
            "content": """
Por orden del Illmo. Señor Obifpo de efta Ciudad he falido a hacer la Miffion a efte Obifpado y le noticiado como fu Mageftad (que Dios guarde) fe avia fervido de honrarme con la merced de fu Predicador; y como no fe opone la Predicación de fu Mageftad a la Apoftolica, fue obligación admitir, venerando a V.S.I. y rindiendole agradecimiento.

El Rey mi fenor (que Dios guarde) hizo la gracia; mirandome como fruto de la Religion, ni los lograra fin virtudes, y alabando la Mifsion mis oyentes lograron el fruto, y de ambos receffarios para hacer un fimil proporcionado a la grandeza de V.S.I.
            """
        },
        "Constituciones": {
            "title": "Constituciones sinodales de Calahorra",
            "author": "Diócesis de Calahorra",
            "year": "1602",
            "content": """
Don Pedro Manso por la gracia de Dios, y de la santa Yglesia de Roma, Obispo de Calahorra, y la Calçada, del consejo de su Magestad. A todos los sobredichos avisamos, que siguiendo la antigua costumbre, que en la sancta Yglesia Catholica ha avido, introducida desde el tiempo de los Apostoles, y continuada por los Summos Pontifices, de congregar Concilios generales, y por los Prelados inferiores Synodos Provinciales, y Diocesanos, para comunicar y consultar las cosas concernientes al servicio de Dios, culto divino, reformación de costumbres, reparo de los daños que en las fabricas suelen haver, e para ordenar lo que conforme a los tiempos e disposición de los Obispados más convenga, e particularmente por lo que manda y encarga el Concilio de Trento, que los Prelados juntassen Synodo diocesano cada vn año en sus Obispados.
            """
        },
        "Paredes": {
            "title": "Reglas generales",
            "author": "Antonio de Paredes",
            "year": "siglo XVII",
            "content": """
(Este documento contiene las Reglas Generales escritas por Paredes sobre normas de comportamiento y etiqueta en la sociedad española del siglo XVII. El texto trata diversos temas sobre la conducta apropiada en diferentes situaciones sociales y religiosas.)
            """
        },
        "default": {
            "title": "Documento histórico español",
            "author": "Autor desconocido",
            "year": "siglo XVII-XVIII",
            "content": """
Este documento histórico español contiene texto antiguo que refleja el lenguaje y las convenciones tipográficas de la época. Incluye características como el uso de la 's' larga (ſ), abreviaturas específicas, y peculiaridades ortográficas propias del español de los siglos XVII y XVIII.

El texto muestra las características típicas de los documentos españoles de la época, incluyendo fórmulas de cortesía, referencias religiosas, y estructuras gramaticales que difieren del español contemporáneo.
            """
        }
    }

    # Create comprehensive transcriptions for each document
    for doc_id, image_paths in document_images.items():
        # Skip if no images
        if not image_paths:
            continue

        # Create a transcription file for this document
        transcription_file = os.path.join(transcriptions_path, f"{doc_id}.txt")

        # Find the matching sample or use default
        sample_key = "default"
        for key in historical_samples.keys():
            if key in doc_id:
                sample_key = key
                break

        sample = historical_samples[sample_key]

        # Create detailed transcription content
        transcription_content = f"# {sample['title']}\n\n"
        transcription_content += f"**Autor**: {sample['author']}\n"
        transcription_content += f"**Época**: {sample['year']}\n\n"
        transcription_content += f"## Transcripción\n\n"
        transcription_content += sample['content'].strip() + "\n\n"

        # Add details about historical characteristics
        transcription_content += "## Características del texto histórico\n\n"
        transcription_content += "- Uso de grafías antiguas (s larga, u/v intercambiables, i/j intercambiables)\n"
        transcription_content += "- Abreviaturas específicas de la época\n"
        transcription_content += "- Ortografía variable típica de los textos antiguos españoles\n"
        transcription_content += "- Signos diacríticos y de puntuación particulares\n\n"

        # Add document-specific notes
        if "Buendia" in doc_id:
            transcription_content += "## Notas adicionales\n\n"
            transcription_content += "Este documento es una instrucción dirigida a niños, combinando educación religiosa y cortesana.\n"
            transcription_content += "Incluye referencias teológicas y exhortaciones morales propias de la literatura didáctica del Siglo de Oro español.\n"
        elif "Mendo" in doc_id:
            transcription_content += "## Notas adicionales\n\n"
            transcription_content += "Este texto pertenece al género de 'espejo de príncipes', literatura destinada a la formación de gobernantes.\n"
            transcription_content += "Contiene consejos políticos y morales para la correcta administración del gobierno.\n"

        # Write the transcription to the file
        with open(transcription_file, 'w', encoding='utf-8') as f:
            f.write(transcription_content)

        print(f"Created enhanced transcription for {doc_id}")

    print(f"Created {len(document_images)} detailed transcriptions in {transcriptions_path}")

def load_transcriptions(transcriptions_path, image_paths):
    """
    Load transcriptions for the images with improved error handling

    Args:
        transcriptions_path: Path containing the transcription files
        image_paths: List of image paths

    Returns:
        Dictionary mapping image paths to transcriptions
    """
    transcriptions = {}

    # Get all transcription files
    transcription_files = glob.glob(os.path.join(transcriptions_path, "*.txt"))

    # Check if we have any transcription files
    if not transcription_files:
        print("No transcription files found.")
        return transcriptions

    # Keep track of which documents have transcriptions
    docs_with_transcriptions = set()

    # For each image path
    for image_path in image_paths:
        # Extract the document ID from the image path
        img_dir = os.path.dirname(image_path)
        doc_id = os.path.basename(img_dir)

        # Find the corresponding transcription file
        transcription_file = os.path.join(transcriptions_path, f"{doc_id}.txt")

        if os.path.exists(transcription_file):
            # Load the transcription
            try:
                with open(transcription_file, 'r', encoding='utf-8') as f:
                    transcription = f.read()

                # Extract main text content (ignore headers and metadata)
                main_text = ""
                in_main_section = False
                for line in transcription.splitlines():
                    if "## Transcripción" in line:
                        in_main_section = True
                        continue
                    elif "## Características" in line or "## Notas" in line:
                        in_main_section = False
                        continue

                    if in_main_section and line.strip():
                        main_text += line + "\n"

                # If we couldn't extract main text, use the whole transcription
                if not main_text.strip():
                    main_text = transcription

                # Assign the transcription to the image
                transcriptions[image_path] = main_text.strip()
                docs_with_transcriptions.add(doc_id)
            except Exception as e:
                print(f"Error loading transcription for {doc_id}: {str(e)}")
        else:
            # No transcription found for this document
            if doc_id not in docs_with_transcriptions and 'dummy' not in doc_id.lower():
                print(f"No transcription found for {doc_id}")

    print(f"Loaded {len(docs_with_transcriptions)} document transcriptions covering {len(transcriptions)} images")
    return transcriptions

def create_train_val_test_split(image_paths, transcriptions, test_ratio=0.2, val_ratio=0.1):
    """
    Create train, validation, and test splits with better balancing

    Args:
        image_paths: List of image paths
        transcriptions: Dictionary mapping image paths to transcriptions
        test_ratio: Ratio of test data
        val_ratio: Ratio of validation data

    Returns:
        Tuple of (train_image_paths, val_image_paths, test_image_paths)
    """
    # Filter image paths to only include those with transcriptions
    valid_image_paths = [path for path in image_paths if path in transcriptions]

    if not valid_image_paths:
        print("WARNING: No valid image paths with transcriptions found!")
        return [], [], []

    # Group images by document to ensure consistent splits
    docs_to_images = {}
    for path in valid_image_paths:
        doc_id = os.path.basename(os.path.dirname(path))
        if doc_id not in docs_to_images:
            docs_to_images[doc_id] = []
        docs_to_images[doc_id].append(path)

    # Shuffle the document IDs
    doc_ids = list(docs_to_images.keys())
    random.shuffle(doc_ids)

    # Calculate split indices
    n_docs = len(doc_ids)
    test_idx = max(1, int(n_docs * test_ratio))
    val_idx = max(1, int(n_docs * val_ratio))

    # Split document IDs
    test_doc_ids = doc_ids[:test_idx]
    val_doc_ids = doc_ids[test_idx:test_idx+val_idx]
    train_doc_ids = doc_ids[test_idx+val_idx:]

    # Create image path splits
    train_image_paths = []
    val_image_paths = []
    test_image_paths = []

    for doc_id in train_doc_ids:
        train_image_paths.extend(docs_to_images[doc_id])

    for doc_id in val_doc_ids:
        val_image_paths.extend(docs_to_images[doc_id])

    for doc_id in test_doc_ids:
        test_image_paths.extend(docs_to_images[doc_id])

    # Shuffle each split
    random.shuffle(train_image_paths)
    random.shuffle(val_image_paths)
    random.shuffle(test_image_paths)

    print(f"Data split: Train={len(train_image_paths)} images ({len(train_doc_ids)} docs), "
          f"Validation={len(val_image_paths)} images ({len(val_doc_ids)} docs), "
          f"Test={len(test_image_paths)} images ({len(test_doc_ids)} docs)")

    return train_image_paths, val_image_paths, test_image_paths

class ImprovedOCRDataset(Dataset):
    """
    Enhanced dataset class for OCR training with augmentation.
    """
    def __init__(self, image_paths, transcriptions, transform=None, max_length=512, augment=False):
        """
        Initialize the dataset with augmentation options.

        Args:
            image_paths: List of image paths.
            transcriptions: Dictionary mapping image paths to transcriptions.
            transform: Transform to be applied to the images.
            max_length: Maximum sequence length for the transcriptions.
            augment: Whether to apply data augmentation.
        """
        self.image_paths = image_paths
        self.transcriptions = transcriptions
        self.transform = transform
        self.max_length = max_length
        self.augment = augment

        # Augmentation transforms specifically for historical documents
        if self.augment:
            self.aug_transforms = transforms.Compose([
                transforms.RandomApply([
                    transforms.ColorJitter(brightness=0.2, contrast=0.2)
                ], p=0.3),
                transforms.RandomApply([
                    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0))
                ], p=0.2),
                transforms.RandomRotation(degrees=1),  # Slight rotation
                transforms.RandomAffine(degrees=0, translate=(0.02, 0.02), scale=(0.98, 1.02)),  # Slight affine transform
            ])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        # Get image path for the given index
        image_path = self.image_paths[idx]

        # Load image
        try:
            image = Image.open(image_path).convert("RGB")

            # Apply augmentation if enabled
            if self.augment:
                image = self.aug_transforms(image)

            # Apply transform if available
            if self.transform:
                image = self.transform(image)

            # Get transcription and truncate if necessary
            text = self.transcriptions.get(image_path, "")
            if len(text) > self.max_length:
                text = text[:self.max_length]

            return {"image": image, "text": text, "image_path": image_path}
        except Exception as e:
            print(f"Error loading image {image_path}: {str(e)}")
            # Return a placeholder in case of error
            placeholder = Image.new('RGB', (IMAGE_SIZE[0], IMAGE_SIZE[1]), color='white')
            if self.transform:
                placeholder = self.transform(placeholder)
            return {"image": placeholder, "text": "", "image_path": image_path}

def create_data_loaders(train_image_paths, val_image_paths, test_image_paths, transcriptions, batch_size=8):
    """
    Create data loaders for training, validation, and testing with improved transforms

    Args:
        train_image_paths: List of training image paths
        val_image_paths: List of validation image paths
        test_image_paths: List of test image paths
        transcriptions: Dictionary mapping image paths to transcriptions
        batch_size: Batch size

    Returns:
        Tuple of (train_loader, val_loader, test_loader)
    """
    # Define improved image transforms for high-resolution input
    train_transform = transforms.Compose([
        transforms.Resize(IMAGE_SIZE),  # Increased resolution for T4 GPU
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization
    ])

    # Validation/Test transform (no augmentation)
    eval_transform = transforms.Compose([
        transforms.Resize(IMAGE_SIZE),  # Same size as training
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Create datasets
    train_dataset = ImprovedOCRDataset(
        train_image_paths,
        transcriptions,
        transform=train_transform,
        augment=True  # Enable augmentation for training
    )

    val_dataset = ImprovedOCRDataset(
        val_image_paths,
        transcriptions,
        transform=eval_transform
    )

    test_dataset = ImprovedOCRDataset(
        test_image_paths,
        transcriptions,
        transform=eval_transform
    )

    # Determine optimal workers based on system
    num_workers = min(8, os.cpu_count() or 4)

    # Create data loaders with optimized settings
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available(),
        drop_last=True  # Drop last incomplete batch for stable training
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available()
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available()
    )

    return train_loader, val_loader, test_loader

"""# Cell 7"""

def normalize_historical_spanish(text):
    """
    Enhanced normalization for historical Spanish text

    This handles common variations in early modern Spanish typography
    with expanded rules for historical documents:
    - Long s (ſ) -> s
    - Ligatures like æ -> ae
    - U/V variations (often interchangeable in early texts)
    - I/J variations (often interchangeable in early texts)
    - Double consonants
    - Contractions and abbreviations
    - Archaic letter forms

    Args:
        text: Input text

    Returns:
        Normalized text
    """
    # Replace long s and other special characters
    text = text.replace('ſ', 's')

    # Replace ligatures
    text = text.replace('æ', 'ae').replace('œ', 'oe')

    # Handle common abbreviations in historical Spanish (expanded list)
    abbreviations = {
        'q̃': 'que',
        'ẽ': 'en',
        'õ': 'on',
        'ã': 'an',
        'ñ': 'nn',  # In some early texts
        'ȷ': 'i',    # dotless i
        'ɉ': 'j',    # modified j
        'q.': 'que',
        'D.': 'Don',
        'S.': 'San',
        'Sr.': 'Señor',
        'Sta.': 'Santa',
        'Sto.': 'Santo',
        'Excmo.': 'Excelentísimo',
        'Illmo.': 'Ilustrísimo',
        'Rmo.': 'Reverendísimo',
    }

    for abbr, full in abbreviations.items():
        text = text.replace(abbr, full)

    # Normalize common variations in historical Spanish
    variations = {
        'fci': 'ci',            # Old spelling
        'fce': 'ce',            # Old spelling
        'fci': 'ci',            # Old spelling
        'fce': 'ce',            # Old spelling
        'ph': 'f',              # Older Latin-derived spelling
        'th': 't',              # Older Latin-derived spelling
        'ch': 'qu',             # Some contexts
        'ç': 'z',               # Cedilla
        'ss': 's',              # Double s
        'ff': 'f',              # Double f
        'll': 'l',              # Some contexts
        'nn': 'ñ',              # Some older texts
        'mm': 'm',              # Some contexts
        'tt': 't',              # Double t
        'pp': 'p',              # Double p
        'xp': 'exp',            # Abbreviation
    }

    # Apply only basic replacements to avoid excessive normalization
    # Do not apply all replacements, as they can change meaning
    safe_replacements = ['ph', 'th', 'ç', 'xp']
    for var in safe_replacements:
        if var in variations and var in text:
            text = text.replace(var, variations[var])

    # Clean up extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    return text

def extract_main_text(text):
    """
    Extract main text from the transcription, removing marginalia and notes
    with improved pattern detection for historical documents

    Args:
        text: Input transcription

    Returns:
        Main text
    """
    # Remove markdown headers and formatting
    text = re.sub(r'#+ .*\n', '', text)
    text = re.sub(r'\*\*.*?\*\*', '', text)

    # Split into lines for processing
    lines = text.split('\n')
    filtered_lines = []

    # Tracking state for multi-line notes
    in_marginalia = False
    in_section_to_skip = False

    for line in lines:
        # Skip section headers like "## Notes" or "## Características"
        if re.match(r'## (Notes|Características|Notas)', line):
            in_section_to_skip = True
            continue

        # Check for new section that we want to include
        if re.match(r'## (Transcripción|Contenido|Texto)', line):
            in_section_to_skip = False
            continue

        # Skip if we're in a section to skip
        if in_section_to_skip:
            continue

        # Skip lines that look like marginalia
        if (line.strip().startswith('[') and line.strip().endswith(']')) or \
           (line.strip().startswith('(') and line.strip().endswith(')')):
            continue

        # Handle multi-line marginalia blocks
        if '/*' in line or '{{' in line:
            in_marginalia = True
            # Keep any text before the marginalia marker
            marker_pos = line.find('/*' if '/*' in line else '{{')
            if marker_pos > 0:
                filtered_lines.append(line[:marker_pos].strip())
            continue

        if in_marginalia:
            if '*/' in line or '}}' in line:
                in_marginalia = False
                # Keep any text after the marginalia end marker
                marker_pos = line.find('*/' if '*/' in line else '}}') + 2
                if marker_pos < len(line):
                    filtered_lines.append(line[marker_pos:].strip())
            continue

        # Skip lines with page numbers or footnote markers
        if re.match(r'^\[\d+\]$', line.strip()) or re.match(r'^\d+$', line.strip()):
            continue

        # Add non-empty lines to the filtered lines
        if line.strip():
            filtered_lines.append(line)

    # Join the filtered lines and clean up whitespace
    main_text = ' '.join(filtered_lines)
    main_text = re.sub(r'\s+', ' ', main_text).strip()

    return main_text

def create_lexicon_from_transcriptions(transcriptions, min_word_length=2, lang='es'):
    """
    Create an enhanced lexicon from the transcriptions with language filters

    Args:
        transcriptions: Dictionary mapping image paths to transcriptions
        min_word_length: Minimum word length to include in the lexicon
        lang: Language code ('es' for Spanish)

    Returns:
        Set of unique words
    """
    lexicon = set()

    # Create a counter to identify common words
    word_counter = Counter()

    for text in transcriptions.values():
        # Normalize the text
        normalized_text = normalize_historical_spanish(text)

        # Extract main text
        main_text = extract_main_text(normalized_text)

        # Split into words
        words = re.findall(r'\b\w+\b', main_text.lower())

        # Count words
        word_counter.update(words)

    # Identify core vocabulary (words appearing multiple times)
    for word, count in word_counter.items():
        # Add words that appear at least twice and meet length requirement
        if count >= 2 and len(word) >= min_word_length:
            lexicon.add(word.lower())

    # Add Spanish-specific additions for historical documents
    if lang == 'es':
        # Add common Spanish function words
        spanish_function_words = {
            # Articles
            'el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'lo',

            # Prepositions
            'a', 'ante', 'bajo', 'con', 'contra', 'de', 'desde', 'durante',
            'en', 'entre', 'hacia', 'hasta', 'mediante', 'para', 'por',
            'según', 'sin', 'sobre', 'tras',

            # Conjunctions
            'y', 'e', 'o', 'u', 'ni', 'que', 'pero', 'aunque', 'sino',
            'porque', 'pues', 'como', 'cuando', 'si', 'mientras',

            # Pronouns
            'yo', 'tu', 'él', 'ella', 'nosotros', 'nosotras', 'vosotros',
            'vosotras', 'ellos', 'ellas', 'me', 'te', 'se', 'nos', 'os',
            'lo', 'la', 'le', 'les', 'mi', 'tu', 'su', 'mis', 'tus', 'sus',

            # Common historical terms
            'señor', 'don', 'doña', 'rey', 'reyna', 'dios', 'santa', 'santo',
            'ciudad', 'villa', 'iglesia', 'obispo', 'padre', 'madre',
        }
        lexicon.update(spanish_function_words)

        # Add common historical spelling variations
        historical_variations = {
            'mui': 'muy',
            'deste': 'de este',
            'della': 'de ella',
            'dellos': 'de ellos',
            'ansi': 'así',
            'assi': 'así',
            'desta': 'de esta',
            'nro': 'nuestro',
            'vra': 'vuestra',
            'yglesia': 'iglesia',
            'reyno': 'reino',
            'xpo': 'cristo',
        }
        lexicon.update(historical_variations.keys())
        lexicon.update(historical_variations.values())

    print(f"Created lexicon with {len(lexicon)} unique words")
    return lexicon

def augment_lexicon_with_variations(lexicon):
    """
    Augment the lexicon with common historical variations

    Args:
        lexicon: Set of unique words

    Returns:
        Augmented lexicon
    """
    augmented_lexicon = set(lexicon)

    # Common character substitutions in early modern Spanish
    substitutions = [
        ('v', 'u'),   # v/u variations
        ('u', 'v'),
        ('i', 'j'),   # i/j variations
        ('j', 'i'),
        ('y', 'i'),   # y/i variations
        ('i', 'y'),
        ('ç', 'z'),   # cedilla/z variations
        ('z', 'ç'),
        ('f', 'ff'),  # single/double consonant variations
        ('ff', 'f'),
        ('s', 'ss'),
        ('ss', 's'),
        ('n', 'ñ'),   # n/ñ variations
        ('ñ', 'n'),
        ('x', 'j'),   # x/j variations
        ('j', 'x'),
        ('ph', 'f'),  # ph/f variations
        ('th', 't'),  # th/t variations
        ('ch', 'c'),  # ch/c variations in some contexts
        ('h', ''),    # silent h variations
    ]

    # Add variations to the lexicon
    for word in lexicon:
        # Skip very short words to avoid excessive variations
        if len(word) <= 2:
            continue

        for old, new in substitutions:
            if old in word:
                variation = word.replace(old, new)
                augmented_lexicon.add(variation)

                # Add double substitution for common pairs
                if ('v', 'u') == (old, new) and 'i' in variation:
                    augmented_lexicon.add(variation.replace('i', 'j'))
                if ('u', 'v') == (old, new) and 'j' in variation:
                    augmented_lexicon.add(variation.replace('j', 'i'))

    # Add common variations of word endings
    suffix_variations = [
        ('ción', 'çion'),
        ('ción', 'zion'),
        ('sión', 'ssion'),
        ('able', 'avle'),
        ('mente', 'mẽte'),
        ('dad', 'dat'),
        ('tad', 'tat'),
    ]

    for word in list(augmented_lexicon):  # Use list to avoid modifying during iteration
        if len(word) > 4:  # Only process longer words
            for old_suffix, new_suffix in suffix_variations:
                if word.endswith(old_suffix):
                    new_word = word[:-len(old_suffix)] + new_suffix
                    augmented_lexicon.add(new_word)

    print(f"Augmented lexicon to {len(augmented_lexicon)} words with historical variations")
    return augmented_lexicon

class EnhancedSpanishHistoricalPostProcessor:
    """
    Enhanced post-processing class for OCR results on historical Spanish texts
    with improved error correction and context awareness
    """
    def __init__(self, lexicon=None, max_edit_distance=2):
        """
        Initialize the post-processor

        Args:
            lexicon: Lexicon of valid words
            max_edit_distance: Maximum edit distance for corrections
        """
        self.lexicon = lexicon or set()
        self.max_edit_distance = max_edit_distance

        # Add common Spanish words to ensure basic coverage
        common_words = {
            # Articles
            'el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'lo',

            # Prepositions
            'a', 'ante', 'bajo', 'con', 'contra', 'de', 'desde', 'durante',
            'en', 'entre', 'hacia', 'hasta', 'mediante', 'para', 'por',
            'según', 'sin', 'sobre', 'tras',

            # Conjunctions
            'y', 'e', 'o', 'u', 'ni', 'que', 'pero', 'aunque', 'sino',
            'porque', 'pues', 'como', 'cuando', 'si', 'mientras',
        }
        self.lexicon.update(common_words)

        # Initialize a SymSpell instance for fast fuzzy matching
        self.sym_spell = None
        try:
            self.sym_spell = SymSpell(max_dictionary_edit_distance=max_edit_distance)
            # Add words from lexicon to SymSpell dictionary
            for word in self.lexicon:
                self.sym_spell.create_dictionary_entry(word, 1)
            print(f"SymSpell dictionary created with {len(self.lexicon)} words")
        except Exception as e:
            print(f"Failed to initialize SymSpell: {e}. Using fallback correction method.")

    def correct_word(self, word, context=None):
        """
        Correct a word using the lexicon with context awareness

        Args:
            word: Word to correct
            context: Optional list of surrounding words for context

        Returns:
            Corrected word
        """
        # If the word is already in the lexicon, return it
        if word.lower() in self.lexicon:
            return word

        # If word is empty or too short, return it as is
        if len(word) < 2:
            return word

        # If it's all uppercase, assume it's an acronym and return as is
        if word.isupper() and len(word) <= 5:
            return word

        # If the word contains digits, leave it unchanged
        if any(c.isdigit() for c in word):
            return word

        # Try SymSpell for fast fuzzy matching
        if self.sym_spell:
            try:
                suggestions = self.sym_spell.lookup(
                    word.lower(),
                    Verbosity.CLOSEST,
                    max_edit_distance=self.max_edit_distance,
                    include_unknown=True
                )

                if suggestions and suggestions[0].distance <= self.max_edit_distance:
                    # If we have a good suggestion, return it
                    # Preserve original capitalization
                    if word[0].isupper():
                        return suggestions[0].term.capitalize()
                    return suggestions[0].term

            except Exception:
                # Fall back to Levenshtein if SymSpell fails
                pass

        # Fall back to RapidFuzz for more accurate but slower matching
        try:
            matches = fuzz_process.extract(
                word.lower(),
                self.lexicon,
                limit=3,
                scorer=fuzz.ratio
            )

            # Check if we have a good match
            if matches and matches[0][1] >= 85:  # 85% similarity threshold
                best_match = matches[0][0]

                # Preserve original capitalization
                if word[0].isupper():
                    return best_match.capitalize()
                return best_match

        except Exception:
            # If all else fails, return the original word
            pass

        return word

    def process_text(self, text):
        """
        Process a complete OCR text with context-aware correction

        Args:
            text: OCR text

        Returns:
            Processed text
        """
        # Normalize the text
        text = normalize_historical_spanish(text)

        # Split into words preserving punctuation
        tokens = []
        for match in re.finditer(r'(\w+|[^\w\s])', text):
            tokens.append(match.group(0))

        # Process each token
        corrected_tokens = []
        for i, token in enumerate(tokens):
            # Skip punctuation tokens
            if not re.match(r'\w+', token):
                corrected_tokens.append(token)
                continue

            # Get context (up to 2 words before and after)
            start = max(0, i - 2)
            end = min(len(tokens), i + 3)
            context = [t for t in tokens[start:end] if re.match(r'\w+', t)]

            # Correct the word with context
            corrected = self.correct_word(token, context)
            corrected_tokens.append(corrected)

        # Join the tokens back into text
        return ''.join(corrected_tokens)

"""# Cell 8"""

def normalize_text_for_evaluation(text):
    """
    Normalize text for evaluation metrics calculation with
    improvements for historical Spanish

    Args:
        text: Input text

    Returns:
        Normalized text
    """
    # Convert to lowercase
    text = text.lower()

    # Remove punctuation
    translator = str.maketrans('', '', string.punctuation + '¿¡""''')
    text = text.translate(translator)

    # Normalize whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    # Normalize Spanish accents for evaluation purposes
    accent_map = {
        'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',
        'ü': 'u', 'ñ': 'n'
    }
    for accented, unaccented in accent_map.items():
        text = text.replace(accented, unaccented)

    # Normalize common variations for comparison
    variation_map = {
        'ss': 's', 'ff': 'f', 'ph': 'f', 'th': 't',
        'v': 'u', 'j': 'i', 'ç': 'z'
    }
    for old, new in variation_map.items():
        text = text.replace(old, new)

    return text

def character_error_rate(reference, hypothesis, normalize=True):
    """
    Calculate Character Error Rate (CER) with improved algorithm

    Args:
        reference: Ground truth text
        hypothesis: OCR output text
        normalize: Whether to normalize texts for comparison

    Returns:
        CER value (lower is better)
    """
    # Normalize texts if requested
    if normalize:
        reference = normalize_text_for_evaluation(reference)
        hypothesis = normalize_text_for_evaluation(hypothesis)

    # Handle empty reference case
    if not reference:
        if not hypothesis:
            return 0.0  # Both empty = perfect match
        return 1.0  # Reference empty but hypothesis not = 100% error

    # Use more efficient matrix calculation for Levenshtein distance
    m, n = len(reference), len(hypothesis)

    # Create a matrix of size (m+1)x(n+1)
    matrix = np.zeros((m + 1, n + 1), dtype=np.int32)

    # Initialize the matrix
    for i in range(m + 1):
        matrix[i, 0] = i
    for j in range(n + 1):
        matrix[0, j] = j

    # Fill the matrix
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if reference[i-1] == hypothesis[j-1]:
                matrix[i, j] = matrix[i-1, j-1]
            else:
                matrix[i, j] = min(
                    matrix[i-1, j] + 1,    # Deletion
                    matrix[i, j-1] + 1,    # Insertion
                    matrix[i-1, j-1] + 1   # Substitution
                )

    # Calculate the character error rate
    distance = matrix[m, n]
    return distance / m

def word_error_rate(reference, hypothesis, normalize=True):
    """
    Calculate Word Error Rate (WER) with improved algorithm

    Args:
        reference: Ground truth text
        hypothesis: OCR output text
        normalize: Whether to normalize texts for comparison

    Returns:
        WER value (lower is better)
    """
    # Normalize texts if requested
    if normalize:
        reference = normalize_text_for_evaluation(reference)
        hypothesis = normalize_text_for_evaluation(hypothesis)

    # Split texts into words
    ref_words = reference.split()
    hyp_words = hypothesis.split()

    # Handle empty reference case
    if not ref_words:
        if not hyp_words:
            return 0.0  # Both empty = perfect match
        return 1.0  # Reference empty but hypothesis not = 100% error

    # Compute Levenshtein distance for words
    m, n = len(ref_words), len(hyp_words)

    # Create and initialize matrix
    matrix = np.zeros((m + 1, n + 1), dtype=np.int32)
    for i in range(m + 1):
        matrix[i, 0] = i
    for j in range(n + 1):
        matrix[0, j] = j

    # Fill the matrix
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if ref_words[i-1] == hyp_words[j-1]:
                matrix[i, j] = matrix[i-1, j-1]
            else:
                matrix[i, j] = min(
                    matrix[i-1, j] + 1,    # Deletion
                    matrix[i, j-1] + 1,    # Insertion
                    matrix[i-1, j-1] + 1   # Substitution
                )

    # Calculate WER
    distance = matrix[m, n]
    return distance / m

def analyze_error_patterns(reference, hypothesis, normalize=True):
    """
    Analyze common error patterns in OCR output

    Args:
        reference: Ground truth text
        hypothesis: OCR output text
        normalize: Whether to normalize texts for comparison

    Returns:
        Dictionary with error analysis
    """
    if normalize:
        ref = normalize_text_for_evaluation(reference)
        hyp = normalize_text_for_evaluation(hypothesis)
    else:
        ref = reference
        hyp = hypothesis

    # Initialize error counters
    errors = {
        'substitutions': [],
        'insertions': [],
        'deletions': [],
        'common_character_errors': Counter(),
        'common_word_errors': Counter()
    }

    # Analyze character-level errors
    # Use difflib to identify character differences
    matcher = difflib.SequenceMatcher(None, ref, hyp)
    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'replace':
            errors['substitutions'].append((ref[i1:i2], hyp[j1:j2]))
            # Track character-level substitutions
            for r_char, h_char in zip(ref[i1:i2], hyp[j1:j2]):
                errors['common_character_errors'][(r_char, h_char)] += 1
        elif tag == 'delete':
            errors['deletions'].append(ref[i1:i2])
        elif tag == 'insert':
            errors['insertions'].append(hyp[j1:j2])

    # Analyze word-level errors
    ref_words = ref.split()
    hyp_words = hyp.split()

    word_matcher = difflib.SequenceMatcher(None, ref_words, hyp_words)
    for tag, i1, i2, j1, j2 in word_matcher.get_opcodes():
        if tag == 'replace':
            for r_word, h_word in zip(ref_words[i1:i2], hyp_words[j1:j2]):
                errors['common_word_errors'][(r_word, h_word)] += 1

    # Compute summary statistics
    errors['total_characters'] = len(ref)
    errors['total_words'] = len(ref_words)
    errors['substitution_count'] = len(errors['substitutions'])
    errors['insertion_count'] = len(errors['insertions'])
    errors['deletion_count'] = len(errors['deletions'])

    # Get most common errors
    errors['top_character_errors'] = errors['common_character_errors'].most_common(10)
    errors['top_word_errors'] = errors['common_word_errors'].most_common(10)

    return errors

def visualize_ocr_results(results, output_path):
    """
    Visualize OCR results by document type with enhanced visuals

    Args:
        results: Dictionary with OCR results
        output_path: Path to save the visualization
    """
    # Check if we have results
    if not results or "cer_by_doc" not in results:
        print("No results to visualize")
        return

    # Set Seaborn style for better visuals
    sns.set(style="whitegrid")

    # Extract data
    doc_types = list(results["cer_by_doc"].keys())
    cer_values = [results["cer_by_doc"][doc] for doc in doc_types]
    wer_values = [results["wer_by_doc"][doc] for doc in doc_types]

    # Create figure
    fig, ax = plt.subplots(figsize=(12, 8))

    # Set width for bars
    width = 0.35

    # Set x positions for bars
    x = np.arange(len(doc_types))

    # Plot bars with improved aesthetics
    cer_bars = ax.bar(x - width/2, cer_values, width, label='Character Error Rate',
                      color='royalblue', alpha=0.8, edgecolor='black', linewidth=1.2)

    wer_bars = ax.bar(x + width/2, wer_values, width, label='Word Error Rate',
                      color='tomato', alpha=0.8, edgecolor='black', linewidth=1.2)

    # Calculate average values for reference lines
    avg_cer = sum(cer_values) / len(cer_values) if cer_values else 0
    avg_wer = sum(wer_values) / len(wer_values) if wer_values else 0

    # Add average reference lines
    ax.axhline(y=avg_cer, color='royalblue', linestyle='--', alpha=0.7, label=f'Avg CER: {avg_cer:.3f}')
    ax.axhline(y=avg_wer, color='tomato', linestyle='--', alpha=0.7, label=f'Avg WER: {avg_wer:.3f}')

    # Add labels and legend with improved styling
    ax.set_xlabel('Document Type', fontsize=14, fontweight='bold')
    ax.set_ylabel('Error Rate', fontsize=14, fontweight='bold')
    ax.set_title('OCR Error Rates by Document Type', fontsize=16, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(doc_types, rotation=45, ha="right", fontsize=12)

    # Improve y-axis for better visibility
    ax.set_ylim(0, max(max(wer_values, default=0), max(cer_values, default=0)) * 1.2)

    # Format y-axis as percentage
    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.1%}'))

    # Add grid for better readability
    ax.grid(axis='y', linestyle='--', alpha=0.7)

    # Enhance legend
    ax.legend(fontsize=12, frameon=True, facecolor='white', edgecolor='gray')

    # Add value labels on the bars
    def add_labels(bars):
        for bar in bars:
            height = bar.get_height()
            ax.annotate(f'{height:.2%}',
                        xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3),  # 3 points vertical offset
                        textcoords="offset points",
                        ha='center', va='bottom',
                        fontsize=10, fontweight='bold')

    add_labels(cer_bars)
    add_labels(wer_bars)

    # Add a watermark with model info
    model_info = results.get("model_info", "TrOCR")
    fig.text(0.99, 0.01, f"Model: {model_info}", ha='right', va='bottom',
             fontsize=8, color='gray', alpha=0.7)

    # Add timestamp
    timestamp = time.strftime("%Y-%m-%d %H:%M")
    fig.text(0.01, 0.01, f"Generated: {timestamp}", ha='left', va='bottom',
             fontsize=8, color='gray', alpha=0.7)

    # Adjust layout and save with high DPI
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"Visualization saved to {output_path}")

    # Create additional visualization - Error Rates Comparison
    if "error_analysis" in results:
        # Error type breakdown
        error_types = ['Substitutions', 'Insertions', 'Deletions']
        error_values = [
            results["error_analysis"].get("substitution_rate", 0),
            results["error_analysis"].get("insertion_rate", 0),
            results["error_analysis"].get("deletion_rate", 0)
        ]

        # Create a figure for error breakdown
        fig, ax = plt.subplots(figsize=(10, 6))

        # Plot pie chart of error types
        wedges, texts, autotexts = ax.pie(
            error_values,
            labels=error_types,
            autopct='%1.1f%%',
            startangle=90,
            colors=['#ff9999', '#66b3ff', '#99ff99'],
            wedgeprops={'edgecolor': 'white', 'linewidth': 1.5}
        )

        # Customize text properties
        for autotext in autotexts:
            autotext.set_fontsize(10)
            autotext.set_fontweight('bold')

        for text in texts:
            text.set_fontsize(12)

        # Equal aspect ratio ensures that pie is drawn as a circle
        ax.axis('equal')

        plt.title('OCR Error Type Distribution', fontsize=16, fontweight='bold')

        # Save the error breakdown chart
        error_breakdown_path = output_path.replace('.png', '_error_breakdown.png')
        plt.savefig(error_breakdown_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"Error breakdown visualization saved to {error_breakdown_path}")

def generate_error_report(results, output_path):
    """
    Generate a detailed error report in HTML format

    Args:
        results: Dictionary with OCR evaluation results
        output_path: Path to save the report
    """
    # Create HTML report
    html_content = f"""
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>OCR Evaluation Report</title>
        <style>
            body {{
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 20px;
                color: #333;
            }}
            .container {{
                max-width: 1200px;
                margin: 0 auto;
            }}
            h1, h2, h3 {{
                color: #2c3e50;
            }}
            h1 {{
                border-bottom: 2px solid #3498db;
                padding-bottom: 10px;
            }}
            table {{
                border-collapse: collapse;
                width: 100%;
                margin: 20px 0;
            }}
            th, td {{
                border: 1px solid #ddd;
                padding: 12px;
                text-align: left;
            }}
            th {{
                background-color: #f2f2f2;
            }}
            tr:nth-child(even) {{
                background-color: #f9f9f9;
            }}
            .metrics {{
                display: flex;
                justify-content: space-between;
                flex-wrap: wrap;
            }}
            .metric-card {{
                background: #fff;
                border-radius: 8px;
                box-shadow: 0 2px 5px rgba(0,0,0,0.1);
                padding: 20px;
                margin: 10px;
                flex: 1;
                min-width: 200px;
            }}
            .metric-value {{
                font-size: 24px;
                font-weight: bold;
                color: #3498db;
            }}
            .error-sample {{
                background: #f8f9fa;
                border-left: 4px solid #e74c3c;
                padding: 15px;
                margin: 20px 0;
            }}
            .highlight {{
                background-color: #ffecb3;
                padding: 2px 4px;
                border-radius: 3px;
            }}
            img {{
                max-width: 100%;
                height: auto;
                border-radius: 5px;
                box-shadow: 0 3px 10px rgba(0,0,0,0.2);
                margin: 20px 0;
            }}
            .footer {{
                margin-top: 50px;
                text-align: center;
                font-size: 12px;
                color: #7f8c8d;
            }}
        </style>
    </head>
    <body>
        <div class="container">
            <h1>OCR Evaluation Report</h1>
            <p><strong>Date:</strong> {time.strftime("%Y-%m-%d %H:%M:%S")}</p>
            <p><strong>Model:</strong> {results.get("model_info", "TrOCR")}</p>

            <h2>Overall Metrics</h2>
            <div class="metrics">
                <div class="metric-card">
                    <h3>Character Error Rate</h3>
                    <div class="metric-value">{results.get("avg_cer", 0):.2%}</div>
                </div>
                <div class="metric-card">
                    <h3>Word Error Rate</h3>
                    <div class="metric-value">{results.get("avg_wer", 0):.2%}</div>
                </div>
                <div class="metric-card">
                    <h3>Total Samples</h3>
                    <div class="metric-value">{results.get("total_samples", 0)}</div>
                </div>
            </div>

            <h2>Results by Document Type</h2>
            <table>
                <tr>
                    <th>Document Type</th>
                    <th>CER</th>
                    <th>WER</th>
                    <th>Samples</th>
                </tr>
    """

    # Add document-specific results
    for doc_type in results.get("cer_by_doc", {}):
        cer = results["cer_by_doc"].get(doc_type, 0)
        wer = results["wer_by_doc"].get(doc_type, 0)
        samples = len(results.get("samples", {}).get(doc_type, []))

        html_content += f"""
                <tr>
                    <td>{doc_type}</td>
                    <td>{cer:.2%}</td>
                    <td>{wer:.2%}</td>
                    <td>{samples}</td>
                </tr>
        """

    html_content += """
            </table>

            <h2>Sample Predictions</h2>
    """

    # Add sample predictions
    for doc_type, samples in results.get("samples", {}).items():
        html_content += f"""
            <h3>{doc_type}</h3>
        """

        for i, sample in enumerate(samples[:2]):  # Limit to 2 samples per doc type
            ref = sample.get("reference", "")
            pred = sample.get("prediction", "")
            cer = sample.get("cer", 0)
            wer = sample.get("wer", 0)

            # Highlight differences (simple approach)
            matcher = difflib.SequenceMatcher(None, ref, pred)
            highlighted_pred = ""
            for tag, i1, i2, j1, j2 in matcher.get_opcodes():
                if tag == 'equal':
                    highlighted_pred += pred[j1:j2]
                elif tag == 'replace':
                    highlighted_pred += f'<span class="highlight">{pred[j1:j2]}</span>'
                elif tag == 'insert':
                    highlighted_pred += f'<span class="highlight">{pred[j1:j2]}</span>'
                elif tag == 'delete':
                    pass  # Skip deletions

            html_content += f"""
                <div class="error-sample">
                    <p><strong>Sample {i+1}</strong> - CER: {cer:.2%}, WER: {wer:.2%}</p>
                    <p><strong>Reference:</strong> {ref[:300]}...</p>
                    <p><strong>Prediction:</strong> {highlighted_pred[:300]}...</p>
                </div>
            """

    # Add error analysis if available
    if "error_analysis" in results:
        html_content += """
            <h2>Error Analysis</h2>
            <h3>Common Character Errors</h3>
            <table>
                <tr>
                    <th>Reference</th>
                    <th>Prediction</th>
                    <th>Count</th>
                </tr>
        """

        for (ref, pred), count in results["error_analysis"].get("top_character_errors", []):
            html_content += f"""
                <tr>
                    <td>'{ref}'</td>
                    <td>'{pred}'</td>
                    <td>{count}</td>
                </tr>
            """

        html_content += """
            </table>

            <h3>Common Word Errors</h3>
            <table>
                <tr>
                    <th>Reference</th>
                    <th>Prediction</th>
                    <th>Count</th>
                </tr>
        """

        for (ref, pred), count in results["error_analysis"].get("top_word_errors", []):
            html_content += f"""
                <tr>
                    <td>{ref}</td>
                    <td>{pred}</td>
                    <td>{count}</td>
                </tr>
            """

        html_content += """
            </table>
        """

    # Add visualizations
    if os.path.exists(output_path):
        img_path = os.path.basename(output_path)
        html_content += f"""
            <h2>Visualizations</h2>
            <img src="{img_path}" alt="OCR Error Rates by Document Type">
        """

        # Add error breakdown visualization if exists
        error_breakdown_path = output_path.replace('.png', '_error_breakdown.png')
        if os.path.exists(error_breakdown_path):
            img_path = os.path.basename(error_breakdown_path)
            html_content += f"""
            <img src="{img_path}" alt="OCR Error Type Distribution">
            """

    # Close HTML
    html_content += """
            <div class="footer">
                <p>Generated using Historical Spanish Document OCR Pipeline with Tesla T4 GPU</p>
            </div>
        </div>
    </body>
    </html>
    """

    # Save HTML report
    report_path = output_path.replace('.png', '_report.html')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(html_content)

    print(f"Error report generated at {report_path}")
    return report_path

"""# cell 9"""

def prepare_dataset_for_trocr(batch, processor, max_length=512):
    """
    Prepare batch data for TrOCR model training with Tesla T4 optimizations

    Args:
        batch: Batch of data from the dataset
        processor: TrOCR processor
        max_length: Maximum sequence length for tokenization

    Returns:
        Processed batch with pixel_values and labels
    """
    # Handle images with proper format conversion
    raw_images = batch["image"]
    processed_images = []

    for img in raw_images:
        # Handle different possible image formats
        if isinstance(img, np.ndarray):
            if img.ndim == 3:  # RGB image
                pil_img = Image.fromarray(img.astype('uint8'))
            elif img.ndim == 2:  # Grayscale image
                pil_img = Image.fromarray(np.repeat(img[:, :, np.newaxis], 3, axis=2).astype('uint8'))
            else:
                pil_img = Image.new('RGB', IMAGE_SIZE, color='white')
        elif isinstance(img, Image.Image):
            pil_img = img
        else:
            pil_img = Image.new('RGB', IMAGE_SIZE, color='white')

        processed_images.append(pil_img)

    # Process the images with the TrOCR processor
    # Use batch_size to avoid OOM errors on Tesla T4
    pixel_values = processor(
        images=processed_images,
        return_tensors="pt"
    ).pixel_values

    # Tokenize the texts with padding and truncation
    labels = processor.tokenizer(
        batch["text"],
        padding="max_length",
        max_length=max_length,
        truncation=True
    ).input_ids

    # Replace padding token id with -100 for cross-entropy ignore_index
    for label_ids in labels:
        label_ids[label_ids == processor.tokenizer.pad_token_id] = -100

    return {
        "pixel_values": pixel_values,
        "labels": labels,
        "texts": batch["text"]  # Keep original texts for evaluation
    }

def convert_dataloader_to_dataset(data_loader, processor, max_samples=None):
    """
    Convert PyTorch DataLoader to HuggingFace Dataset optimized for T4 GPU

    Args:
        data_loader: PyTorch DataLoader
        processor: TrOCR processor
        max_samples: Maximum number of samples to convert (None = all)

    Returns:
        HuggingFace Dataset
    """
    all_data = []
    sample_count = 0

    # Set up a more informative progress bar
    progress_bar = tqdm(
        total=len(data_loader.dataset) if max_samples is None else min(max_samples, len(data_loader.dataset)),
        desc="Converting to HF Dataset"
    )

    for batch in data_loader:
        # Process batch
        for i in range(len(batch["image"])):
            # Stop if we've reached the maximum number of samples
            if max_samples is not None and sample_count >= max_samples:
                break

            # Convert tensor to numpy safely
            if torch.is_tensor(batch["image"][i]):
                # Properly handle tensor dimensions
                if batch["image"][i].dim() == 3 and batch["image"][i].shape[0] == 3:
                    img_np = batch["image"][i].permute(1, 2, 0).numpy()
                else:
                    img_np = batch["image"][i].numpy()

                # Scale to 0-255 if needed
                if img_np.max() <= 1.0:
                    img_np = (img_np * 255).astype('uint8')
            else:
                img_np = batch["image"][i]

            # Add the sample to the dataset
            item = {
                "image": img_np,
                "text": batch["text"][i],
                "image_path": batch["image_path"][i]
            }
            all_data.append(item)
            sample_count += 1
            progress_bar.update(1)

        # Stop if we've reached the maximum number of samples
        if max_samples is not None and sample_count >= max_samples:
            break

    progress_bar.close()

    # Create the dataset, optimized for T4 GPU performance
    hf_dataset = Dataset.from_list(all_data)

    # Pre-process and cache the dataset for faster training
    # This helps reduce CPU-GPU transfer bottlenecks on the T4
    prepare_fn = lambda examples: prepare_dataset_for_trocr(examples, processor)

    # Process in smaller batches to avoid OOM on T4
    processed_dataset = hf_dataset.map(
        prepare_fn,
        batched=True,
        batch_size=8,  # Smaller batch size for processing
        remove_columns=["image", "image_path"],
        desc="Preprocessing for TrOCR"
    )

    print(f"Created dataset with {len(processed_dataset)} samples")
    return processed_dataset

def load_optimized_trocr_model(model_name_or_path="microsoft/trocr-large-handwritten", device=None):
    """
    Load a TrOCR model optimized for Tesla T4 GPU

    Args:
        model_name_or_path: Model name or path to load from
        device: Device to place the model on

    Returns:
        Tuple of (model, processor)
    """
    # Use the default device if none specified
    if device is None:
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    print(f"Loading TrOCR model: {model_name_or_path}")

    # Load processor with optimized settings
    try:
        processor = TrOCRProcessor.from_pretrained(
            model_name_or_path,
            use_auth_token=False,  # Set to True if using private model with HF token
            revision="main",
            use_fast=True,  # Use the faster tokenizer if available
        )
    except Exception as e:
        print(f"Error loading processor: {e}. Trying fallback approach...")
        # Fallback approach: Load tokenizer and feature extractor separately
        tokenizer = AutoTokenizer.from_pretrained(
            model_name_or_path,
            use_fast=True,
            model_max_length=512
        )
        feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)
        processor = TrOCRProcessor(feature_extractor, tokenizer)

    # Load model with optimized settings for Tesla T4
    try:
        # For fine-tuning, load in float32 (will convert to mixed precision later)
        # For inference, directly use float16 for better speed on T4
        model = VisionEncoderDecoderModel.from_pretrained(
            model_name_or_path,
            use_auth_token=False,
            revision="main",
            torch_dtype=torch.float32,  # Start with float32, then use mixed precision
        )

        # Ensure pad_token_id and decoder_start_token_id are set correctly
        model.config.pad_token_id = processor.tokenizer.pad_token_id
        model.config.decoder_start_token_id = processor.tokenizer.bos_token_id

        # Set special token IDs for decoder config as well
        if hasattr(model, 'decoder') and hasattr(model.decoder, 'config'):
            model.decoder.config.pad_token_id = processor.tokenizer.pad_token_id
            model.decoder.config.decoder_start_token_id = processor.tokenizer.bos_token_id

        # Optimize model parameters for Tesla T4 GPU
        model.config.use_cache = True  # Enable KV caching for faster inference
        model.config.output_attentions = False  # Disable attention outputs to save memory
        model.config.output_hidden_states = False  # Disable hidden states outputs to save memory

    except Exception as e:
        print(f"Error loading model: {e}. Using default configuration...")

        # Fallback configuration for T4 GPU
        encoder_config = AutoConfig.from_pretrained("microsoft/trocr-base-handwritten", num_hidden_layers=6)
        decoder_config = AutoConfig.from_pretrained("microsoft/trocr-base-handwritten", num_hidden_layers=6)

        config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(
            encoder_config,
            decoder_config
        )

        config.pad_token_id = processor.tokenizer.pad_token_id
        config.decoder_start_token_id = processor.tokenizer.bos_token_id
        config.use_cache = True

        model = VisionEncoderDecoderModel(config)

    # Move model to device and apply half precision for T4 GPU efficiency
    model.to(device)

    # Configure generation parameters optimized for T4 GPU
    model.config.max_length = 128  # Set maximum generation length
    model.config.early_stopping = True  # Enable early stopping
    model.config.no_repeat_ngram_size = 2  # Avoid repeating bigrams
    model.config.length_penalty = 1.0  # Balanced length penalty
    model.config.num_beams = 4  # Beam search with 4 beams for reasonable speed/quality trade-off

    # Print model size info
    model_size = sum(p.numel() for p in model.parameters())
    print(f"Model size: {model_size/1e6:.2f}M parameters")

    return model, processor

def setup_training_environment(model, train_dataset, output_dir, num_epochs=3, batch_size=8, learning_rate=5e-5):
    """
    Set up training environment with T4 GPU optimizations

    Args:
        model: The TrOCR model
        train_dataset: The training dataset
        output_dir: Directory to save checkpoints and outputs
        num_epochs: Number of training epochs
        batch_size: Batch size for training
        learning_rate: Learning rate

    Returns:
        Dictionary with training environment objects
    """
    # Initialize accelerator with mixed precision for T4 GPU
    accelerator = Accelerator(
        mixed_precision="fp16",  # Using FP16 for T4 GPU efficiency
        gradient_accumulation_steps=4,  # Accumulate gradients for effective larger batch
        log_with="tensorboard",  # Use TensorBoard for logging
        project_dir=os.path.join(output_dir, "logs")
    )

    # Set device based on accelerator
    device = accelerator.device

    # Prepare optimizer with AdamW (same algorithm as original implementation)
    # but with weight decay and learning rate tuned for T4 GPU
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        weight_decay=0.05,  # Higher weight decay for regularization
        eps=1e-8,  # Stability term
        betas=(0.9, 0.999)  # Adam betas
    )

    # Create a cosine learning rate scheduler with warmup
    # This scheduler helps with training stability and convergence
    lr_scheduler = get_scheduler(
        name="cosine",  # Cosine with warmup works well for vision-language models
        optimizer=optimizer,
        num_warmup_steps=int(0.1 * (len(train_dataset) // batch_size) * num_epochs),
        num_training_steps=(len(train_dataset) // batch_size) * num_epochs,
    )

    # Set up gradient scaler for mixed precision training
    scaler = GradScaler()

    # Create the training arguments for Seq2Seq training
    training_args = {
        "output_dir": output_dir,
        "per_device_train_batch_size": batch_size,
        "per_device_eval_batch_size": batch_size * 2,  # Larger eval batch size
        "num_train_epochs": num_epochs,
        "logging_dir": os.path.join(output_dir, "logs"),
        "logging_steps": 100,
        "save_steps": len(train_dataset) // batch_size,  # Save every epoch
        "learning_rate": learning_rate,
        "weight_decay": 0.05,
        "fp16": True,  # Use FP16 for T4 GPU
        "gradient_accumulation_steps": 4,  # Accumulate gradients
        "max_grad_norm": 1.0,  # Gradient clipping
    }

    print(f"Training on device: {device}")
    print(f"Mixed precision: {accelerator.mixed_precision}")
    print(f"Batch size: {batch_size} (effective: {batch_size * 4} with gradient accumulation)")

    return {
        "accelerator": accelerator,
        "optimizer": optimizer,
        "scheduler": lr_scheduler,
        "scaler": scaler,
        "device": device,
        "training_args": training_args
    }

def train_epoch(model, train_dataloader, optimizer, scheduler, device, scaler, accelerator, epoch):
    """
    Train for one epoch with T4 GPU optimizations

    Args:
        model: The TrOCR model
        train_dataloader: Training data loader
        optimizer: Optimizer
        scheduler: Learning rate scheduler
        device: Device to train on
        scaler: Gradient scaler for mixed precision
        accelerator: HuggingFace Accelerator
        epoch: Current epoch number

    Returns:
        Average training loss for the epoch
    """
    model.train()
    total_loss = 0

    # Initialize progress bar
    progress_bar = tqdm(
        total=len(train_dataloader),
        desc=f"Epoch {epoch+1}",
        disable=not accelerator.is_local_main_process
    )

    # Track GPU memory and time
    start_time = time.time()
    print_gpu_memory_usage()

    for step, batch in enumerate(train_dataloader):
        # Move batch to device
        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}

        # Clear gradients
        optimizer.zero_grad()

        # Forward pass with mixed precision
        with autocast():
            outputs = model(
                pixel_values=batch["pixel_values"],
                labels=batch["labels"]
            )
            loss = outputs.loss

        # Scale loss and perform backward pass
        scaler.scale(loss).backward()

        # Clip gradients to prevent exploding gradients
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # Update weights with scaler
        scaler.step(optimizer)
        scaler.update()

        # Update learning rate
        scheduler.step()

        # Update progress bar
        total_loss += loss.item()
        progress_bar.update(1)
        progress_bar.set_postfix({"loss": loss.item()})

        # Free up memory by explicitly clearing GPU cache periodically
        if step % 100 == 0 and step > 0:
            # Output memory usage every 100 steps
            if accelerator.is_local_main_process:
                print_gpu_memory_usage()

            # Clear cache to free up memory (if manually handling)
            if not accelerator.use_fp16:
                torch.cuda.empty_cache()

    progress_bar.close()

    # Report training statistics
    avg_loss = total_loss / len(train_dataloader)
    elapsed_time = time.time() - start_time

    print(f"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}, Time: {elapsed_time:.2f}s")
    print_gpu_memory_usage()

    return avg_loss

def evaluate_model(model, eval_dataloader, processor, device, post_processor=None):
    """
    Evaluate the TrOCR model with memory-efficient inference for T4 GPU

    Args:
        model: The TrOCR model
        eval_dataloader: Evaluation data loader
        processor: TrOCR processor
        device: Device to evaluate on
        post_processor: Optional post-processor for predictions

    Returns:
        Dictionary with evaluation results
    """
    model.eval()

    # Initialize metrics
    all_predictions = []
    all_references = []
    all_image_paths = []
    all_cer_scores = []
    all_wer_scores = []

    # Process in batches with no_grad for memory efficiency
    progress_bar = tqdm(total=len(eval_dataloader), desc="Evaluating")

    with torch.no_grad():
        for batch in eval_dataloader:
            # Move batch to device
            pixel_values = batch["image"].to(device)
            references = batch["text"]
            image_paths = batch["image_path"]

            # Generate predictions with memory-efficient settings
            generated_ids = model.generate(
                processor(pixel_values, return_tensors="pt").pixel_values.to(device),
                max_length=128,
                num_beams=4,  # Balance quality and speed
                early_stopping=True,
                no_repeat_ngram_size=2,
                length_penalty=1.0,
                use_cache=True,  # Use KV caching for faster inference
            )

            # Decode predictions
            predictions = processor.batch_decode(generated_ids, skip_special_tokens=True)

            # Apply post-processing if provided
            if post_processor:
                predictions = [post_processor.process_text(p) for p in predictions]

            # Calculate metrics
            for i, (pred, ref) in enumerate(zip(predictions, references)):
                all_predictions.append(pred)
                all_references.append(ref)
                all_image_paths.append(image_paths[i])

                # Calculate error metrics
                cer = character_error_rate(ref, pred)
                wer = word_error_rate(ref, pred)

                all_cer_scores.append(cer)
                all_wer_scores.append(wer)

            progress_bar.update(1)

    progress_bar.close()

    # Group results by document type
    doc_cer = {}
    doc_wer = {}
    doc_samples = {}

    for i, image_path in enumerate(all_image_paths):
        doc_id = image_path.split(os.sep)[-2]  # Extract document ID from path

        # Get document type from ID
        doc_type = "unknown"
        for type_name in ["Buendia", "Mendo", "Ezcaray", "Constituciones", "Paredes"]:
            if type_name in doc_id:
                doc_type = type_name
                break

        # Initialize document type entries if needed
        if doc_type not in doc_cer:
            doc_cer[doc_type] = []
            doc_wer[doc_type] = []
            doc_samples[doc_type] = []

        # Add metrics
        doc_cer[doc_type].append(all_cer_scores[i])
        doc_wer[doc_type].append(all_wer_scores[i])

        # Add sample if we don't have enough yet
        if len(doc_samples[doc_type]) < 2:
            doc_samples[doc_type].append({
                "reference": all_references[i],
                "prediction": all_predictions[i],
                "cer": all_cer_scores[i],
                "wer": all_wer_scores[i],
                "image_path": all_image_paths[i]
            })

    # Calculate average metrics
    avg_cer = sum(all_cer_scores) / len(all_cer_scores) if all_cer_scores else 0
    avg_wer = sum(all_wer_scores) / len(all_wer_scores) if all_wer_scores else 0

    # Calculate average metrics by document type
    avg_cer_by_doc = {doc: sum(scores) / len(scores) if scores else 0
                      for doc, scores in doc_cer.items()}
    avg_wer_by_doc = {doc: sum(scores) / len(scores) if scores else 0
                      for doc, scores in doc_wer.items()}

    # Perform error analysis
    if all_predictions and all_references:
        # Select a representative sample for analysis
        sample_idx = len(all_references) // 2  # Middle sample
        error_analysis = analyze_error_patterns(
            all_references[sample_idx],
            all_predictions[sample_idx]
        )

        # Calculate error rates
        error_analysis["substitution_rate"] = error_analysis["substitution_count"] / error_analysis["total_characters"]
        error_analysis["insertion_rate"] = error_analysis["insertion_count"] / error_analysis["total_characters"]
        error_analysis["deletion_rate"] = error_analysis["deletion_count"] / error_analysis["total_characters"]
    else:
        error_analysis = {}

    # Create results dictionary
    results = {
        "avg_cer": avg_cer,
        "avg_wer": avg_wer,
        "cer_by_doc": avg_cer_by_doc,
        "wer_by_doc": avg_wer_by_doc,
        "samples": doc_samples,
        "error_analysis": error_analysis,
        "total_samples": len(all_predictions),
        "model_info": getattr(model, "name_or_path", "TrOCR"),
    }

    # Log results
    print(f"Average CER: {avg_cer:.4f}")
    print(f"Average WER: {avg_wer:.4f}")

    for doc_type, cer in avg_cer_by_doc.items():
        print(f"{doc_type:<15} - CER: {cer:.4f}, WER: {avg_wer_by_doc[doc_type]:.4f}")

    return results

def fine_tune_trocr_model(train_loader, val_loader, output_dir, num_epochs=3, learning_rate=3e-5):
    """
    Fine-tune a TrOCR model on custom data with T4 GPU optimizations

    Args:
        train_loader: Training data loader
        val_loader: Validation data loader
        output_dir: Directory to save the fine-tuned model
        num_epochs: Number of training epochs
        learning_rate: Learning rate for fine-tuning

    Returns:
        Tuple of (model, processor, results)
    """
    # Create the output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Determine the device
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    # Load the TrOCR model and processor with T4 optimizations
    model, processor = load_optimized_trocr_model(
        model_name_or_path=OCR_MODELS["handwritten"],
        device=device
    )

    # Convert PyTorch DataLoaders to HuggingFace Datasets
    print("\nPreparing datasets for fine-tuning...")
    train_dataset = convert_dataloader_to_dataset(train_loader, processor, max_samples=None)
    val_dataset = convert_dataloader_to_dataset(val_loader, processor, max_samples=None)

    # Set an optimal batch size for T4 GPU
    # We'll use a smaller batch size but with gradient accumulation
    batch_size = 4  # T4 8GB memory optimized batch size

    # Set up training environment
    training_env = setup_training_environment(
        model=model,
        train_dataset=train_dataset,
        output_dir=output_dir,
        num_epochs=num_epochs,
        batch_size=batch_size,
        learning_rate=learning_rate
    )

    # Extract components from training environment
    accelerator = training_env["accelerator"]
    optimizer = training_env["optimizer"]
    scheduler = training_env["scheduler"]
    scaler = training_env["scaler"]

    # Create data loaders for training
    train_dataloader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=2,
        pin_memory=True
    )

    # Prepare model and optimizer with accelerator
    model, optimizer, train_dataloader, scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, scheduler
    )

    # Training loop
    print(f"\nFine-tuning TrOCR model for {num_epochs} epochs...")
    best_cer = float('inf')
    best_model_path = None

    for epoch in range(num_epochs):
        # Train one epoch
        train_loss = train_epoch(
            model=model,
            train_dataloader=train_dataloader,
            optimizer=optimizer,
            scheduler=scheduler,
            device=device,
            scaler=scaler,
            accelerator=accelerator,
            epoch=epoch
        )

        # Evaluate on validation set
        print(f"\nEvaluating after epoch {epoch+1}...")
        eval_results = evaluate_model(
            model=accelerator.unwrap_model(model),  # Unwrap for evaluation
            eval_dataloader=val_loader,
            processor=processor,
            device=device
        )

        # Save the model if it's the best so far
        epoch_cer = eval_results["avg_cer"]
        if epoch_cer < best_cer:
            best_cer = epoch_cer
            epoch_dir = os.path.join(output_dir, f"epoch_{epoch+1}")
            os.makedirs(epoch_dir, exist_ok=True)

            # Save unwrapped model
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(epoch_dir)
            processor.save_pretrained(epoch_dir)

            # Save evaluation results
            with open(os.path.join(epoch_dir, "eval_results.json"), 'w') as f:
                import json
                json.dump(
                    {k: v for k, v in eval_results.items() if k != 'samples'},
                    f,
                    indent=2
                )

            best_model_path = epoch_dir
            print(f"New best model saved to {epoch_dir} (CER: {best_cer:.4f})")

            # Create visualization
            viz_path = os.path.join(epoch_dir, "error_rates_by_doc.png")
            visualize_ocr_results(eval_results, viz_path)

            # Generate detailed report
            generate_error_report(eval_results, viz_path)

    # Load the best model
    if best_model_path:
        print(f"\nLoading best model from {best_model_path}...")
        model = VisionEncoderDecoderModel.from_pretrained(best_model_path)
        model.to(device)

    # Final path for the fine-tuned model
    final_path = os.path.join(output_dir, "final")
    os.makedirs(final_path, exist_ok=True)

    # Save the final model
    model.save_pretrained(final_path)
    processor.save_pretrained(final_path)
    print(f"Final model saved to {final_path}")

    # Run a final evaluation
    final_results = evaluate_model(
        model=model,
        eval_dataloader=val_loader,
        processor=processor,
        device=device
    )

    # Create visualization for final results
    final_viz_path = os.path.join(final_path, "error_rates_by_doc.png")
    visualize_ocr_results(final_results, final_viz_path)

    # Generate detailed report for final results
    generate_error_report(final_results, final_viz_path)

    return model, processor, final_results

def run_ocr_evaluation(val_loader, transcriptions, output_dir, model_name=None):
    """
    Run OCR evaluation with memory-efficient inference for T4 GPU

    Args:
        val_loader: Validation data loader
        transcriptions: Dictionary mapping image paths to transcriptions
        output_dir: Directory to save evaluation results
        model_name: Optional model name to use (default: None = use handwritten model)

    Returns:
        Evaluation results
    """
    # Create the output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Determine the device
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    # Select the model to use
    if not model_name:
        # Use handwritten model by default
        model_name = OCR_MODELS["handwritten"]

    print(f"Starting OCR evaluation using model: {model_name}")

    # Load the TrOCR model and processor with T4 optimizations
    model, processor = load_optimized_trocr_model(model_name, device)

    # Create lexicon for post-processing
    print("\nCreating lexicon for post-processing...")
    lexicon = create_lexicon_from_transcriptions(transcriptions)
    lexicon = augment_lexicon_with_variations(lexicon)

    # Create post-processor
    post_processor = EnhancedSpanishHistoricalPostProcessor(lexicon)

    # Adjust model for inference
    if device.type == 'cuda':
        # For T4 GPU, use mixed precision (FP16) for inference
        model = model.half()  # Convert to half precision
        print("Using FP16 for inference")

    # Run evaluation
    print("\nEvaluating OCR model...")
    results = evaluate_model(
        model=model,
        eval_dataloader=val_loader,
        processor=processor,
        device=device,
        post_processor=post_processor
    )

    # Save results
    results_file = os.path.join(output_dir, "evaluation_results.json")
    with open(results_file, 'w') as f:
        import json
        json.dump(
            {k: v for k, v in results.items() if k != 'samples'},
            f,
            indent=2
        )

    # Create visualization
    viz_path = os.path.join(output_dir, "error_rates_by_doc.png")
    visualize_ocr_results(results, viz_path)

    # Generate detailed report
    report_path = generate_error_report(results, viz_path)

    print(f"\nEvaluation results saved to {results_file}")
    print(f"Visualization saved to {viz_path}")
    print(f"Detailed report saved to {report_path}")

    return results

"""# Cell 10"""

def run_full_ocr_pipeline(max_pages_per_pdf=8, evaluate_model=True, fine_tune=False, num_epochs=5, learning_rate=3e-5):
    """
    Run the complete OCR pipeline with Tesla T4 GPU optimizations

    Args:
        max_pages_per_pdf: Maximum number of pages to process per PDF
        evaluate_model: Whether to evaluate the pre-trained model
        fine_tune: Whether to fine-tune the model
        num_epochs: Number of epochs for fine-tuning
        learning_rate: Learning rate for fine-tuning

    Returns:
        Dictionary with pipeline results
    """
    start_time = time.time()

    print("=" * 80)
    print("GPU-Optimized Historical Spanish Document OCR Pipeline")
    print("=" * 80)
    print(f"\nUsing device: {device}")
    print_gpu_memory_usage()

    print("\nStep 1: Upload PDFs")
    uploaded_paths = upload_pdfs_to_colab()

    if not uploaded_paths:
        print("No PDFs uploaded. Please upload at least one PDF file.")
        return None

    print(f"\nUploaded {len(uploaded_paths)} PDFs")

    print("\nStep 2: Convert PDFs to Images")
    document_images = process_all_pdfs(
        pdf_folder,
        output_base_path,
        dpi=DEFAULT_DPI,
        max_pages_per_pdf=max_pages_per_pdf
    )

    # Check if we got any processed images
    total_images = sum(len(paths) for paths in document_images.values())

    if total_images == 0:
        print("\nNo images were extracted from PDFs. Exiting.")
        return None

    print(f"\nExtracted {total_images} images from {len(document_images)} documents")

    print("\nStep 3: Preprocess Images")
    processed_document_images = preprocess_document_images(document_images, output_base_path)

    # Check total processed images
    total_processed = sum(len(paths) for paths in processed_document_images.values())

    if total_processed == 0:
        print("\nNo images were successfully preprocessed. Exiting.")
        return None

    print(f"\nSuccessfully preprocessed {total_processed} images from {len(processed_document_images)} documents")

    print("\nStep 4: Creating Transcriptions")
    create_rich_transcriptions(processed_document_images, transcriptions_path)

    # Collect all processed image paths
    print("\nStep 5: Collecting Processed Image Paths")
    all_processed_images = []
    for doc_id, image_paths in processed_document_images.items():
        all_processed_images.extend(image_paths)

    print(f"Collected {len(all_processed_images)} processed images")

    # Load transcriptions
    print("\nStep 6: Loading Transcriptions")
    transcriptions = load_transcriptions(transcriptions_path, all_processed_images)

    # Create train-validation-test split
    print("\nStep 7: Creating Data Splits")
    train_image_paths, val_image_paths, test_image_paths = create_train_val_test_split(
        all_processed_images,
        transcriptions,
        test_ratio=0.2,
        val_ratio=0.1
    )

    print(f"Train set: {len(train_image_paths)} images")
    print(f"Validation set: {len(val_image_paths)} images")
    print(f"Test set: {len(test_image_paths)} images")

    # Create data loaders
    print("\nStep 8: Creating Data Loaders")
    batch_size = 8 if device.type == 'cuda' else 4  # Adjusted batch size for GPU

    train_loader, val_loader, test_loader = create_data_loaders(
        train_image_paths,
        val_image_paths,
        test_image_paths,
        transcriptions,
        batch_size=batch_size
    )

    # Show example images
    print("\nStep 9: Showing Example Images")
    save_example_images(processed_document_images, output_base_path, num_examples=2)

    # Store initial results
    results = {
        "document_images": processed_document_images,
        "transcriptions": transcriptions,
        "train_loader": train_loader,
        "val_loader": val_loader,
        "test_loader": test_loader,
        "train_image_paths": train_image_paths,
        "val_image_paths": val_image_paths,
        "test_image_paths": test_image_paths,
        "processing_time": {}
    }

    # Checkpoint 1: Save initial results
    checkpoint_path = os.path.join(results_path, "pipeline_checkpoint_1.pt")
    torch.save({
        "train_paths": train_image_paths,
        "val_paths": val_image_paths,
        "test_paths": test_image_paths
    }, checkpoint_path)

    print(f"Data preparation checkpoint saved to {checkpoint_path}")

    # Evaluate model if requested
    if evaluate_model:
        print("\nStep 10: Evaluating Pre-trained OCR Model")
        eval_start_time = time.time()

        eval_results = run_ocr_evaluation(
            val_loader=val_loader,
            transcriptions=transcriptions,
            output_dir=results_path
        )

        eval_time = time.time() - eval_start_time
        results["evaluation_results"] = eval_results
        results["processing_time"]["evaluation"] = eval_time

        print(f"Model evaluation completed in {eval_time:.2f} seconds")

    # Fine-tune model if requested
    if fine_tune:
        print("\nStep 11: Fine-tuning OCR Model")
        ft_start_time = time.time()

        model, processor, ft_results = fine_tune_trocr_model(
            train_loader=train_loader,
            val_loader=val_loader,
            output_dir=os.path.join(output_base_path, "fine_tuned_model"),
            num_epochs=num_epochs,
            learning_rate=learning_rate
        )

        ft_time = time.time() - ft_start_time
        results["fine_tuned_model"] = model
        results["fine_tuned_processor"] = processor
        results["fine_tuning_results"] = ft_results
        results["processing_time"]["fine_tuning"] = ft_time

        print(f"Model fine-tuning completed in {ft_time:.2f} seconds")

        # Evaluate the fine-tuned model on the test set
        print("\nStep 12: Evaluating Fine-tuned Model on Test Set")
        test_start_time = time.time()

        test_results = evaluate_model(
            model=model,
            eval_dataloader=test_loader,
            processor=processor,
            device=device
        )

        test_time = time.time() - test_start_time
        results["test_results"] = test_results
        results["processing_time"]["testing"] = test_time

        # Save test results visualization
        test_viz_path = os.path.join(output_base_path, "fine_tuned_model", "test_results.png")
        visualize_ocr_results(test_results, test_viz_path)

        # Generate detailed test report
        generate_error_report(test_results, test_viz_path)

        print(f"Test evaluation completed in {test_time:.2f} seconds")

    # Calculate total processing time
    total_time = time.time() - start_time
    results["processing_time"]["total"] = total_time

    print("\n" + "=" * 80)
    print(f"OCR pipeline completed successfully in {total_time:.2f} seconds!")
    print("=" * 80)

    # Print results summary
    print("\nResults Summary:")
    if "evaluation_results" in results:
        print(f"Pre-trained Model - Avg CER: {results['evaluation_results']['avg_cer']:.2%}, "
              f"Avg WER: {results['evaluation_results']['avg_wer']:.2%}")

    if "fine_tuning_results" in results:
        print(f"Fine-tuned Model - Avg CER: {results['fine_tuning_results']['avg_cer']:.2%}, "
              f"Avg WER: {results['fine_tuning_results']['avg_wer']:.2%}")

    if "test_results" in results:
        print(f"Test Set Results - Avg CER: {results['test_results']['avg_cer']:.2%}, "
              f"Avg WER: {results['test_results']['avg_wer']:.2%}")

    # Output paths for results
    print(f"\nResults saved in: {results_path}")
    if fine_tune:
        print(f"Fine-tuned model saved in: {os.path.join(output_base_path, 'fine_tuned_model')}")

    return results

def run_inference_on_new_document(pdf_path, model_path=None, output_dir=None):
    """
    Run inference on a new document using the trained model

    Args:
        pdf_path: Path to the PDF file
        model_path: Path to the fine-tuned model (None = use pre-trained)
        output_dir: Output directory for results

    Returns:
        Dictionary with OCR results
    """
    if output_dir is None:
        output_dir = os.path.join(output_base_path, "inference_results")

    os.makedirs(output_dir, exist_ok=True)

    # Step 1: Convert PDF to images
    print(f"Processing PDF: {os.path.basename(pdf_path)}")
    pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]

    images_folder = os.path.join(output_dir, "images", pdf_filename)
    processed_folder = os.path.join(output_dir, "processed_images", pdf_filename)
    binary_folder = os.path.join(output_dir, "binary_images", pdf_filename)
    enhanced_folder = os.path.join(output_dir, "enhanced_images", pdf_filename)

    os.makedirs(images_folder, exist_ok=True)
    os.makedirs(processed_folder, exist_ok=True)
    os.makedirs(binary_folder, exist_ok=True)
    os.makedirs(enhanced_folder, exist_ok=True)

    # Convert PDF to images
    image_paths = convert_pdf_to_images(
        pdf_path,
        images_folder,
        dpi=DEFAULT_DPI
    )

    if not image_paths:
        print(f"Failed to extract images from {pdf_filename}")
        return None

    print(f"Extracted {len(image_paths)} images from PDF")

    # Step 2: Preprocess images
    processed_paths = []
    for img_path in tqdm(image_paths, desc="Preprocessing images"):
        try:
            processed_path, _, _ = preprocess_image(
                img_path,
                processed_folder,
                binary_folder,
                enhanced_folder
            )

            if processed_path:
                processed_paths.append(processed_path)

        except Exception as e:
            print(f"Error preprocessing {img_path}: {str(e)}")

    if not processed_paths:
        print("Failed to preprocess any images")
        return None

    print(f"Preprocessed {len(processed_paths)} images")

    # Step 3: Load model and processor
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    if model_path:
        print(f"Loading fine-tuned model from {model_path}")
        model, processor = load_optimized_trocr_model(model_path, device)
    else:
        print("Using pre-trained model")
        model, processor = load_optimized_trocr_model(OCR_MODELS["handwritten"], device)

    # For inference on T4 GPU, use half precision
    if device.type == 'cuda':
        model = model.half()

    # Step 4: Create a simple dataset for inference
    class InferenceDataset(Dataset):
        def __init__(self, image_paths, transform=None):
            self.image_paths = image_paths
            self.transform = transform

        def __len__(self):
            return len(self.image_paths)

        def __getitem__(self, idx):
            image_path = self.image_paths[idx]
            image = Image.open(image_path).convert('RGB')

            if self.transform:
                image = self.transform(image)

            return {"image": image, "image_path": image_path}

    # Create transform and dataset
    transform = transforms.Compose([
        transforms.Resize(IMAGE_SIZE),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    inference_dataset = InferenceDataset(processed_paths, transform)

    # Create data loader with optimal batch size for T4 GPU
    batch_size = 8 if device.type == 'cuda' else 2
    inference_loader = DataLoader(
        inference_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2 if device.type == 'cuda' else 0,
        pin_memory=device.type == 'cuda'
    )

    # Step 5: Run inference
    model.eval()

    results = {
        "text_by_page": {},
        "confidence_by_page": {},
        "combined_text": ""
    }

    page_texts = []

    with torch.no_grad():
        for batch in tqdm(inference_loader, desc="Running OCR"):
            # Move batch to device
            images = batch["image"].to(device)
            paths = batch["image_path"]

            # Generate predictions with memory-efficient settings
            generated_ids = model.generate(
                processor(images=images, return_tensors="pt").pixel_values.to(device),
                max_length=128,
                num_beams=4,
                early_stopping=True,
                no_repeat_ngram_size=2,
                length_penalty=1.0,
                use_cache=True,
                output_scores=True,
                return_dict_in_generate=True
            )

            # Decode predictions
            predictions = processor.batch_decode(
                generated_ids.sequences,
                skip_special_tokens=True
            )

            # Calculate confidence scores (mean log-probability of tokens)
            confidences = []
            for i in range(len(predictions)):
                # Get token scores for this sequence
                scores = generated_ids.scores
                sequence = generated_ids.sequences[i]

                # Calculate mean log probability
                log_probs = []
                for j, token_id in enumerate(sequence):
                    if j < len(scores) and j > 0:  # Skip first token (BOS)
                        token_scores = scores[j-1]
                        if i < len(token_scores):
                            token_prob = torch.nn.functional.softmax(token_scores[i], dim=-1)
                            token_id_prob = token_prob[token_id]
                            log_probs.append(torch.log(token_id_prob + 1e-9))

                if log_probs:
                    mean_log_prob = torch.mean(torch.stack(log_probs)).item()
                    confidence = np.exp(mean_log_prob)
                else:
                    confidence = 0.0

                confidences.append(confidence)

            # Store results by page
            for i, (path, text, confidence) in enumerate(zip(paths, predictions, confidences)):
                page_num = int(re.search(r'page_(\d+)', path).group(1))
                results["text_by_page"][page_num] = text
                results["confidence_by_page"][page_num] = confidence

                # Add to page texts in order
                page_texts.append((page_num, text))

    # Sort page texts by page number
    page_texts.sort(key=lambda x: x[0])

    # Combine all text in order
    results["combined_text"] = "\n\n".join([text for _, text in page_texts])

    # Calculate average confidence
    if results["confidence_by_page"]:
        results["average_confidence"] = np.mean(list(results["confidence_by_page"].values()))
    else:
        results["average_confidence"] = 0.0

    # Step 6: Save results
    output_text_file = os.path.join(output_dir, f"{pdf_filename}_ocr.txt")
    with open(output_text_file, 'w', encoding='utf-8') as f:
        f.write(results["combined_text"])

    # Save results as JSON
    output_json_file = os.path.join(output_dir, f"{pdf_filename}_results.json")
    with open(output_json_file, 'w', encoding='utf-8') as f:
        import json
        json.dump(
            {
                "average_confidence": results["average_confidence"],
                "confidence_by_page": results["confidence_by_page"],
                "page_count": len(results["text_by_page"])
            },
            f,
            indent=2
        )

    print(f"\nOCR completed for {pdf_filename}")
    print(f"Processed {len(processed_paths)} pages with average confidence: {results['average_confidence']:.2%}")
    print(f"Results saved to:\n - {output_text_file}\n - {output_json_file}")

    return results

"""# Cell 11"""

# Run the complete OCR pipeline with Tesla T4 GPU optimizations
# You can customize these parameters based on your requirements

# Use more pages for T4 GPU (it has enough memory)
max_pages = 6  # Process up to 8 pages per PDF

# Set workflow controls
evaluate = True   # Evaluate the pre-trained model
fine_tune = True  # Fine-tune the model

# Fine-tuning parameters - optimized for Tesla T4 GPU
num_epochs = 5    # Train for 5 epochs (increase for better results)
learning_rate = 3e-5  # T4-optimized learning rate

# Execute the pipeline with GPU optimizations
results = run_full_ocr_pipeline(
    max_pages_per_pdf=max_pages,
    evaluate_model=evaluate,
    fine_tune=fine_tune,
    num_epochs=num_epochs,
    learning_rate=learning_rate
)

if results:
    print("\nPipeline completed successfully!")

    # Print GPU memory usage after pipeline runs
    print_gpu_memory_usage()

    # List available output files
    print("\nGenerated Files:")
    for root, dirs, files in os.walk(results_path):
        for file in files:
            if file.endswith('.png') or file.endswith('.html') or file.endswith('.json'):
                print(f" - {os.path.join(root, file)}")

    # If fine-tuned, show the model directory
    if fine_tune:
        fine_tuned_model_path = os.path.join(output_base_path, "fine_tuned_model", "final")
        print(f"\nFine-tuned model is saved in: {fine_tuned_model_path}")
        print("Use this model path for inference on new documents.")
else:
    print("\nPipeline did not complete successfully. Please check the errors above.")

"""# Cell 12"""

# Process a new document with the fine-tuned model

# First, upload a new PDF document
print("Please upload a new PDF document for OCR processing:")
new_pdf = files.upload()

if not new_pdf:
    print("No PDF uploaded. Please run the cell again and upload a PDF.")
else:
    # Get the uploaded file path
    pdf_path = list(new_pdf.keys())[0]
    pdf_file_path = os.path.join(pdf_folder, pdf_path)

    # Save the uploaded file
    with open(pdf_file_path, 'wb') as f:
        f.write(new_pdf[pdf_path])

    print(f"Processing {pdf_path}...")

    # Check if we have a fine-tuned model available
    fine_tuned_model_path = os.path.join(output_base_path, "fine_tuned_model", "final")

    if os.path.exists(fine_tuned_model_path):
        print(f"Using fine-tuned model from: {fine_tuned_model_path}")
        model_path = fine_tuned_model_path
    else:
        print("No fine-tuned model found. Using pre-trained model.")
        model_path = None

    # Create output directory for inference
    inference_dir = os.path.join(output_base_path, "inference_results")
    os.makedirs(inference_dir, exist_ok=True)

    # Run inference
    inference_results = run_inference_on_new_document(
        pdf_file_path,
        model_path=model_path,
        output_dir=inference_dir
    )

    if inference_results:
        # Display a sample of the OCR text
        print("\nOCR Text Sample:")
        text_sample = inference_results["combined_text"][:500] + "..." if len(inference_results["combined_text"]) > 500 else inference_results["combined_text"]
        print(text_sample)

        # Output file paths
        pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]
        output_text_file = os.path.join(inference_dir, f"{pdf_filename}_ocr.txt")

        print(f"\nFull OCR text saved to: {output_text_file}")
        print(f"View the file to see complete OCR results")
    else:
        print("Inference failed. Please check the errors above.")

"""# Cell 13"""

# Tips for maximizing Tesla T4 GPU performance in OCR tasks

print("=" * 80)
print("Tesla T4 GPU Optimization Tips for Historical Document OCR")
print("=" * 80)

# Check if we're actually using a T4 GPU
if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    if "T4" in gpu_name:
        print(f"\nConfirmed: Using Tesla T4 GPU")
    else:
        print(f"\nNote: Using GPU {gpu_name} (not Tesla T4)")
else:
    print("\nWarning: No GPU detected. These tips are specific to Tesla T4 GPUs.")

print("\n1. Memory Management")
print("   - The Tesla T4 has 16GB memory (Colab provides access to ~12-15GB)")
print("   - For larger documents, process them in smaller batches")
print("   - Use mixed precision (FP16) instead of FP32 for 2-3x speedup")
print("   - Set batch_size=8 with gradient accumulation for optimal performance")
print("   - Use torch.cuda.empty_cache() regularly during processing")

print("\n2. Model Selection")
print("   - For fastest inference, use microsoft/trocr-base-handwritten")
print("   - For higher accuracy, use microsoft/trocr-large-handwritten (slower)")
print("   - Custom fine-tuned models provide the best balance of speed/accuracy")
print("   - When fine-tuning, start from a pre-trained model to save time")

print("\n3. Image Processing")
print("   - Optimal image size: 384x384 to 512x512 for T4 GPU")
print("   - Use higher DPI (400) during PDF conversion with T4 GPU")
print("   - Process max 8-10 pages at once for best T4 memory utilization")
print("   - For best results, use the enhanced preprocessing pipeline")

print("\n4. Fine-tuning Parameters")
print("   - Optimal batch size: 4-8 with gradient accumulation_steps=4")
print("   - Learning rate: 3e-5 with cosine scheduler and warmup")
print("   - Training epochs: 5-10 is sufficient for good results")
print("   - Use mixed precision training (fp16) for 2-3x speedup")

print("\n5. Inference Optimization")
print("   - Use half precision (FP16) for 2-3x faster inference")
print("   - Set num_beams=4 for good quality/speed trade-off")
print("   - Enable use_cache=True for faster generation")
print("   - Process documents in batches of 8-16 for highest throughput")

# Check if we can monitor GPU performance live
try:
    # Print current GPU utilization
    !nvidia-smi
    print("\nUse '!nvidia-smi' to monitor GPU utilization")
except:
    print("\nCannot access nvidia-smi in this environment")

# Memory usage comparison
print("\nMemory Usage Comparison (approximate):")
print("┌────────────────────────┬───────────────┐")
print("│ Operation              │ Memory (T4)   │")
print("├────────────────────────┼───────────────┤")
print("│ Base model (FP32)      │ ~500 MB       │")
print("│ Large model (FP32)     │ ~1.5 GB       │")
print("│ Base model (FP16)      │ ~250 MB       │")
print("│ Large model (FP16)     │ ~750 MB       │")
print("│ 512x512 image batch=8  │ ~100 MB       │")
print("│ Full pipeline overhead │ ~2-3 GB       │")
print("└────────────────────────┴───────────────┘")

# Current memory usage
print_gpu_memory_usage()