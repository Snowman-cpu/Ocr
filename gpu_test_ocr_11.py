# -*- coding: utf-8 -*-
"""GPU_test_ocr_11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lN5VNC9QUKmqsbrnWJSmCkMxr1VoTk9G

# Cell 1
"""

# Install required system packages for PDF processing
!apt-get update
!apt-get install -y poppler-utils

# Install required Python packages
!pip install pdf2image pytesseract opencv-python matplotlib tqdm
!pip install torch torchvision
!pip install transformers datasets
!pip install pillow seaborn pandas
!pip install PyPDF2

"""# Cell 2"""

import os
import glob
import re
import random
import warnings
import shutil
import math
import string
import numpy as np
import cv2
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw
from tqdm.notebook import tqdm
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import pdf2image
import difflib
from collections import Counter
import pandas as pd
import seaborn as sns
from google.colab import files

# Import transformers libraries
try:
    from transformers import TrOCRProcessor, VisionEncoderDecoderModel
    from datasets import Dataset
    print("Successfully imported transformers and datasets libraries")
except ImportError:
    print("Error importing transformers or datasets. Installing again...")
    !pip install transformers datasets
    from transformers import TrOCRProcessor, VisionEncoderDecoderModel
    from datasets import Dataset

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

# Set random seeds for reproducibility
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)

print("Libraries imported successfully!")

# Check for available GPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

"""# Cell 3"""

# Define base paths and create necessary directories
base_path = '/content'
pdf_folder = os.path.join(base_path, 'pdfs')
output_base_path = os.path.join(base_path, 'ocr_data')
transcriptions_path = os.path.join(base_path, 'transcriptions')
training_data_path = os.path.join(output_base_path, 'training_data')
results_path = os.path.join(output_base_path, 'results')

# Create necessary directories
os.makedirs(pdf_folder, exist_ok=True)
os.makedirs(os.path.join(output_base_path, "images"), exist_ok=True)
os.makedirs(os.path.join(output_base_path, "processed_images"), exist_ok=True)
os.makedirs(os.path.join(output_base_path, "binary_images"), exist_ok=True)
os.makedirs(training_data_path, exist_ok=True)
os.makedirs(transcriptions_path, exist_ok=True)
os.makedirs(results_path, exist_ok=True)

print("Setup complete! All directories created.")

"""# Cell 4

"""

def convert_pdf_to_images(pdf_path, output_folder, dpi=300, first_page=None, last_page=None):
    """
    Convert PDF pages to images with robust error handling and memory management

    Args:
        pdf_path: Path to the PDF file
        output_folder: Folder to save the images
        dpi: Resolution in DPI (dots per inch)
        first_page: First page to convert (1-based index)
        last_page: Last page to convert (1-based index)

    Returns:
        List of paths to the saved images
    """
    # Create output folder if it doesn't exist
    os.makedirs(output_folder, exist_ok=True)

    # Get the PDF filename without extension
    pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]

    # Check file size to adjust DPI for large files
    file_size_mb = os.path.getsize(pdf_path) / (1024 * 1024)

    # Reduce DPI for large files to avoid memory issues
    adjusted_dpi = dpi
    if file_size_mb > 20:  # If larger than 20MB
        adjusted_dpi = min(dpi, 150)  # Reduce to 150 DPI max
    if file_size_mb > 50:  # If larger than 50MB
        adjusted_dpi = min(dpi, 100)  # Reduce to 100 DPI max

    if adjusted_dpi != dpi:
        print(f"Large PDF detected ({file_size_mb:.1f} MB). Reducing DPI from {dpi} to {adjusted_dpi}")

    try:
        # First check if poppler is properly installed
        import subprocess
        try:
            subprocess.run(["pdftoppm", "-v"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
        except (subprocess.SubprocessError, FileNotFoundError):
            print("Poppler not found in PATH. Attempting to use absolute path...")
            poppler_path = "/usr/bin"  # Default path in Colab after apt-get install
            os.environ["PATH"] += os.pathsep + poppler_path

        # Use pdf2image with memory limit protections
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")

                # Use a more memory-efficient approach for large PDFs
                if file_size_mb > 20:
                    print(f"Using single-page conversion for large PDF: {pdf_filename}")

                    # Process one page at a time for large PDFs
                    start_page = first_page if first_page else 1
                    end_page = last_page if last_page else float('inf')

                    # Get actual page count if end_page is infinity
                    if end_page == float('inf'):
                        from PyPDF2 import PdfReader
                        reader = PdfReader(pdf_path)
                        end_page = len(reader.pages)

                    image_paths = []
                    for page_num in range(start_page, end_page + 1):
                        try:
                            # Process single page with reduced memory usage
                            single_image = pdf2image.convert_from_path(
                                pdf_path,
                                dpi=adjusted_dpi,
                                first_page=page_num,
                                last_page=page_num,
                                use_pdftocairo=True,  # More memory efficient
                                size=(1000, None)  # Limit width to control memory usage
                            )

                            if single_image and len(single_image) > 0:
                                image_path = os.path.join(output_folder, f"{pdf_filename}_page_{page_num:03d}.jpg")
                                single_image[0].save(image_path, "JPEG", quality=85)  # Lower quality to save memory
                                image_paths.append(image_path)
                            else:
                                print(f"No image extracted for page {page_num}")
                        except Exception as e:
                            print(f"Error processing page {page_num}: {str(e)}")
                            # Continue with next page

                    return image_paths
                else:
                    # Normal processing for regular PDFs
                    images = pdf2image.convert_from_path(
                        pdf_path,
                        dpi=adjusted_dpi,
                        first_page=first_page,
                        last_page=last_page,
                        use_pdftocairo=True
                    )

            image_paths = []
            for i, image in enumerate(images):
                # Determine the page number (1-based index)
                page_num = i + 1 if first_page is None else first_page + i

                # Save the image with reduced quality for large files
                image_path = os.path.join(output_folder, f"{pdf_filename}_page_{page_num:03d}.jpg")
                image.save(image_path, "JPEG", quality=90 if file_size_mb < 20 else 85)
                image_paths.append(image_path)

            print(f"Successfully converted {len(image_paths)} pages from {pdf_filename}")
            return image_paths

        except Exception as pil_error:
            print(f"PIL Error for {pdf_filename}: {str(pil_error)}. Using alternative approach...")
            # Fall through to alternative approach
            raise

    except Exception as e:
        print(f"Error converting PDF {pdf_path}: {str(e)}")

        # Try alternative conversion method using PyPDF2
        try:
            print("Attempting alternative conversion method...")
            from PyPDF2 import PdfReader
            from PIL import Image, ImageDraw, ImageFont

            # Get page count
            reader = PdfReader(pdf_path)
            page_count = len(reader.pages)

            # Set page range
            start_page = first_page if first_page else 1
            end_page = min(last_page if last_page else page_count, page_count)

            # Create placeholder images with page numbers
            image_paths = []
            for page_num in range(start_page, end_page + 1):
                # Create a blank image for this page (reduced size for memory savings)
                blank_img = Image.new('RGB', (800, 1000), color='white')
                draw = ImageDraw.Draw(blank_img)

                # Add page number text
                draw.text((400, 500), f"Page {page_num} - {pdf_filename}", fill='black')
                draw.text((400, 550), "(PDF conversion failed - placeholder image)", fill='black')

                # Save the image
                image_path = os.path.join(output_folder, f"{pdf_filename}_page_{page_num:03d}.jpg")
                blank_img.save(image_path, "JPEG", quality=85)
                image_paths.append(image_path)

            print(f"Created {len(image_paths)} placeholder images for {pdf_filename}")
            return image_paths

        except Exception as backup_error:
            print(f"Alternative conversion also failed: {str(backup_error)}")
            return []

def upload_pdfs_to_colab():
    """
    Upload PDF files to Google Colab

    Returns:
        List of uploaded file paths
    """
    print("Please upload your PDF files (you can select multiple files)...")

    # Create the PDF folder if it doesn't exist
    os.makedirs(pdf_folder, exist_ok=True)

    # Upload files
    uploaded = files.upload()

    # Move uploaded files to the PDF folder
    uploaded_paths = []
    for filename, content in uploaded.items():
        # Check if the file is a PDF
        if not filename.lower().endswith('.pdf'):
            print(f"Warning: {filename} is not a PDF file. Skipping...")
            continue

        # Write the file to the PDF folder
        filepath = os.path.join(pdf_folder, filename)
        with open(filepath, 'wb') as f:
            f.write(content)

        uploaded_paths.append(filepath)
        print(f"Uploaded: {filename} -> {filepath}")

    return uploaded_paths

"""# Cell 5"""

def correct_skew(image, delta=0.5, limit=5):
    """
    Correct skew in images using Hough Line Transform

    Args:
        image: Grayscale image
        delta: Angle step size for scoring
        limit: Maximum angle to check

    Returns:
        Deskewed image
    """
    # Create edges image for better line detection
    edges = cv2.Canny(image, 50, 150, apertureSize=3)

    # Try to detect lines using Hough Transform
    lines = cv2.HoughLines(edges, 1, np.pi/180, 100)

    if lines is not None:
        # Calculate the angle histogram
        angle_counts = {}
        for line in lines:
            rho, theta = line[0]
            # Convert radians to degrees and normalize to [-90, 90]
            angle = (theta * 180 / np.pi) % 180
            if angle > 90:
                angle = angle - 180

            # Bin the angles (rounded to nearest integer)
            angle_key = round(angle)
            angle_counts[angle_key] = angle_counts.get(angle_key, 0) + 1

        # Find the angle with the most occurrences
        if angle_counts:
            max_angle = max(angle_counts, key=angle_counts.get)

            # Only correct if the angle is within reasonable limits
            if abs(max_angle) <= limit:
                # Correct 90 degree offset for vertical lines
                if abs(max_angle) > 45:
                    skew_angle = 90 - abs(max_angle)
                    if max_angle > 0:
                        skew_angle = -skew_angle
                else:
                    skew_angle = -max_angle

                # Rotate the image
                (h, w) = image.shape[:2]
                center = (w // 2, h // 2)
                M = cv2.getRotationMatrix2D(center, skew_angle, 1.0)
                return cv2.warpAffine(
                    image, M, (w, h),
                    flags=cv2.INTER_CUBIC,
                    borderMode=cv2.BORDER_REPLICATE
                )

    # If line detection fails, try projection profile method
    scores = []
    angles = np.arange(-limit, limit + delta, delta)

    for angle in angles:
        # Rotate image
        (h, w) = image.shape[:2]
        center = (w // 2, h // 2)
        M = cv2.getRotationMatrix2D(center, angle, 1.0)
        rotated = cv2.warpAffine(
            image, M, (w, h),
            flags=cv2.INTER_CUBIC,
            borderMode=cv2.BORDER_REPLICATE
        )

        # Sum the pixel values along each row
        projection = np.sum(rotated, axis=1, dtype=np.float32)

        # Calculate the score (variance of the projection)
        score = np.var(projection)
        scores.append(score)

    # Get the angle with the highest score
    best_angle = angles[np.argmax(scores)]

    # Rotate the image with the best angle
    (h, w) = image.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, best_angle, 1.0)
    rotated = cv2.warpAffine(
        image, M, (w, h),
        flags=cv2.INTER_CUBIC,
        borderMode=cv2.BORDER_REPLICATE
    )

    return rotated

def preprocess_image(image_path, output_folder, binary_folder):
    """
    Preprocess image for OCR:
    1. Convert to grayscale
    2. Apply contrast enhancement (CLAHE)
    3. Denoise the image
    4. Correct skew
    5. Apply adaptive thresholding

    Args:
        image_path: Path to the input image
        output_folder: Folder to save the preprocessed image
        binary_folder: Folder to save the binary image

    Returns:
        Tuple of (processed_image_path, binary_image_path)
    """
    # Create output folders if they don't exist
    os.makedirs(output_folder, exist_ok=True)
    os.makedirs(binary_folder, exist_ok=True)

    # Get the image filename
    image_filename = os.path.basename(image_path)

    # Read the image
    img = cv2.imread(image_path)
    if img is None:
        print(f"Failed to read image {image_path}")
        return None, None

    # Convert to grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Apply contrast enhancement (CLAHE)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    enhanced = clahe.apply(gray)

    # Denoise
    denoised = cv2.fastNlMeansDenoising(enhanced, h=10)

    # Detect and correct skew
    corrected = correct_skew(denoised)

    # Apply adaptive thresholding to create binary image
    # This helps to separate text from background
    binary = cv2.adaptiveThreshold(
        corrected,
        255,
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY,
        15,  # Block size
        9    # Constant subtracted from the mean
    )

    # Save processed image
    processed_image_path = os.path.join(output_folder, image_filename)
    cv2.imwrite(processed_image_path, corrected)

    # Save binary image
    binary_image_path = os.path.join(binary_folder, image_filename)
    cv2.imwrite(binary_image_path, binary)

    return processed_image_path, binary_image_path

def process_all_pdfs(pdf_folder, output_base_path, dpi=300, max_pages_per_pdf=None):
    """
    Process all PDFs in a folder with improved error handling and recovery

    Args:
        pdf_folder: Folder containing the PDFs
        output_base_path: Base folder to save the output
        dpi: Resolution in DPI (dots per inch)
        max_pages_per_pdf: Maximum number of pages to process per PDF

    Returns:
        Dictionary mapping document IDs to lists of processed image paths
    """
    # Define output folders
    images_folder = os.path.join(output_base_path, "images")
    processed_folder = os.path.join(output_base_path, "processed_images")
    binary_folder = os.path.join(output_base_path, "binary_images")

    # Create output folders if they don't exist
    os.makedirs(images_folder, exist_ok=True)
    os.makedirs(processed_folder, exist_ok=True)
    os.makedirs(binary_folder, exist_ok=True)

    # Get all PDF files
    pdf_files = glob.glob(os.path.join(pdf_folder, "*.pdf"))
    print(f"Found {len(pdf_files)} PDF files")

    # Dictionary to store document ID to image paths mapping
    document_images = {}

    # Process each PDF
    for pdf_file in tqdm(pdf_files, desc="Processing PDFs"):
        pdf_filename = os.path.splitext(os.path.basename(pdf_file))[0]
        print(f"\nProcessing {pdf_filename}")

        # Create folders for this PDF
        pdf_images_folder = os.path.join(images_folder, pdf_filename)
        pdf_processed_folder = os.path.join(processed_folder, pdf_filename)
        pdf_binary_folder = os.path.join(binary_folder, pdf_filename)

        os.makedirs(pdf_images_folder, exist_ok=True)
        os.makedirs(pdf_processed_folder, exist_ok=True)
        os.makedirs(pdf_binary_folder, exist_ok=True)

        # Handle large PDFs with a different approach
        if os.path.getsize(pdf_file) > 10*1024*1024:  # > 10MB
            print(f"Large PDF detected: {pdf_filename}. Processing in chunks...")

            # Process large PDF in chunks of 5 pages
            chunk_size = 5
            first_page = 1
            all_image_paths = []

            # Try to get total page count first
            try:
                from PyPDF2 import PdfReader
                reader = PdfReader(pdf_file)
                total_pages = len(reader.pages)
                max_page = min(total_pages, max_pages_per_pdf) if max_pages_per_pdf else total_pages

                print(f"PDF has {total_pages} pages. Processing up to page {max_page}.")

                while first_page <= max_page:
                    last_page = min(first_page + chunk_size - 1, max_page)
                    chunk_image_paths = convert_pdf_to_images(
                        pdf_file,
                        pdf_images_folder,
                        dpi=dpi,
                        first_page=first_page,
                        last_page=last_page
                    )

                    if not chunk_image_paths:
                        # Try with a smaller chunk if failed
                        if chunk_size > 1:
                            print(f"Trying with smaller chunk size for pages {first_page}-{last_page}...")
                            chunk_size = 1
                            continue
                        else:
                            # If even single page processing fails, skip to next chunk
                            print(f"Failed to process pages {first_page}-{last_page}, skipping.")
                            first_page += chunk_size
                            continue

                    all_image_paths.extend(chunk_image_paths)
                    first_page += chunk_size

                # Store the image paths
                document_images[pdf_filename] = all_image_paths

            except Exception as e:
                print(f"Error determining page count: {str(e)}")
                # Fallback: just try to process first 10 pages
                chunk_image_paths = convert_pdf_to_images(
                    pdf_file,
                    pdf_images_folder,
                    dpi=dpi,
                    first_page=1,
                    last_page=10
                )
                document_images[pdf_filename] = chunk_image_paths if chunk_image_paths else []
        else:
            # Convert PDF to images
            image_paths = convert_pdf_to_images(
                pdf_file,
                pdf_images_folder,
                dpi=dpi,
                first_page=1,
                last_page=max_pages_per_pdf
            )

            # Store the image paths
            document_images[pdf_filename] = image_paths if image_paths else []

        # Check if we got any images for this document
        if not document_images[pdf_filename]:
            print(f"WARNING: No images were extracted from {pdf_filename}")
            continue

        # Preprocess each image
        processed_image_paths = []
        for image_path in tqdm(document_images[pdf_filename], desc=f"Preprocessing images for {pdf_filename}"):
            try:
                processed_path, _ = preprocess_image(
                    image_path,
                    pdf_processed_folder,
                    pdf_binary_folder
                )
                if processed_path:
                    processed_image_paths.append(processed_path)
            except Exception as e:
                print(f"Error preprocessing {image_path}: {str(e)}")
                # Continue with next image

        # Update the document images dictionary
        document_images[pdf_filename] = processed_image_paths

    # Check if we processed any images successfully
    total_processed = sum(len(paths) for paths in document_images.values())
    if total_processed == 0:
        print("\nWARNING: No PDFs were successfully processed.")
        print("Please make sure poppler-utils is installed correctly.")
    else:
        print(f"\nSuccessfully processed {total_processed} images from {len(document_images)} documents")

    return document_images

def visualize_preprocessing(original_path, processed_path, binary_path):
    """
    Visualize the preprocessing steps

    Args:
        original_path: Path to the original image
        processed_path: Path to the processed image
        binary_path: Path to the binary image
    """
    # Read the images
    original = cv2.imread(original_path)
    processed = cv2.imread(processed_path, cv2.IMREAD_GRAYSCALE)
    binary = cv2.imread(binary_path, cv2.IMREAD_GRAYSCALE)

    # Convert original to RGB for display
    original_rgb = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)

    # Create a figure with 3 subplots
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # Display the images
    axes[0].imshow(original_rgb)
    axes[0].set_title("Original Image")
    axes[0].axis("off")

    axes[1].imshow(processed, cmap="gray")
    axes[1].set_title("Processed Image")
    axes[1].axis("off")

    axes[2].imshow(binary, cmap="gray")
    axes[2].set_title("Binary Image")
    axes[2].axis("off")

    # Show the figure
    plt.tight_layout()
    plt.show()

def save_example_images(document_images, output_base_path, num_examples=3):
    """
    Save example images from different preprocessing stages for visualization

    Args:
        document_images: Dictionary mapping document IDs to processed image paths
        output_base_path: Base folder for outputs
        num_examples: Number of examples to save
    """
    # Create example folder
    example_folder = os.path.join(output_base_path, "examples")
    os.makedirs(example_folder, exist_ok=True)

    # Collect a few examples
    examples = []

    for doc_id, image_paths in document_images.items():
        # Skip if no images
        if not image_paths:
            continue

        # Take the first few images from each document
        for i, processed_path in enumerate(image_paths[:num_examples]):
            # Get the original and binary image paths
            original_path = processed_path.replace("processed_images", "images")
            binary_path = processed_path.replace("processed_images", "binary_images")

            if os.path.exists(original_path) and os.path.exists(binary_path):
                examples.append((original_path, processed_path, binary_path))

    # Visualize the examples
    for original_path, processed_path, binary_path in examples:
        visualize_preprocessing(original_path, processed_path, binary_path)

"""# Cell 6"""

def create_dummy_transcriptions(document_images, transcriptions_path):
    """
    Create dummy transcriptions for testing purposes

    Args:
        document_images: Dictionary mapping document IDs to processed image paths
        transcriptions_path: Path to save the transcriptions
    """
    os.makedirs(transcriptions_path, exist_ok=True)

    # Create dummy transcriptions for each document
    for doc_id, image_paths in document_images.items():
        # Create a transcription file for this document
        transcription_file = os.path.join(transcriptions_path, f"{doc_id}.txt")

        # Create dummy transcription content
        transcription_content = f"Sample transcription for {doc_id}\n\n"
        transcription_content += "This is a placeholder text that simulates a transcription of the document.\n"
        transcription_content += "In a real scenario, this would contain the actual text from the document.\n\n"

        # Add more specific content based on document ID
        if "Buendia" in doc_id:
            transcription_content += "Este documento contiene texto histórico en español del siglo XVII.\n"
            transcription_content += "El autor es Don Fausto Agustín de Buendía, quien escribió sobre temas religiosos y políticos.\n"
        elif "Mendo" in doc_id:
            transcription_content += "Este documento fue escrito por Andrés Mendo, y trata de documentos políticos y morales.\n"
            transcription_content += "El texto incluye varios emblemas y enseñanzas para un Príncipe Perfecto.\n"
        elif "Ezcaray" in doc_id:
            transcription_content += "Este documento de Antonio de Ezcaray, trata sobre voces del dolor contra la profanidad.\n"
            transcription_content += "Incluye varias reflexiones sobre la religión y la moral en la sociedad española.\n"
        elif "Constituciones" in doc_id:
            transcription_content += "Este documento contiene las Constituciones Sinodales de Calahorra.\n"
            transcription_content += "El texto legal data de 1602 y establece normas eclesiásticas para la diócesis.\n"
        elif "Paredes" in doc_id:
            transcription_content += "Este documento contiene las Reglas Generales escritas por Paredes.\n"
            transcription_content += "El texto trata sobre normas de comportamiento y etiqueta en la sociedad española.\n"
        else:
            transcription_content += "Este documento histórico español requiere un análisis detallado.\n"
            transcription_content += "El texto contiene información valiosa sobre la cultura y sociedad de su época.\n"

        # Write the transcription to the file
        with open(transcription_file, 'w', encoding='utf-8') as f:
            f.write(transcription_content)

        print(f"Created dummy transcription for {doc_id}")

def load_transcriptions(transcriptions_path, image_paths):
    """
    Load transcriptions for the images

    Args:
        transcriptions_path: Path containing the transcription files
        image_paths: List of image paths

    Returns:
        Dictionary mapping image paths to transcriptions
    """
    transcriptions = {}

    # Get all transcription files
    transcription_files = glob.glob(os.path.join(transcriptions_path, "*.txt"))

    # Check if we have any transcription files
    if not transcription_files:
        print("No transcription files found.")
        return transcriptions

    # For each image path
    for image_path in image_paths:
        # Extract the document ID from the image path
        img_dir = os.path.dirname(image_path)
        doc_id = os.path.basename(img_dir)

        # Find the corresponding transcription file
        transcription_file = os.path.join(transcriptions_path, f"{doc_id}.txt")

        if os.path.exists(transcription_file):
            # Load the transcription
            with open(transcription_file, 'r', encoding='utf-8') as f:
                transcription = f.read()

            # Assign the transcription to the image
            transcriptions[image_path] = transcription
        else:
            # No transcription found for this document
            if 'dummy' not in doc_id.lower():  # Skip warning for dummy images
                print(f"No transcription found for {doc_id}")

    print(f"Loaded {len(transcriptions)} transcriptions")
    return transcriptions

def create_train_val_split(image_paths, transcriptions, val_ratio=0.2):
    """
    Create train and validation splits

    Args:
        image_paths: List of image paths
        transcriptions: Dictionary mapping image paths to transcriptions
        val_ratio: Ratio of validation data

    Returns:
        Tuple of (train_image_paths, val_image_paths)
    """
    # Filter image paths to only include those with transcriptions
    valid_image_paths = [path for path in image_paths if path in transcriptions]

    # Shuffle the image paths
    random.shuffle(valid_image_paths)

    # Calculate the split index
    split_idx = int(len(valid_image_paths) * (1 - val_ratio))

    # Split the image paths
    train_image_paths = valid_image_paths[:split_idx]
    val_image_paths = valid_image_paths[split_idx:]

    return train_image_paths, val_image_paths

class OCRDataset(torch.utils.data.Dataset):
    """
    Dataset class for OCR training.
    """
    def __init__(self, image_paths, transcriptions, transform=None, max_length=512):
        """
        Initialize the dataset.

        Args:
            image_paths: List of image paths.
            transcriptions: Dictionary mapping image paths to transcriptions.
            transform: Optional transform to be applied to the images.
            max_length: Maximum sequence length for the transcriptions.
        """
        self.image_paths = image_paths
        self.transcriptions = transcriptions
        self.transform = transform
        self.max_length = max_length

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        # Get image path for the given index
        image_path = self.image_paths[idx]

        # Load image and apply transform if available
        image = Image.open(image_path).convert("RGB")
        if self.transform:
            image = self.transform(image)

        # Get transcription and truncate if necessary
        text = self.transcriptions.get(image_path, "")
        if len(text) > self.max_length:
            text = text[:self.max_length]

        return {"image": image, "text": text, "image_path": image_path}

def create_data_loaders(train_image_paths, val_image_paths, transcriptions, batch_size=4):
    """
    Create data loaders for training and validation

    Args:
        train_image_paths: List of training image paths
        val_image_paths: List of validation image paths
        transcriptions: Dictionary mapping image paths to transcriptions
        batch_size: Batch size

    Returns:
        Tuple of (train_loader, val_loader)
    """
    # Define image transforms
    transform = transforms.Compose([
        transforms.Resize((384, 384)),  # Resize for TrOCR
        transforms.ToTensor(),
    ])

    # Create datasets
    train_dataset = OCRDataset(train_image_paths, transcriptions, transform)
    val_dataset = OCRDataset(val_image_paths, transcriptions, transform)

    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=2 if torch.cuda.is_available() else 0,
        pin_memory=torch.cuda.is_available()
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2 if torch.cuda.is_available() else 0,
        pin_memory=torch.cuda.is_available()
    )

    return train_loader, val_loader

"""# Cell 7"""

def normalize_historical_spanish(text):
    """
    Normalize historical Spanish text

    This handles common variations in early modern Spanish typography:
    - Long s (ſ) -> s
    - Ligatures like æ -> ae
    - U/V variations (often interchangeable in early texts)
    - I/J variations (often interchangeable in early texts)
    - Double consonants
    - Contractions and abbreviations

    Args:
        text: Input text

    Returns:
        Normalized text
    """
    # Replace long s with regular s
    text = text.replace('ſ', 's')

    # Replace ligatures
    text = text.replace('æ', 'ae').replace('œ', 'oe')

    # Handle common abbreviations in historical Spanish
    # This is a simplified example - a complete list would be much longer
    abbreviations = {
        'q̃': 'que',
        'ẽ': 'en',
        'õ': 'on',
        'ñ': 'nn',  # In some early texts
        'ȷ': 'i',    # dotless i
    }

    for abbr, full in abbreviations.items():
        text = text.replace(abbr, full)

    return text

def extract_main_text(text):
    """
    Extract main text from the transcription, removing marginalia and notes

    Args:
        text: Input transcription

    Returns:
        Main text
    """
    # This is a simplified example - in practice, you would need more sophisticated rules
    # based on the specific formatting of your transcriptions

    # Remove lines starting with common marginalia markers
    lines = text.split('\n')
    filtered_lines = []

    in_marginalia = False
    for line in lines:
        # Skip lines that look like marginalia
        if line.strip().startswith('[') and line.strip().endswith(']'):
            continue

        # Skip lines that look like notes or editorial comments
        if line.strip().startswith('(') and line.strip().endswith(')'):
            continue

        # Handle multi-line marginalia blocks
        if line.strip().startswith('/*'):
            in_marginalia = True
            continue

        if in_marginalia:
            if line.strip().endswith('*/'):
                in_marginalia = False
            continue

        filtered_lines.append(line)

    return '\n'.join(filtered_lines)

def create_lexicon_from_transcriptions(transcriptions, min_word_length=2):
    """
    Create a lexicon from the transcriptions to help with post-processing

    Args:
        transcriptions: Dictionary mapping image paths to transcriptions
        min_word_length: Minimum word length to include in the lexicon

    Returns:
        Set of unique words
    """
    lexicon = set()

    for text in transcriptions.values():
        # Normalize the text
        normalized_text = normalize_historical_spanish(text)

        # Extract main text
        main_text = extract_main_text(normalized_text)

        # Split into words and add to lexicon
        words = main_text.split()
        for word in words:
            # Clean the word
            clean_word = word.strip('.,;:!?()[]{}"\'-—')

            # Only add words that meet the minimum length
            if len(clean_word) >= min_word_length:
                lexicon.add(clean_word.lower())

    return lexicon

def augment_lexicon_with_variations(lexicon):
    """
    Augment the lexicon with common historical variations

    Args:
        lexicon: Set of unique words

    Returns:
        Augmented lexicon
    """
    augmented_lexicon = set(lexicon)

    # Common character substitutions in early modern Spanish
    substitutions = [
        ('v', 'u'),   # v/u variations
        ('u', 'v'),
        ('i', 'j'),   # i/j variations
        ('j', 'i'),
        ('y', 'i'),   # y/i variations
        ('i', 'y'),
        ('ç', 'z'),   # cedilla/z variations
        ('z', 'ç'),
        ('f', 'ff'),  # single/double consonant variations
        ('ff', 'f'),
        ('s', 'ss'),
        ('ss', 's'),
        ('n', 'ñ'),   # n/ñ variations
        ('ñ', 'n'),
    ]

    # Add variations to the lexicon
    for word in lexicon:
        for old, new in substitutions:
            if old in word:
                variation = word.replace(old, new)
                augmented_lexicon.add(variation)

    return augmented_lexicon

class SpanishHistoricalPostProcessor:
    """
    Post-processing class for OCR results on historical Spanish texts
    """
    def __init__(self, lexicon=None):
        """
        Initialize the post-processor

        Args:
            lexicon: Lexicon of valid words
        """
        self.lexicon = lexicon or set()

        # Add common Spanish articles, prepositions, etc. to ensure basic words are covered
        common_words = {
            'el', 'la', 'los', 'las',       # Articles
            'de', 'en', 'con', 'por', 'a',  # Prepositions
            'y', 'e', 'o', 'u',             # Conjunctions
            'que', 'como', 'si',            # Conjunctions/relative pronouns
            'no', 'ni',                     # Negation
        }
        self.lexicon.update(common_words)

    def correct_word(self, word, max_edit_distance=2):
        """
        Correct a word using the lexicon

        Args:
            word: Word to correct
            max_edit_distance: Maximum edit distance for correction

        Returns:
            Corrected word
        """
        # If the word is already in the lexicon, return it
        if word.lower() in self.lexicon:
            return word

        # If word is empty or too short, return it as is
        if len(word) < 2:
            return word

        # Simple edit distance function
        def levenshtein_distance(s1, s2):
            if len(s1) < len(s2):
                return levenshtein_distance(s2, s1)

            if len(s2) == 0:
                return len(s1)

            previous_row = range(len(s2) + 1)
            for i, c1 in enumerate(s1):
                current_row = [i + 1]
                for j, c2 in enumerate(s2):
                    insertions = previous_row[j + 1] + 1
                    deletions = current_row[j] + 1
                    substitutions = previous_row[j] + (c1 != c2)
                    current_row.append(min(insertions, deletions, substitutions))
                previous_row = current_row

            return previous_row[-1]

        # Find the closest word in the lexicon
        candidates = []
        for lex_word in self.lexicon:
            # Skip words with significantly different lengths
            if abs(len(lex_word) - len(word)) > max_edit_distance:
                continue

            # Calculate edit distance
            distance = levenshtein_distance(word.lower(), lex_word)

            # Only consider words within the maximum edit distance
            if distance <= max_edit_distance:
                candidates.append((lex_word, distance))

        # Sort candidates by edit distance
        candidates.sort(key=lambda x: x[1])

        # Return the closest match if any, otherwise return the original word
        return candidates[0][0] if candidates else word

    def process_text(self, text):
        """
        Process a complete OCR text

        Args:
            text: OCR text

        Returns:
            Processed text
        """
        # Normalize the text
        text = normalize_historical_spanish(text)

        # Split into words
        words = []
        for word in text.split():
            # Extract the word and its surrounding punctuation
            prefix = ""
            suffix = ""

            while word and not word[0].isalnum():
                prefix += word[0]
                word = word[1:]

            while word and not word[-1].isalnum():
                suffix = word[-1] + suffix
                word = word[:-1]

            # Correct the word if it's not empty
            if word:
                corrected_word = self.correct_word(word)
                words.append(prefix + corrected_word + suffix)
            else:
                words.append(prefix + suffix)

        # Join the words back into text
        return ' '.join(words)

"""# Cell 8"""

def normalize_text(text):
    """
    Normalize text for evaluation:
    - Convert to lowercase
    - Remove punctuation
    - Remove extra whitespace

    Args:
        text: Input text

    Returns:
        Normalized text
    """
    # Convert to lowercase
    text = text.lower()

    # Remove punctuation
    import string
    translator = str.maketrans('', '', string.punctuation)
    text = text.translate(translator)

    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    return text

def character_error_rate(reference, hypothesis):
    """
    Calculate Character Error Rate (CER)

    CER = (S + D + I) / N
    Where:
    S = number of substitutions
    D = number of deletions
    I = number of insertions
    N = number of characters in reference

    Args:
        reference: Ground truth text
        hypothesis: OCR output text

    Returns:
        CER value (lower is better)
    """
    # Normalize texts
    reference = normalize_text(reference)
    hypothesis = normalize_text(hypothesis)

    # Compute Levenshtein distance
    distances = np.zeros((len(reference) + 1, len(hypothesis) + 1))

    # Initialize first row and column
    for i in range(len(reference) + 1):
        distances[i][0] = i
    for j in range(len(hypothesis) + 1):
        distances[0][j] = j

    # Fill distance matrix
    for i in range(1, len(reference) + 1):
        for j in range(1, len(hypothesis) + 1):
            if reference[i-1] == hypothesis[j-1]:
                distances[i][j] = distances[i-1][j-1]
            else:
                substitution = distances[i-1][j-1] + 1
                insertion = distances[i][j-1] + 1
                deletion = distances[i-1][j] + 1
                distances[i][j] = min(substitution, insertion, deletion)

    # Levenshtein distance is the value in the bottom right corner of the matrix
    levenshtein = distances[len(reference)][len(hypothesis)]

    # CER is Levenshtein distance divided by reference length
    if len(reference) == 0:
        return 0.0  # Handle empty reference case

    return levenshtein / len(reference)

def word_error_rate(reference, hypothesis):
    """
    Calculate Word Error Rate (WER)

    WER = (S + D + I) / N
    Where:
    S = number of substituted words
    D = number of deleted words
    I = number of inserted words
    N = number of words in reference

    Args:
        reference: Ground truth text
        hypothesis: OCR output text

    Returns:
        WER value (lower is better)
    """
    # Normalize texts
    reference = normalize_text(reference)
    hypothesis = normalize_text(hypothesis)

    # Split into words
    ref_words = reference.split()
    hyp_words = hypothesis.split()

    # Compute Levenshtein distance
    distances = np.zeros((len(ref_words) + 1, len(hyp_words) + 1))

    # Initialize first row and column
    for i in range(len(ref_words) + 1):
        distances[i][0] = i
    for j in range(len(hyp_words) + 1):
        distances[0][j] = j

    # Fill distance matrix
    for i in range(1, len(ref_words) + 1):
        for j in range(1, len(hyp_words) + 1):
            if ref_words[i-1] == hyp_words[j-1]:
                distances[i][j] = distances[i-1][j-1]
            else:
                substitution = distances[i-1][j-1] + 1
                insertion = distances[i][j-1] + 1
                deletion = distances[i-1][j] + 1
                distances[i][j] = min(substitution, insertion, deletion)

    # Levenshtein distance is the value in the bottom right corner of the matrix
    levenshtein = distances[len(ref_words)][len(hyp_words)]

    # WER is Levenshtein distance divided by reference length
    if len(ref_words) == 0:
        return 0.0  # Handle empty reference case

    return levenshtein / len(ref_words)

def visualize_ocr_results(results, output_path):
    """
    Visualize OCR results by document type

    Args:
        results: Dictionary with OCR results
        output_path: Path to save the visualization
    """
    # Check if we have results
    if not results or "cer_by_doc" not in results:
        print("No results to visualize")
        return

    # Extract data
    doc_types = list(results["cer_by_doc"].keys())
    cer_values = [results["cer_by_doc"][doc] for doc in doc_types]
    wer_values = [results["wer_by_doc"][doc] for doc in doc_types]

    # Create figure and axis
    fig, ax1 = plt.subplots(figsize=(12, 6))

    # Set width for bars
    width = 0.35

    # Set x positions for bars
    x = np.arange(len(doc_types))

    # Plot CER bars
    cer_bars = ax1.bar(x - width/2, cer_values, width, label='CER', color='skyblue')

    # Plot WER bars
    wer_bars = ax1.bar(x + width/2, wer_values, width, label='WER', color='lightcoral')

    # Add labels and legend
    ax1.set_xlabel('Document Type')
    ax1.set_ylabel('Error Rate')
    ax1.set_title('OCR Error Rates by Document Type')
    ax1.set_xticks(x)
    ax1.set_xticklabels(doc_types, rotation=45, ha="right")
    ax1.legend()

    # Add value labels on the bars
    def add_labels(bars):
        for bar in bars:
            height = bar.get_height()
            ax1.annotate(f'{height:.2f}',
                         xy=(bar.get_x() + bar.get_width() / 2, height),
                         xytext=(0, 3),  # 3 points vertical offset
                         textcoords="offset points",
                         ha='center', va='bottom')

    add_labels(cer_bars)
    add_labels(wer_bars)

    # Adjust layout and save
    plt.tight_layout()
    plt.savefig(output_path)
    plt.close()

    print(f"Visualization saved to {output_path}")

"""# Cell 9"""

def prepare_dataset(batch, processor):
    """
    Prepare batch data for TrOCR model training

    Args:
        batch: Batch of data from the dataset
        processor: TrOCR processor

    Returns:
        Processed batch with pixel_values and labels
    """
    # Handle images with proper format conversion
    raw_images = batch["image"]
    processed_images = []

    for img in raw_images:
        # Convert from numpy array to PIL Image
        from PIL import Image
        import numpy as np

        # Handle different possible formats
        if isinstance(img, np.ndarray):
            if img.ndim == 3:  # RGB image
                pil_img = Image.fromarray(img.astype('uint8'))
            elif img.ndim == 2:  # Grayscale image
                pil_img = Image.fromarray(np.repeat(img[:, :, np.newaxis], 3, axis=2).astype('uint8'))
            else:
                # Create a blank image as fallback
                pil_img = Image.new('RGB', (384, 384), color='white')
        elif isinstance(img, Image.Image):
            pil_img = img
        else:
            # Last resort - create a blank image
            pil_img = Image.new('RGB', (384, 384), color='white')

        processed_images.append(pil_img)

    # Process the images with the TrOCR processor
    pixel_values = processor(images=processed_images, return_tensors="pt").pixel_values

    # Tokenize the texts
    labels = processor.tokenizer(batch["text"], padding="max_length", truncation=True).input_ids

    return {"pixel_values": pixel_values, "labels": labels}

def convert_dataloader_to_dataset(data_loader):
    """
    Convert PyTorch DataLoader to HuggingFace Dataset

    Args:
        data_loader: PyTorch DataLoader

    Returns:
        HuggingFace Dataset
    """
    all_data = []

    for batch in data_loader:
        for i in range(len(batch["image"])):
            # Convert tensor to numpy safely
            if torch.is_tensor(batch["image"][i]):
                # Transpose from [C, H, W] to [H, W, C] if needed
                if batch["image"][i].dim() == 3 and batch["image"][i].shape[0] == 3:
                    img_np = batch["image"][i].permute(1, 2, 0).numpy()
                else:
                    img_np = batch["image"][i].numpy()

                # Normalize to 0-255 range for PIL compatibility
                if img_np.max() <= 1.0:
                    img_np = (img_np * 255).astype('uint8')
            else:
                # Just in case it's already a numpy array
                img_np = batch["image"][i]

            item = {
                "image": img_np,
                "text": batch["text"][i],
                "image_path": batch["image_path"][i]
            }
            all_data.append(item)

    # Create the dataset
    return Dataset.from_list(all_data)

def evaluate_trocr_model(model, processor, val_loader, post_processor=None, device="cpu"):
    """
    Evaluate a TrOCR model on a validation set

    Args:
        model: TrOCR model
        processor: TrOCR processor
        val_loader: Validation data loader
        post_processor: Optional post-processor for Spanish historical text
        device: Device to run the model on

    Returns:
        Dictionary with evaluation results
    """
    model.eval()

    cer_scores = []
    wer_scores = []

    # Group results by document type
    cer_by_doc = {}
    wer_by_doc = {}
    doc_samples = {}

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Evaluating model"):
            # Get images and texts
            images = batch["image"].to(device)
            texts = batch["text"]
            image_paths = batch["image_path"]

            # Generate predictions
            generated_ids = model.generate(
                processor(images=images, return_tensors="pt").pixel_values.to(device)
            )

            # Decode predictions
            preds = processor.batch_decode(generated_ids, skip_special_tokens=True)

            # Apply post-processing if available
            if post_processor:
                preds = [post_processor.process_text(p) for p in preds]

            # Calculate metrics for each sample
            for i, (pred, ref) in enumerate(zip(preds, texts)):
                cer = character_error_rate(ref, pred)
                wer = word_error_rate(ref, pred)

                cer_scores.append(cer)
                wer_scores.append(wer)

                # Extract document type from image path
                image_path = image_paths[i]
                doc_type = "unknown"

                # Check for document type in image path
                for type_name in ["Buendia", "Mendo", "Ezcaray", "Constituciones", "Paredes"]:
                    if type_name in image_path:
                        doc_type = type_name
                        break

                # Add to document-specific results
                if doc_type not in cer_by_doc:
                    cer_by_doc[doc_type] = []
                    wer_by_doc[doc_type] = []
                    doc_samples[doc_type] = []

                cer_by_doc[doc_type].append(cer)
                wer_by_doc[doc_type].append(wer)

                # Store a sample prediction for each document type
                if len(doc_samples[doc_type]) < 2:  # Keep max 2 samples per type
                    doc_samples[doc_type].append({
                        "reference": ref,
                        "prediction": pred,
                        "cer": cer,
                        "wer": wer,
                        "image_path": image_path
                    })

    # Calculate average metrics
    avg_cer = sum(cer_scores) / len(cer_scores) if cer_scores else 0
    avg_wer = sum(wer_scores) / len(wer_scores) if wer_scores else 0

    # Calculate average metrics by document type
    avg_cer_by_doc = {doc: sum(scores) / len(scores) if scores else 0
                      for doc, scores in cer_by_doc.items()}
    avg_wer_by_doc = {doc: sum(scores) / len(scores) if scores else 0
                      for doc, scores in wer_by_doc.items()}

    return {
        "avg_cer": avg_cer,
        "avg_wer": avg_wer,
        "cer_by_doc": avg_cer_by_doc,
        "wer_by_doc": avg_wer_by_doc,
        "samples": doc_samples
    }

def fine_tune_trocr_model(train_loader, val_loader, output_dir, num_epochs=3):
    """
    Fine-tune a TrOCR model on custom data

    Args:
        train_loader: Training data loader
        val_loader: Validation data loader
        output_dir: Directory to save the fine-tuned model
        num_epochs: Number of training epochs

    Returns:
        Tuple of (model, processor, results)
    """
    from transformers import TrOCRProcessor, VisionEncoderDecoderModel
    from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments

    # Check if GPU is available
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Fine-tuning using device: {device}")

    # Load processor and model
    print("Loading TrOCR model and processor...")
    processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")
    model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten")

    # ---- FIX: Explicitly set pad_token_id and decoder_start_token_id in model config ----
    # Check and set the pad_token_id in the shared config
    if not hasattr(model.config, 'pad_token_id') or model.config.pad_token_id is None:
        # Default value for TrOCR model
        model.config.pad_token_id = 1
        print(f"Set pad_token_id to {model.config.pad_token_id} in the shared config")

    # Check and set the decoder_start_token_id in the shared config
    if not hasattr(model.config, 'decoder_start_token_id') or model.config.decoder_start_token_id is None:
        # Default value from the TrOCR model
        model.config.decoder_start_token_id = 2
        print(f"Set decoder_start_token_id to {model.config.decoder_start_token_id} in the shared config")

    # Also set them in the decoder config for safety
    if hasattr(model, 'decoder') and hasattr(model.decoder, 'config'):
        if not hasattr(model.decoder.config, 'pad_token_id') or model.decoder.config.pad_token_id is None:
            model.decoder.config.pad_token_id = 1
            print(f"Set pad_token_id to {model.decoder.config.pad_token_id} in the decoder config")

        if not hasattr(model.decoder.config, 'decoder_start_token_id') or model.decoder.config.decoder_start_token_id is None:
            model.decoder.config.decoder_start_token_id = 2
            print(f"Set decoder_start_token_id to {model.decoder.config.decoder_start_token_id} in the decoder config")
    # ---- End of fix ----

    # Move model to device
    model.to(device)

    # Convert PyTorch DataLoaders to HuggingFace Datasets
    print("Converting DataLoaders to Datasets...")
    train_dataset = convert_dataloader_to_dataset(train_loader)
    val_dataset = convert_dataloader_to_dataset(val_loader)

    print(f"Created training dataset with {len(train_dataset)} samples")
    print(f"Created validation dataset with {len(val_dataset)} samples")

    # Prepare the datasets for training
    print("Preparing datasets for training...")

    # Use a partial function to include the processor
    from functools import partial
    prepare_with_processor = partial(prepare_dataset, processor=processor)

    train_dataset = train_dataset.map(
        prepare_with_processor,
        batched=True,
        batch_size=4,
        remove_columns=["image", "image_path"]
    )

    val_dataset = val_dataset.map(
        prepare_with_processor,
        batched=True,
        batch_size=4,
        remove_columns=["image", "image_path"]
    )

    # Define training arguments
    training_args = Seq2SeqTrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        predict_with_generate=True,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        num_train_epochs=num_epochs,
        fp16=torch.cuda.is_available(),
        learning_rate=5e-5,
        weight_decay=0.01,
        save_total_limit=2,
        load_best_model_at_end=True
    )

    # Define compute metrics function
    def compute_metrics(pred):
        labels_ids = pred.label_ids
        pred_ids = pred.predictions

        # Replace -100 with pad token id
        labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id

        # Decode predictions and labels
        pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
        labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id
        label_str = processor.tokenizer.batch_decode(labels_ids, skip_special_tokens=True)

        # Compute CER
        cer_scores = []
        for pred, label in zip(pred_str, label_str):
            cer = character_error_rate(label, pred)
            cer_scores.append(cer)

        avg_cer = sum(cer_scores) / len(cer_scores) if cer_scores else 0

        return {
            "cer": avg_cer,
            "accuracy": 1 - avg_cer
        }

    # Create Seq2Seq trainer
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics
    )

    # Train the model
    print(f"\nFine-tuning TrOCR model for {num_epochs} epochs...")
    trainer.train()

    # Save the fine-tuned model
    model_save_path = os.path.join(output_dir, "final")
    model.save_pretrained(model_save_path)
    processor.save_pretrained(model_save_path)
    print(f"Fine-tuned model saved to {model_save_path}")

    # Evaluate the model
    print("\nEvaluating fine-tuned model...")

    # Create post-processor for evaluation
    post_processor = None  # Set to None for now

    # Run evaluation
    results = evaluate_trocr_model(model, processor, val_loader, post_processor, device)

    # Save results
    results_file = os.path.join(output_dir, "evaluation_results.json")
    import json
    with open(results_file, 'w') as f:
        json.dump({k: v for k, v in results.items() if k != 'samples'}, f, indent=2)

    # Print summary
    print(f"Average CER: {results['avg_cer']:.4f}")
    print(f"Average WER: {results['avg_wer']:.4f}")

    for doc_type, cer in results['cer_by_doc'].items():
        print(f"{doc_type} - CER: {cer:.4f}, WER: {results['wer_by_doc'][doc_type]:.4f}")

    # Create visualization
    viz_path = os.path.join(output_dir, "error_rates_by_doc.png")
    visualize_ocr_results(results, viz_path)

    return model, processor, results

def run_ocr_evaluation(val_loader, transcriptions, output_dir):
    """
    Run OCR evaluation using a pre-trained TrOCR model

    Args:
        val_loader: Validation data loader
        transcriptions: Dictionary mapping image paths to transcriptions
        output_dir: Directory to save evaluation results

    Returns:
        Evaluation results
    """
    from transformers import TrOCRProcessor, VisionEncoderDecoderModel

    # Check if GPU is available
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Evaluation using device: {device}")

    # Load processor and model
    print("Loading TrOCR model and processor...")
    processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")
    model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten")

    # Move model to device
    model.to(device)

    # Create lexicon for post-processing
    print("Creating lexicon for post-processing...")
    lexicon = create_lexicon_from_transcriptions(transcriptions)
    lexicon = augment_lexicon_with_variations(lexicon)
    print(f"Created lexicon with {len(lexicon)} words")

    # Create post-processor
    post_processor = SpanishHistoricalPostProcessor(lexicon)

    # Run evaluation
    print("\nEvaluating pre-trained TrOCR model...")
    results = evaluate_trocr_model(model, processor, val_loader, post_processor, device)

    # Save results
    results_file = os.path.join(output_dir, "evaluation_results.json")
    import json
    with open(results_file, 'w') as f:
        json.dump({k: v for k, v in results.items() if k != 'samples'}, f, indent=2)

    # Print summary
    print(f"Average CER: {results['avg_cer']:.4f}")
    print(f"Average WER: {results['avg_wer']:.4f}")

    for doc_type, cer in results['cer_by_doc'].items():
        print(f"{doc_type} - CER: {cer:.4f}, WER: {results['wer_by_doc'][doc_type]:.4f}")

    # Create visualization
    viz_path = os.path.join(output_dir, "error_rates_by_doc.png")
    visualize_ocr_results(results, viz_path)

    # Show some sample predictions
    print("\nSample predictions:")
    sample_counter = 0
    for doc_type, samples in results['samples'].items():
        for sample in samples:
            if sample_counter < 5:  # Limit to 5 samples
                print(f"\nDocument type: {doc_type}")
                print(f"Reference: {sample['reference'][:100]}...")
                print(f"Prediction: {sample['prediction'][:100]}...")
                print(f"CER: {sample['cer']:.4f}, WER: {sample['wer']:.4f}")
                sample_counter += 1

    return results

"""# Cell 10"""

def run_ocr_pipeline(max_pages_per_pdf=5, evaluate_model=True, fine_tune=False, num_epochs=3):
    """
    Run the complete OCR pipeline

    Args:
        max_pages_per_pdf: Maximum number of pages to process per PDF
        evaluate_model: Whether to evaluate a pre-trained model
        fine_tune: Whether to fine-tune a model
        num_epochs: Number of epochs for fine-tuning

    Returns:
        Dictionary with pipeline results
    """
    print("Step 1: Upload PDFs")
    uploaded_paths = upload_pdfs_to_colab()

    if not uploaded_paths:
        print("No PDFs uploaded. Exiting.")
        return None

    print(f"\nUploaded {len(uploaded_paths)} PDFs")

    print("\nStep 2: Process PDFs")
    document_images = process_all_pdfs(
        pdf_folder,
        output_base_path,
        dpi=300,
        max_pages_per_pdf=max_pages_per_pdf
    )

    # Check if we got any processed images
    total_processed = sum(len(paths) for paths in document_images.values())

    if total_processed == 0:
        print("\nNo images were processed. Exiting.")
        return None

    print(f"\nProcessed {total_processed} images from {len(document_images)} documents")

    print("\nStep 3: Creating transcriptions")
    create_dummy_transcriptions(document_images, transcriptions_path)

    # Collect all processed image paths
    print("\nStep 4: Collecting processed image paths")
    all_processed_images = []
    for doc_id, image_paths in document_images.items():
        all_processed_images.extend(image_paths)

    print(f"Collected {len(all_processed_images)} processed images")

    # Load transcriptions
    print("\nStep 5: Loading transcriptions")
    transcriptions = load_transcriptions(transcriptions_path, all_processed_images)

    # Create train-validation split
    print("\nStep 6: Creating train-validation split")
    train_image_paths, val_image_paths = create_train_val_split(
        all_processed_images,
        transcriptions,
        val_ratio=0.2
    )

    print(f"Train set: {len(train_image_paths)} images")
    print(f"Validation set: {len(val_image_paths)} images")

    # Create data loaders
    print("\nStep 7: Creating data loaders")
    train_loader, val_loader = create_data_loaders(
        train_image_paths,
        val_image_paths,
        transcriptions,
        batch_size=4
    )

    # Show examples
    print("\nStep 8: Showing example images")
    save_example_images(document_images, output_base_path, num_examples=2)

    # Store results
    results = {
        "document_images": document_images,
        "transcriptions": transcriptions,
        "train_loader": train_loader,
        "val_loader": val_loader,
        "train_image_paths": train_image_paths,
        "val_image_paths": val_image_paths
    }

    # Evaluate model if requested
    if evaluate_model:
        print("\nStep 9: Evaluating OCR model")
        eval_results = run_ocr_evaluation(
            val_loader=val_loader,
            transcriptions=transcriptions,
            output_dir=results_path
        )
        results["evaluation_results"] = eval_results

    # Fine-tune model if requested
    if fine_tune:
        print("\nStep 10: Fine-tuning OCR model")
        model, processor, ft_results = fine_tune_trocr_model(
            train_loader=train_loader,
            val_loader=val_loader,
            output_dir=os.path.join(output_base_path, "fine_tuned_model"),
            num_epochs=num_epochs
        )
        results["fine_tuned_model"] = model
        results["fine_tuned_processor"] = processor
        results["fine_tuning_results"] = ft_results

    print("\nOCR pipeline completed successfully!")
    return results

# Main execution
if __name__ == "__main__":
    # Print welcome message
    print("=" * 80)
    print("Historical Document OCR Pipeline")
    print("=" * 80)
    print("\nThis pipeline processes historical Spanish documents using OCR technology.")
    print("You'll be prompted to upload up to 6 PDF files of historical documents.")
    print("\nThe pipeline will:")
    print("1. Convert the PDFs to images")
    print("2. Preprocess the images for OCR")
    print("3. Evaluate OCR accuracy using TrOCR models")
    print("4. Optionally fine-tune a model for your specific documents")

    # Ask for user preferences
    max_pages = int(input("\nMaximum pages to process per PDF (recommended: 5-10): ") or "5")
    evaluate = input("Evaluate OCR model? (y/n, default: y): ").lower() != 'n'
    fine_tune = input("Fine-tune OCR model? This may take significant time. (y/n, default: n): ").lower() == 'y'

    if fine_tune:
        num_epochs = int(input("Number of fine-tuning epochs (recommended: 3-5): ") or "3")
    else:
        num_epochs = 3

    # Run the pipeline
    results = run_ocr_pipeline(
        max_pages_per_pdf=max_pages,
        evaluate_model=evaluate,
        fine_tune=fine_tune,
        num_epochs=num_epochs
    )

    if results:
        print("\nPipeline completed successfully!")
        print("Results are saved in:", results_path)
    else:
        print("\nPipeline did not complete successfully. Please check the errors above.")

"""# Cell 11

"""

# Run the pipeline with default parameters
# Or you can customize the parameters as needed
max_pages = 6  # Process up to 6 pages per PDF
evaluate = True  # Evaluate the OCR model
fine_tune = True  # Fine-tune the model
num_epochs = 3  # Number of fine-tuning epochs

# Execute the pipeline
results = run_ocr_pipeline(
    max_pages_per_pdf=max_pages,
    evaluate_model=evaluate,
    fine_tune=fine_tune,
    num_epochs=num_epochs
)