# -*- coding: utf-8 -*-
"""test_ocr_10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lr2ix2mnTlmMkX5BuP7Et8WXPRpjEDwm
"""

# Fix for missing datasets library
!pip install transformers datasets

"""#Cell 1: Setup and Installation


"""

# Install system packages first (this is the critical fix - use apt-get for poppler-utils)
!apt-get update
!apt-get install -y poppler-utils

# Install ALL necessary Python packages at the start
!pip install pdf2image pytesseract opencv-python matplotlib tqdm
!pip install torch torchvision
!pip install transformers datasets  # Install datasets library early
!pip install PyPDF2 pillow seaborn pandas

# Import libraries
import os
import glob
import re
import random
import warnings
import shutil
import math
import string
import numpy as np
import cv2
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw  # Import ImageDraw explicitly
from tqdm.notebook import tqdm
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import pdf2image
import difflib
from collections import Counter
import pandas as pd
import seaborn as sns

# Import transformers libraries right at the start
try:
    from transformers import TrOCRProcessor, VisionEncoderDecoderModel
    from datasets import Dataset
    print("Successfully imported transformers and datasets libraries")
except ImportError:
    print("Error importing transformers or datasets. Installing again...")
    !pip install transformers datasets
    from transformers import TrOCRProcessor, VisionEncoderDecoderModel
    from datasets import Dataset

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

# Set random seeds for reproducibility
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)

print("Libraries imported successfully!")

# Check for available GPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Define base paths
base_path = '/content'
pdf_folder = os.path.join(base_path, 'pdfs')
output_base_path = os.path.join(base_path, 'ocr_data')
transcriptions_path = os.path.join(base_path, 'transcriptions')
training_data_path = os.path.join(output_base_path, 'training_data')

# Create necessary directories
os.makedirs(pdf_folder, exist_ok=True)
os.makedirs(os.path.join(output_base_path, "images"), exist_ok=True)
os.makedirs(os.path.join(output_base_path, "processed_images"), exist_ok=True)
os.makedirs(os.path.join(output_base_path, "binary_images"), exist_ok=True)
os.makedirs(training_data_path, exist_ok=True)
os.makedirs(transcriptions_path, exist_ok=True)

print("Setup complete! All directories created.")

"""#Cell 2: PDF Upload and Conversion Functions


"""

def convert_pdf_to_images(pdf_path, output_folder, dpi=200, first_page=None, last_page=None):
    """
    Convert PDF pages to images with robust error handling and memory limits

    Args:
        pdf_path: Path to the PDF file
        output_folder: Folder to save the images
        dpi: Resolution in DPI (dots per inch) - reduced from 300 to 200 for large PDFs
        first_page: First page to convert (1-based index)
        last_page: Last page to convert (1-based index)

    Returns:
        List of paths to the saved images
    """
    # Create output folder if it doesn't exist
    os.makedirs(output_folder, exist_ok=True)

    # Get the PDF filename without extension
    pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]

    # Check file size to adjust DPI for large files
    file_size_mb = os.path.getsize(pdf_path) / (1024 * 1024)

    # Reduce DPI for large files to avoid memory issues
    adjusted_dpi = dpi
    if file_size_mb > 20:  # If larger than 20MB
        adjusted_dpi = min(dpi, 150)  # Reduce to 150 DPI max
    if file_size_mb > 50:  # If larger than 50MB
        adjusted_dpi = min(dpi, 100)  # Reduce to 100 DPI max

    if adjusted_dpi != dpi:
        print(f"Large PDF detected ({file_size_mb:.1f} MB). Reducing DPI from {dpi} to {adjusted_dpi}")

    try:
        # First check if poppler is properly installed
        import subprocess
        try:
            # Check if pdftoppm (part of poppler) is available
            subprocess.run(["pdftoppm", "-v"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
        except (subprocess.SubprocessError, FileNotFoundError):
            print("Poppler not found in PATH. Attempting to use absolute path...")
            # Try with potential Colab poppler paths
            poppler_path = "/usr/bin"  # Default path in Colab after apt-get install

            # Set environment path for pdf2image
            os.environ["PATH"] += os.pathsep + poppler_path

        # Use pdf2image with memory limit protections
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")

                # Use a more memory-efficient approach for large PDFs
                if file_size_mb > 20:
                    print(f"Using single-page conversion for large PDF: {pdf_filename}")

                    # Process one page at a time for large PDFs
                    start_page = first_page if first_page else 1
                    end_page = last_page if last_page else float('inf')

                    # Get actual page count if end_page is infinity
                    if end_page == float('inf'):
                        from PyPDF2 import PdfReader
                        reader = PdfReader(pdf_path)
                        end_page = len(reader.pages)

                    image_paths = []
                    for page_num in range(start_page, end_page + 1):
                        try:
                            # Process single page with reduced memory usage
                            single_image = pdf2image.convert_from_path(
                                pdf_path,
                                dpi=adjusted_dpi,
                                first_page=page_num,
                                last_page=page_num,
                                use_pdftocairo=True,  # More memory efficient
                                size=(1000, None)  # Limit width to control memory usage
                            )

                            if single_image and len(single_image) > 0:
                                image_path = os.path.join(output_folder, f"{pdf_filename}_page_{page_num:03d}.jpg")
                                single_image[0].save(image_path, "JPEG", quality=85)  # Lower quality to save memory
                                image_paths.append(image_path)
                            else:
                                print(f"No image extracted for page {page_num}")
                        except Exception as e:
                            print(f"Error processing page {page_num}: {str(e)}")
                            # Continue with next page

                    return image_paths
                else:
                    # Normal processing for regular PDFs
                    images = pdf2image.convert_from_path(
                        pdf_path,
                        dpi=adjusted_dpi,
                        first_page=first_page,
                        last_page=last_page,
                        use_pdftocairo=True
                    )

            image_paths = []
            for i, image in enumerate(images):
                # Determine the page number (1-based index)
                page_num = i + 1 if first_page is None else first_page + i

                # Save the image with reduced quality for large files
                image_path = os.path.join(output_folder, f"{pdf_filename}_page_{page_num:03d}.jpg")
                image.save(image_path, "JPEG", quality=90 if file_size_mb < 20 else 85)
                image_paths.append(image_path)

            print(f"Successfully converted {len(image_paths)} pages from {pdf_filename}")
            return image_paths

        except Exception as pil_error:
            print(f"PIL Error for {pdf_filename}: {str(pil_error)}. Using alternative approach...")
            # Fall through to alternative approach
            raise

    except Exception as e:
        print(f"Error converting PDF {pdf_path}: {str(e)}")

        # Try alternative conversion method using PyPDF2
        try:
            print("Attempting alternative conversion method...")
            from PyPDF2 import PdfReader
            from PIL import Image, ImageDraw, ImageFont

            # Get page count
            reader = PdfReader(pdf_path)
            page_count = len(reader.pages)

            # Set page range
            start_page = first_page if first_page else 1
            end_page = min(last_page if last_page else page_count, page_count)

            # Create placeholder images with page numbers
            image_paths = []
            for page_num in range(start_page, end_page + 1):
                # Create a blank image for this page (reduced size for memory savings)
                blank_img = Image.new('RGB', (800, 1000), color='white')
                draw = ImageDraw.Draw(blank_img)

                # Add page number text
                draw.text((400, 500), f"Page {page_num} - {pdf_filename}", fill='black')
                draw.text((400, 550), "(PDF conversion failed - placeholder image)", fill='black')

                # Save the image
                image_path = os.path.join(output_folder, f"{pdf_filename}_page_{page_num:03d}.jpg")
                blank_img.save(image_path, "JPEG", quality=85)
                image_paths.append(image_path)

            print(f"Created {len(image_paths)} placeholder images for {pdf_filename}")
            return image_paths

        except Exception as backup_error:
            print(f"Alternative conversion also failed: {str(backup_error)}")
            return []

def upload_pdfs_to_colab():
    """
    Upload PDF files to Google Colab

    Returns:
        List of uploaded file paths
    """
    from google.colab import files
    import shutil

    print("Please upload your PDF files (you can select multiple files)...")

    # Create the PDF folder if it doesn't exist
    pdf_folder = '/content/pdfs'
    os.makedirs(pdf_folder, exist_ok=True)

    # Upload files
    uploaded = files.upload()

    # Move uploaded files to the PDF folder
    uploaded_paths = []
    for filename, content in uploaded.items():
        # Check if the file is a PDF
        if not filename.lower().endswith('.pdf'):
            print(f"Warning: {filename} is not a PDF file. Skipping...")
            continue

        # Write the file to the PDF folder
        filepath = os.path.join(pdf_folder, filename)
        with open(filepath, 'wb') as f:
            f.write(content)

        uploaded_paths.append(filepath)
        print(f"Uploaded: {filename} -> {filepath}")

    return uploaded_paths

def check_for_large_pdfs(pdf_folder, size_threshold_mb=50):
    """
    Check for large PDF files that might cause memory issues

    Args:
        pdf_folder: Folder containing the PDFs
        size_threshold_mb: Size threshold in MB

    Returns:
        List of large PDF files
    """
    large_pdfs = []

    for pdf_file in glob.glob(os.path.join(pdf_folder, "*.pdf")):
        # Get file size in MB
        size_mb = os.path.getsize(pdf_file) / (1024 * 1024)

        if size_mb > size_threshold_mb:
            large_pdfs.append((pdf_file, size_mb))

    return large_pdfs

def handle_file_uploads():
    """
    Handle file uploads and prepare for processing

    Returns:
        Path to the folder containing the PDFs
    """
    # Check if we're running in Colab
    try:
        import google.colab
        in_colab = True
    except ImportError:
        in_colab = False

    pdf_folder = '/content/pdfs'
    os.makedirs(pdf_folder, exist_ok=True)

    if in_colab:
        print("Running in Google Colab")
        print("\nYou have two options for uploading PDFs:")
        print("1. Upload directly from your computer")
        print("2. Use files already in Google Drive")

        choice = input("Enter your choice (1 or 2): ")

        if choice == '1':
            print("\nUploading PDFs directly...")
            upload_pdfs_to_colab()
        else:
            print("\nUsing files from Google Drive...")

            # Mount Google Drive if not already mounted
            try:
                from google.colab import drive
                drive.mount('/content/drive')
            except:
                print("Google Drive already mounted")

            # Ask for the folder path
            drive_folder = input("Enter the path to your PDF folder in Google Drive (e.g., 'MyDrive/PDFs'): ")
            drive_path = os.path.join('/content/drive', drive_folder)

            if not os.path.exists(drive_path):
                print(f"Error: The path {drive_path} does not exist!")
                return pdf_folder

            # Copy PDFs from Google Drive to the local folder
            for pdf_file in glob.glob(os.path.join(drive_path, "*.pdf")):
                filename = os.path.basename(pdf_file)
                local_path = os.path.join(pdf_folder, filename)
                shutil.copy(pdf_file, local_path)
                print(f"Copied: {filename} -> {local_path}")
    else:
        print("Not running in Google Colab")
        print("Please place your PDF files in the folder:", pdf_folder)

    # Check if we have any PDFs
    pdf_files = glob.glob(os.path.join(pdf_folder, "*.pdf"))
    print(f"\nFound {len(pdf_files)} PDF files in {pdf_folder}")

    if len(pdf_files) == 0:
        print("Error: No PDF files found!")
        return pdf_folder

    # Print the list of PDFs
    for pdf_file in pdf_files:
        print(f" - {os.path.basename(pdf_file)}")

    # Check for large PDFs
    large_pdfs = check_for_large_pdfs(pdf_folder)
    if large_pdfs:
        print("\nWarning: The following PDFs are large and may cause memory issues:")
        for pdf_file, size_mb in large_pdfs:
            print(f" - {os.path.basename(pdf_file)} ({size_mb:.1f} MB)")

        print("\nThese large PDFs will be processed in chunks to avoid memory errors.")

    return pdf_folder

print("PDF handling functions defined successfully!")

"""# Cell 3: Image Preprocessing Functions


"""

def correct_skew(image, delta=0.5, limit=5):
    """
    Correct skew in images using Hough Line Transform

    Args:
        image: Grayscale image
        delta: Angle step size for scoring
        limit: Maximum angle to check

    Returns:
        Deskewed image
    """
    # Create edges image for better line detection
    edges = cv2.Canny(image, 50, 150, apertureSize=3)

    # Try to detect lines using Hough Transform
    lines = cv2.HoughLines(edges, 1, np.pi/180, 100)

    if lines is not None:
        # Calculate the angle histogram
        angle_counts = {}
        for line in lines:
            rho, theta = line[0]
            # Convert radians to degrees and normalize to [-90, 90]
            angle = (theta * 180 / np.pi) % 180
            if angle > 90:
                angle = angle - 180

            # Bin the angles (rounded to nearest integer)
            angle_key = round(angle)
            angle_counts[angle_key] = angle_counts.get(angle_key, 0) + 1

        # Find the angle with the most occurrences
        if angle_counts:
            max_angle = max(angle_counts, key=angle_counts.get)

            # Only correct if the angle is within reasonable limits
            if abs(max_angle) <= limit:
                # Correct 90 degree offset for vertical lines
                if abs(max_angle) > 45:
                    skew_angle = 90 - abs(max_angle)
                    if max_angle > 0:
                        skew_angle = -skew_angle
                else:
                    skew_angle = -max_angle

                # Rotate the image
                (h, w) = image.shape[:2]
                center = (w // 2, h // 2)
                M = cv2.getRotationMatrix2D(center, skew_angle, 1.0)
                return cv2.warpAffine(
                    image, M, (w, h),
                    flags=cv2.INTER_CUBIC,
                    borderMode=cv2.BORDER_REPLICATE
                )

    # If line detection fails, try projection profile method
    scores = []
    angles = np.arange(-limit, limit + delta, delta)

    for angle in angles:
        # Rotate image
        (h, w) = image.shape[:2]
        center = (w // 2, h // 2)
        M = cv2.getRotationMatrix2D(center, angle, 1.0)
        rotated = cv2.warpAffine(
            image, M, (w, h),
            flags=cv2.INTER_CUBIC,
            borderMode=cv2.BORDER_REPLICATE
        )

        # Sum the pixel values along each row
        projection = np.sum(rotated, axis=1, dtype=np.float32)

        # Calculate the score (variance of the projection)
        score = np.var(projection)
        scores.append(score)

    # Get the angle with the highest score
    best_angle = angles[np.argmax(scores)]

    # Rotate the image with the best angle
    (h, w) = image.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, best_angle, 1.0)
    rotated = cv2.warpAffine(
        image, M, (w, h),
        flags=cv2.INTER_CUBIC,
        borderMode=cv2.BORDER_REPLICATE
    )

    return rotated

def preprocess_image(image_path, output_folder, binary_folder):
    """
    Preprocess image for OCR:
    1. Convert to grayscale
    2. Apply contrast enhancement (CLAHE)
    3. Denoise the image
    4. Correct skew
    5. Apply adaptive thresholding

    Args:
        image_path: Path to the input image
        output_folder: Folder to save the preprocessed image
        binary_folder: Folder to save the binary image

    Returns:
        Tuple of (processed_image_path, binary_image_path)
    """
    # Create output folders if they don't exist
    os.makedirs(output_folder, exist_ok=True)
    os.makedirs(binary_folder, exist_ok=True)

    # Get the image filename
    image_filename = os.path.basename(image_path)

    # Read the image
    img = cv2.imread(image_path)
    if img is None:
        print(f"Failed to read image {image_path}")
        return None, None

    # Convert to grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Apply contrast enhancement (CLAHE)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    enhanced = clahe.apply(gray)

    # Denoise
    denoised = cv2.fastNlMeansDenoising(enhanced, h=10)

    # Detect and correct skew
    corrected = correct_skew(denoised)

    # Apply adaptive thresholding to create binary image
    # This helps to separate text from background
    binary = cv2.adaptiveThreshold(
        corrected,
        255,
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY,
        15,  # Block size
        9    # Constant subtracted from the mean
    )

    # Save processed image
    processed_image_path = os.path.join(output_folder, image_filename)
    cv2.imwrite(processed_image_path, corrected)

    # Save binary image
    binary_image_path = os.path.join(binary_folder, image_filename)
    cv2.imwrite(binary_image_path, binary)

    return processed_image_path, binary_image_path

def process_all_pdfs(pdf_folder, output_base_path, dpi=300, max_pages_per_pdf=None):
    """
    Process all PDFs in a folder with improved error handling and recovery

    Args:
        pdf_folder: Folder containing the PDFs
        output_base_path: Base folder to save the output
        dpi: Resolution in DPI (dots per inch)
        max_pages_per_pdf: Maximum number of pages to process per PDF (None for all)

    Returns:
        Dictionary mapping document IDs to lists of processed image paths
    """
    # Define output folders
    images_folder = os.path.join(output_base_path, "images")
    processed_folder = os.path.join(output_base_path, "processed_images")
    binary_folder = os.path.join(output_base_path, "binary_images")

    # Create output folders if they don't exist
    os.makedirs(images_folder, exist_ok=True)
    os.makedirs(processed_folder, exist_ok=True)
    os.makedirs(binary_folder, exist_ok=True)

    # Get all PDF files
    pdf_files = glob.glob(os.path.join(pdf_folder, "*.pdf"))
    print(f"Found {len(pdf_files)} PDF files")

    # Check if we have any duplicate filenames (different only by extension or path)
    pdf_basenames = [os.path.splitext(os.path.basename(f))[0] for f in pdf_files]
    duplicate_basenames = [name for name in set(pdf_basenames) if pdf_basenames.count(name) > 1]

    if duplicate_basenames:
        print(f"WARNING: Found duplicate PDF filenames (ignoring extensions): {duplicate_basenames}")
        print("Processing only one copy of each duplicate.")

        # Filter out duplicates, keeping only one copy of each
        unique_pdfs = []
        processed_basenames = set()

        for pdf_file in pdf_files:
            basename = os.path.splitext(os.path.basename(pdf_file))[0]
            if basename not in processed_basenames:
                unique_pdfs.append(pdf_file)
                processed_basenames.add(basename)

        pdf_files = unique_pdfs
        print(f"Will process {len(pdf_files)} unique PDFs")

    # Dictionary to store document ID to image paths mapping
    document_images = {}

    # Process each PDF
    for pdf_file in tqdm(pdf_files, desc="Processing PDFs"):
        pdf_filename = os.path.splitext(os.path.basename(pdf_file))[0]
        print(f"\nProcessing {pdf_filename}")

        # Create folders for this PDF
        pdf_images_folder = os.path.join(images_folder, pdf_filename)
        pdf_processed_folder = os.path.join(processed_folder, pdf_filename)
        pdf_binary_folder = os.path.join(binary_folder, pdf_filename)

        os.makedirs(pdf_images_folder, exist_ok=True)
        os.makedirs(pdf_processed_folder, exist_ok=True)
        os.makedirs(pdf_binary_folder, exist_ok=True)

        # Handle large PDFs with a different approach
        if "PORCONES" in pdf_filename or os.path.getsize(pdf_file) > 10*1024*1024:  # > 10MB
            print(f"Large PDF detected: {pdf_filename}. Processing in chunks...")

            # Process large PDF in chunks of 5 pages
            chunk_size = 5
            first_page = 1
            all_image_paths = []

            # Try to get total page count first
            try:
                from PyPDF2 import PdfReader
                reader = PdfReader(pdf_file)
                total_pages = len(reader.pages)
                max_page = min(total_pages, max_pages_per_pdf) if max_pages_per_pdf else total_pages

                print(f"PDF has {total_pages} pages. Processing up to page {max_page}.")

                while first_page <= max_page:
                    last_page = min(first_page + chunk_size - 1, max_page)
                    chunk_image_paths = convert_pdf_to_images(
                        pdf_file,
                        pdf_images_folder,
                        dpi=dpi,
                        first_page=first_page,
                        last_page=last_page
                    )

                    if not chunk_image_paths:
                        # Try with a smaller chunk if failed
                        if chunk_size > 1:
                            print(f"Trying with smaller chunk size for pages {first_page}-{last_page}...")
                            chunk_size = 1
                            continue
                        else:
                            # If even single page processing fails, skip to next chunk
                            print(f"Failed to process pages {first_page}-{last_page}, skipping.")
                            first_page += chunk_size
                            continue

                    all_image_paths.extend(chunk_image_paths)
                    first_page += chunk_size

                # Store the image paths
                document_images[pdf_filename] = all_image_paths

            except Exception as e:
                print(f"Error determining page count: {str(e)}")
                # Fallback: just try to process first 10 pages
                chunk_image_paths = convert_pdf_to_images(
                    pdf_file,
                    pdf_images_folder,
                    dpi=dpi,
                    first_page=1,
                    last_page=10
                )
                document_images[pdf_filename] = chunk_image_paths if chunk_image_paths else []
        else:
            # Convert PDF to images
            image_paths = convert_pdf_to_images(
                pdf_file,
                pdf_images_folder,
                dpi=dpi,
                first_page=1,
                last_page=max_pages_per_pdf
            )

            # Store the image paths
            document_images[pdf_filename] = image_paths if image_paths else []

        # Check if we got any images for this document
        if not document_images[pdf_filename]:
            print(f"WARNING: No images were extracted from {pdf_filename}")
            continue

        # Preprocess each image
        processed_image_paths = []
        for image_path in tqdm(document_images[pdf_filename], desc=f"Preprocessing images for {pdf_filename}"):
            try:
                processed_path, _ = preprocess_image(
                    image_path,
                    pdf_processed_folder,
                    pdf_binary_folder
                )
                if processed_path:
                    processed_image_paths.append(processed_path)
            except Exception as e:
                print(f"Error preprocessing {image_path}: {str(e)}")
                # Continue with next image

        # Update the document images dictionary
        document_images[pdf_filename] = processed_image_paths

    # Check if we processed any images successfully
    total_processed = sum(len(paths) for paths in document_images.values())
    if total_processed == 0:
        print("\nWARNING: No PDFs were successfully processed.")
        print("Please make sure poppler-utils is installed correctly:")
        print("!apt-get update && apt-get install -y poppler-utils")

        # Create a dummy image for testing if no real images were processed
        dummy_folder = os.path.join(images_folder, "dummy")
        os.makedirs(dummy_folder, exist_ok=True)

        dummy_processed_folder = os.path.join(processed_folder, "dummy")
        os.makedirs(dummy_processed_folder, exist_ok=True)

        dummy_binary_folder = os.path.join(binary_folder, "dummy")
        os.makedirs(dummy_binary_folder, exist_ok=True)

        # Create dummy images for testing
        dummy_images = []
        for i in range(3):
            # Create a test image with text
            from PIL import Image, ImageDraw, ImageFont
            img = Image.new('RGB', (1000, 1414), color='white')
            draw = ImageDraw.Draw(img)

            # Add page number and dummy text
            draw.text((100, 100), f"Test Page {i+1}", fill='black')
            draw.text((100, 200), "This is a dummy image created because PDF processing failed.", fill='black')
            draw.text((100, 300), "Please check the poppler installation in your environment.", fill='black')

            # Save the image
            dummy_path = os.path.join(dummy_folder, f"dummy_page_{i+1:03d}.jpg")
            img.save(dummy_path)

            # Process the dummy image
            processed_path, binary_path = preprocess_image(
                dummy_path,
                dummy_processed_folder,
                dummy_binary_folder
            )

            if processed_path:
                dummy_images.append(processed_path)

        if dummy_images:
            document_images["dummy"] = dummy_images
            print(f"Created {len(dummy_images)} dummy images for testing purposes.")

    return document_images

def visualize_preprocessing(original_path, processed_path, binary_path):
    """
    Visualize the preprocessing steps

    Args:
        original_path: Path to the original image
        processed_path: Path to the processed image
        binary_path: Path to the binary image
    """
    # Read the images
    original = cv2.imread(original_path)
    processed = cv2.imread(processed_path, cv2.IMREAD_GRAYSCALE)
    binary = cv2.imread(binary_path, cv2.IMREAD_GRAYSCALE)

    # Convert original to RGB for display
    original_rgb = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)

    # Create a figure with 3 subplots
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # Display the images
    axes[0].imshow(original_rgb)
    axes[0].set_title("Original Image")
    axes[0].axis("off")

    axes[1].imshow(processed, cmap="gray")
    axes[1].set_title("Processed Image")
    axes[1].axis("off")

    axes[2].imshow(binary, cmap="gray")
    axes[2].set_title("Binary Image")
    axes[2].axis("off")

    # Show the figure
    plt.tight_layout()
    plt.show()

def save_example_images(document_images, output_base_path, num_examples=3):
    """
    Save example images from different preprocessing stages for visualization

    Args:
        document_images: Dictionary mapping document IDs to processed image paths
        output_base_path: Base folder for outputs
        num_examples: Number of examples to save
    """
    # Create example folder
    example_folder = os.path.join(output_base_path, "examples")
    os.makedirs(example_folder, exist_ok=True)

    # Collect a few examples
    examples = []

    for doc_id, image_paths in document_images.items():
        # Take the first few images from each document
        for i, processed_path in enumerate(image_paths[:num_examples]):
            # Get the original and binary image paths
            original_path = processed_path.replace("processed_images", "images")
            binary_path = processed_path.replace("processed_images", "binary_images")

            if os.path.exists(original_path) and os.path.exists(binary_path):
                examples.append((original_path, processed_path, binary_path))

    # Visualize the examples
    for original_path, processed_path, binary_path in examples:
        visualize_preprocessing(original_path, processed_path, binary_path)

print("Image preprocessing functions defined successfully!")

"""# Cell 4: Dataset Class and Utilities


"""

import numpy as np
import torch
from PIL import Image
from datasets import Dataset

class OCRDataset(torch.utils.data.Dataset):
    """
    Dataset class for OCR training.
    """
    def __init__(self, image_paths, transcriptions, transform=None, max_length=512):
        """
        Initialize the dataset.
        Args:
            image_paths: List of image paths.
            transcriptions: Dictionary mapping image paths to transcriptions.
            transform: Optional transform to be applied to the images.
            max_length: Maximum sequence length for the transcriptions.
        """
        self.image_paths = image_paths
        self.transcriptions = transcriptions
        self.transform = transform
        self.max_length = max_length

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        # Handle batched indices if idx is a list
        if isinstance(idx, list):
            return [self.__getitem__(i) for i in idx]

        # Get image path for the given index
        image_path = self.image_paths[idx]
        # Load image and apply transform if available
        image = Image.open(image_path).convert("RGB")
        if self.transform:
            image = self.transform(image)

        # Get transcription and truncate if necessary
        text = self.transcriptions.get(image_path, "")
        if len(text) > self.max_length:
            text = text[:self.max_length]

        return {"image": image, "text": text, "image_path": image_path}


def prepare_dataset(batch):
    """
    Prepare batch data for TrOCR model training with robust image conversion.

    Args:
        batch: Batch of data from the dataset.

    Returns:
        Processed batch with pixel_values and labels.
    """
    # Handle images with proper format conversion
    raw_images = batch["image"]
    processed_images = []

    for img in raw_images:
        # If img is a nested list, extract the first element
        if isinstance(img, list):
            img = img[0]

        # Convert from numpy array (stored in dataset) to PIL Image
        if isinstance(img, np.ndarray):
            if img.ndim == 3:  # RGB image
                pil_img = Image.fromarray(img.astype('uint8'))
            elif img.ndim == 2:  # Grayscale image
                pil_img = Image.fromarray(np.repeat(img[:, :, np.newaxis], 3, axis=2).astype('uint8'))
            else:
                # Reshape if needed
                if img.size == 384 * 384 * 3:
                    img_reshaped = img.reshape((384, 384, 3))
                else:
                    img_reshaped = img.reshape((384, 384))
                if img_reshaped.ndim == 2:  # Grayscale
                    pil_img = Image.fromarray(np.repeat(img_reshaped[:, :, np.newaxis], 3, axis=2).astype('uint8'))
                else:  # RGB
                    pil_img = Image.fromarray(img_reshaped.astype('uint8'))
        elif isinstance(img, Image.Image):
            pil_img = img
        else:
            # Last resort - create a blank image
            pil_img = Image.new('RGB', (384, 384), color='white')
            print(f"Warning: Unhandled image type {type(img)}, created blank image")

        processed_images.append(pil_img)

    # Process the images with the TrOCR processor (make sure 'processor' is defined globally)
    pixel_values = processor(images=processed_images, return_tensors="pt").pixel_values

    # Tokenize the texts
    texts = batch["text"]
    labels = processor.tokenizer(texts, padding="max_length", truncation=True).input_ids

    return {"pixel_values": pixel_values, "labels": labels}


def convert_dataloader_to_dataset(data_loader):
    """
    Convert PyTorch DataLoader to HuggingFace Dataset with improved error handling.

    Args:
        data_loader: PyTorch DataLoader.

    Returns:
        HuggingFace Dataset.
    """
    all_data = []

    for batch in data_loader:
        for i in range(len(batch["image"])):
            # Convert tensor to numpy safely
            if torch.is_tensor(batch["image"][i]):
                # Transpose from [C, H, W] to [H, W, C] if needed
                if batch["image"][i].dim() == 3 and batch["image"][i].shape[0] == 3:
                    img_np = batch["image"][i].permute(1, 2, 0).numpy()
                else:
                    img_np = batch["image"][i].numpy()

                # Normalize to 0-255 range for PIL compatibility
                if img_np.max() <= 1.0:
                    img_np = (img_np * 255).astype('uint8')
            else:
                # Just in case it's already a numpy array
                img_np = batch["image"][i]

            item = {
                "image": img_np,
                "text": batch["text"][i],
                "image_path": batch["image_path"][i]
            }
            all_data.append(item)

    # Verify we have data before creating the dataset
    if not all_data:
        print("WARNING: No data found in DataLoader")
        dummy_img = np.zeros((384, 384, 3), dtype=np.uint8)
        all_data = [{"image": dummy_img, "text": "dummy text", "image_path": "dummy_path"}]

    return Dataset.from_list(all_data)

"""# Cell 5: Spanish Historical Text Utilities


"""

def normalize_historical_spanish(text):
    """
    Normalize historical Spanish text

    This handles common variations in early modern Spanish typography:
    - Long s (ſ) -> s
    - Ligatures like æ -> ae
    - U/V variations (often interchangeable in early texts)
    - I/J variations (often interchangeable in early texts)
    - Double consonants
    - Contractions and abbreviations

    Args:
        text: Input text

    Returns:
        Normalized text
    """
    # Replace long s with regular s
    text = text.replace('ſ', 's')

    # Replace ligatures
    text = text.replace('æ', 'ae').replace('œ', 'oe')

    # Standardize u/v variations (optional - depending on your transcription standards)
    # text = text.replace('v', 'u')  # For medieval/early modern texts where v was often used as u

    # Handle common abbreviations in historical Spanish
    # This is a simplified example - a complete list would be much longer
    abbreviations = {
        'q̃': 'que',
        'ẽ': 'en',
        'õ': 'on',
        'ñ': 'nn',  # In some early texts
        'ȷ': 'i',    # dotless i
    }

    for abbr, full in abbreviations.items():
        text = text.replace(abbr, full)

    return text

def extract_main_text(text):
    """
    Extract main text from the transcription, removing marginalia and notes

    Args:
        text: Input transcription

    Returns:
        Main text
    """
    # This is a simplified example - in practice, you would need more sophisticated rules
    # based on the specific formatting of your transcriptions

    # Remove lines starting with common marginalia markers
    lines = text.split('\n')
    filtered_lines = []

    in_marginalia = False
    for line in lines:
        # Skip lines that look like marginalia
        if line.strip().startswith('[') and line.strip().endswith(']'):
            continue

        # Skip lines that look like notes or editorial comments
        if line.strip().startswith('(') and line.strip().endswith(')'):
            continue

        # Handle multi-line marginalia blocks
        if line.strip().startswith('/*'):
            in_marginalia = True
            continue

        if in_marginalia:
            if line.strip().endswith('*/'):
                in_marginalia = False
            continue

        filtered_lines.append(line)

    return '\n'.join(filtered_lines)

def create_lexicon_from_transcriptions(transcriptions, min_word_length=2):
    """
    Create a lexicon from the transcriptions to help with post-processing

    Args:
        transcriptions: Dictionary mapping image paths to transcriptions
        min_word_length: Minimum word length to include in the lexicon

    Returns:
        Set of unique words
    """
    lexicon = set()

    for text in transcriptions.values():
        # Normalize the text
        normalized_text = normalize_historical_spanish(text)

        # Extract main text
        main_text = extract_main_text(normalized_text)

        # Split into words and add to lexicon
        words = main_text.split()
        for word in words:
            # Clean the word
            clean_word = word.strip('.,;:!?()[]{}"\'-—')

            # Only add words that meet the minimum length
            if len(clean_word) >= min_word_length:
                lexicon.add(clean_word.lower())

    return lexicon

def augment_lexicon_with_variations(lexicon):
    """
    Augment the lexicon with common historical variations

    Args:
        lexicon: Set of unique words

    Returns:
        Augmented lexicon
    """
    augmented_lexicon = set(lexicon)

    # Common character substitutions in early modern Spanish
    substitutions = [
        ('v', 'u'),   # v/u variations
        ('u', 'v'),
        ('i', 'j'),   # i/j variations
        ('j', 'i'),
        ('y', 'i'),   # y/i variations
        ('i', 'y'),
        ('ç', 'z'),   # cedilla/z variations
        ('z', 'ç'),
        ('f', 'ff'),  # single/double consonant variations
        ('ff', 'f'),
        ('s', 'ss'),
        ('ss', 's'),
        ('n', 'ñ'),   # n/ñ variations
        ('ñ', 'n'),
    ]

    # Add variations to the lexicon
    for word in lexicon:
        for old, new in substitutions:
            if old in word:
                variation = word.replace(old, new)
                augmented_lexicon.add(variation)

    return augmented_lexicon

class SpanishHistoricalPostProcessor:
    """
    Post-processing class for OCR results on historical Spanish texts
    """
    def __init__(self, lexicon=None):
        """
        Initialize the post-processor

        Args:
            lexicon: Lexicon of valid words
        """
        self.lexicon = lexicon or set()

        # Add common Spanish articles, prepositions, etc. to ensure basic words are covered
        common_words = {
            'el', 'la', 'los', 'las',       # Articles
            'de', 'en', 'con', 'por', 'a',  # Prepositions
            'y', 'e', 'o', 'u',             # Conjunctions
            'que', 'como', 'si',            # Conjunctions/relative pronouns
            'no', 'ni',                     # Negation
        }
        self.lexicon.update(common_words)

    def correct_word(self, word, max_edit_distance=2):
        """
        Correct a word using the lexicon

        Args:
            word: Word to correct
            max_edit_distance: Maximum edit distance for correction

        Returns:
            Corrected word
        """
        # If the word is already in the lexicon, return it
        if word.lower() in self.lexicon:
            return word

        # If word is empty or too short, return it as is
        if len(word) < 2:
            return word

        # Simple edit distance function
        def levenshtein_distance(s1, s2):
            if len(s1) < len(s2):
                return levenshtein_distance(s2, s1)

            if len(s2) == 0:
                return len(s1)

            previous_row = range(len(s2) + 1)
            for i, c1 in enumerate(s1):
                current_row = [i + 1]
                for j, c2 in enumerate(s2):
                    insertions = previous_row[j + 1] + 1
                    deletions = current_row[j] + 1
                    substitutions = previous_row[j] + (c1 != c2)
                    current_row.append(min(insertions, deletions, substitutions))
                previous_row = current_row

            return previous_row[-1]

        # Find the closest word in the lexicon
        candidates = []
        for lex_word in self.lexicon:
            # Skip words with significantly different lengths
            if abs(len(lex_word) - len(word)) > max_edit_distance:
                continue

            # Calculate edit distance
            distance = levenshtein_distance(word.lower(), lex_word)

            # Only consider words within the maximum edit distance
            if distance <= max_edit_distance:
                candidates.append((lex_word, distance))

        # Sort candidates by edit distance
        candidates.sort(key=lambda x: x[1])

        # Return the closest match if any, otherwise return the original word
        return candidates[0][0] if candidates else word

    def process_text(self, text):
        """
        Process a complete OCR text

        Args:
            text: OCR text

        Returns:
            Processed text
        """
        # Normalize the text
        text = normalize_historical_spanish(text)

        # Split into words
        words = []
        for word in text.split():
            # Extract the word and its surrounding punctuation
            prefix = ""
            suffix = ""

            while word and not word[0].isalnum():
                prefix += word[0]
                word = word[1:]

            while word and not word[-1].isalnum():
                suffix = word[-1] + suffix
                word = word[:-1]

            # Correct the word if it's not empty
            if word:
                corrected_word = self.correct_word(word)
                words.append(prefix + corrected_word + suffix)
            else:
                words.append(prefix + suffix)

        # Join the words back into text
        return ' '.join(words)

print("Spanish historical text utilities defined successfully!")

"""# Cell 6: OCR Evaluation Metrics and Analysis


"""

def normalize_text(text):
    """
    Normalize text for evaluation:
    - Convert to lowercase
    - Remove punctuation
    - Remove extra whitespace

    Args:
        text: Input text

    Returns:
        Normalized text
    """
    # Convert to lowercase
    text = text.lower()

    # Remove punctuation
    import string
    translator = str.maketrans('', '', string.punctuation)
    text = text.translate(translator)

    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    return text

def character_error_rate(reference, hypothesis):
    """
    Calculate Character Error Rate (CER)

    CER = (S + D + I) / N
    Where:
    S = number of substitutions
    D = number of deletions
    I = number of insertions
    N = number of characters in reference

    Args:
        reference: Ground truth text
        hypothesis: OCR output text

    Returns:
        CER value (lower is better)
    """
    # Normalize texts
    reference = normalize_text(reference)
    hypothesis = normalize_text(hypothesis)

    # Compute Levenshtein distance
    distances = np.zeros((len(reference) + 1, len(hypothesis) + 1))

    # Initialize first row and column
    for i in range(len(reference) + 1):
        distances[i][0] = i
    for j in range(len(hypothesis) + 1):
        distances[0][j] = j

    # Fill distance matrix
    for i in range(1, len(reference) + 1):
        for j in range(1, len(hypothesis) + 1):
            if reference[i-1] == hypothesis[j-1]:
                distances[i][j] = distances[i-1][j-1]
            else:
                substitution = distances[i-1][j-1] + 1
                insertion = distances[i][j-1] + 1
                deletion = distances[i-1][j] + 1
                distances[i][j] = min(substitution, insertion, deletion)

    # Levenshtein distance is the value in the bottom right corner of the matrix
    levenshtein = distances[len(reference)][len(hypothesis)]

    # CER is Levenshtein distance divided by reference length
    if len(reference) == 0:
        return 0.0  # Handle empty reference case

    return levenshtein / len(reference)

def word_error_rate(reference, hypothesis):
    """
    Calculate Word Error Rate (WER)

    WER = (S + D + I) / N
    Where:
    S = number of substituted words
    D = number of deleted words
    I = number of inserted words
    N = number of words in reference

    Args:
        reference: Ground truth text
        hypothesis: OCR output text

    Returns:
        WER value (lower is better)
    """
    # Normalize texts
    reference = normalize_text(reference)
    hypothesis = normalize_text(hypothesis)

    # Split into words
    ref_words = reference.split()
    hyp_words = hypothesis.split()

    # Compute Levenshtein distance
    distances = np.zeros((len(ref_words) + 1, len(hyp_words) + 1))

    # Initialize first row and column
    for i in range(len(ref_words) + 1):
        distances[i][0] = i
    for j in range(len(hyp_words) + 1):
        distances[0][j] = j

    # Fill distance matrix
    for i in range(1, len(ref_words) + 1):
        for j in range(1, len(hyp_words) + 1):
            if ref_words[i-1] == hyp_words[j-1]:
                distances[i][j] = distances[i-1][j-1]
            else:
                substitution = distances[i-1][j-1] + 1
                insertion = distances[i][j-1] + 1
                deletion = distances[i-1][j] + 1
                distances[i][j] = min(substitution, insertion, deletion)

    # Levenshtein distance is the value in the bottom right corner of the matrix
    levenshtein = distances[len(ref_words)][len(hyp_words)]

    # WER is Levenshtein distance divided by reference length
    if len(ref_words) == 0:
        return 0.0  # Handle empty reference case

    return levenshtein / len(ref_words)

# More evaluation functions can be included as in your original code
# but for brevity I'm including just the core functions

print("OCR evaluation metrics defined successfully!")

"""# Cell 7: TrOCR Model Functions


"""

# Install transformers if not already installed
try:
    import transformers
except ImportError:
    !pip install transformers datasets
    import transformers

# The key fixed functions that were causing the issue
def prepare_dataset(batch):
    """
    Prepare batch data for TrOCR model training with robust image conversion

    Args:
        batch: Batch of data from the dataset

    Returns:
        Processed batch with pixel_values and labels
    """
    # Handle images with proper format conversion
    raw_images = batch["image"]
    processed_images = []

    for img in raw_images:
        # Convert from numpy array (stored in dataset) to PIL Image
        from PIL import Image
        import numpy as np

        # If img is a numpy array with the right shape
        if isinstance(img, np.ndarray):
            if img.ndim == 3:  # RGB image
                pil_img = Image.fromarray(img.astype('uint8'))
            elif img.ndim == 2:  # Grayscale image
                pil_img = Image.fromarray(np.repeat(img[:, :, np.newaxis], 3, axis=2).astype('uint8'))
            else:
                # Reshape if needed
                try:
                    img_reshaped = img.reshape((384, 384, 3)) if img.size == 384*384*3 else img.reshape((384, 384))
                    if img_reshaped.ndim == 2:  # Grayscale
                        pil_img = Image.fromarray(np.repeat(img_reshaped[:, :, np.newaxis], 3, axis=2).astype('uint8'))
                    else:  # RGB
                        pil_img = Image.fromarray(img_reshaped.astype('uint8'))
                except:
                    # Last resort - create a blank image
                    pil_img = Image.new('RGB', (384, 384), color='white')
                    print(f"Warning: Failed to reshape image with shape {img.shape}, created blank image")
        elif isinstance(img, Image.Image):
            pil_img = img
        else:
            # Last resort - create a blank image
            pil_img = Image.new('RGB', (384, 384), color='white')
            print(f"Warning: Unhandled image type {type(img)}, created blank image")

        processed_images.append(pil_img)

    # Now process with TrOCR processor
    pixel_values = processor(images=processed_images, return_tensors="pt").pixel_values

    # Tokenize the texts
    texts = batch["text"]
    labels = processor.tokenizer(texts, padding="max_length", truncation=True).input_ids

    return {"pixel_values": pixel_values, "labels": labels}

def convert_dataloader_to_dataset(data_loader):
    """
    Convert PyTorch DataLoader to HuggingFace Dataset with improved error handling

    Args:
        data_loader: PyTorch DataLoader

    Returns:
        HuggingFace Dataset
    """
    all_data = []

    for batch in data_loader:
        for i in range(len(batch["image"])):
            # Convert tensor to numpy safely
            if torch.is_tensor(batch["image"][i]):
                # Transpose from [C, H, W] to [H, W, C] if needed
                if batch["image"][i].dim() == 3 and batch["image"][i].shape[0] == 3:
                    img_np = batch["image"][i].permute(1, 2, 0).numpy()
                else:
                    img_np = batch["image"][i].numpy()

                # Normalize to 0-255 range for PIL compatibility
                if img_np.max() <= 1.0:
                    img_np = (img_np * 255).astype('uint8')
            else:
                # Just in case it's already a numpy array
                img_np = batch["image"][i]

            item = {
                "image": img_np,
                "text": batch["text"][i],
                "image_path": batch["image_path"][i]
            }
            all_data.append(item)

    # Verify we have data before creating the dataset
    if not all_data:
        print("WARNING: No data found in DataLoader")
        # Create a minimal dummy dataset with one item
        dummy_img = np.zeros((384, 384, 3), dtype=np.uint8)
        all_data = [{"image": dummy_img, "text": "dummy text", "image_path": "dummy_path"}]

    return Dataset.from_list(all_data)

def fine_tune_trocr_model_and_evaluate(results_from_preprocessing, num_epochs=5):
    """
    Fine-tune a TrOCR model on historical Spanish documents and evaluate it

    Args:
        results_from_preprocessing: Results from the preprocessing pipeline
        num_epochs: Number of epochs for fine-tuning

    Returns:
        Evaluation results
    """
    # Import necessary libraries
    from transformers import TrOCRProcessor, VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments
    import torch
    from datasets import Dataset

    # Extract data from preprocessing results
    train_loader = results_from_preprocessing["train_loader"]
    val_loader = results_from_preprocessing["val_loader"]
    document_images = results_from_preprocessing["document_images"]
    transcriptions = results_from_preprocessing["transcriptions"]
    output_base_path = results_from_preprocessing["output_base_path"]

    # Check if GPU is available
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Load TrOCR model and processor
    print("\nLoading TrOCR model for fine-tuning...")
    global processor  # Make processor global so prepare_dataset can use it
    processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")
    model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten")

    # Move model to device
    model.to(device)

    # Convert PyTorch dataloaders to HuggingFace datasets
    print("Converting DataLoaders to Datasets...")
    train_dataset = convert_dataloader_to_dataset(train_loader)
    val_dataset = convert_dataloader_to_dataset(val_loader)

    print(f"Created training dataset with {len(train_dataset)} samples")
    print(f"Created validation dataset with {len(val_dataset)} samples")

    # Apply the preprocessing to the datasets
    print("Preparing datasets for training...")
    train_dataset = train_dataset.map(
        prepare_dataset,
        batched=True,
        batch_size=4,  # Reduced batch size for stable processing
        remove_columns=["image", "image_path"]  # Remove columns that aren't needed for training
    )

    val_dataset = val_dataset.map(
        prepare_dataset,
        batched=True,
        batch_size=4,
        remove_columns=["image", "image_path"]
    )

    # Verify prepared dataset structure
    print("Training dataset features:", train_dataset.features)

    # Set up training arguments
    training_args = Seq2SeqTrainingArguments(
        output_dir=os.path.join(output_base_path, "trocr_fine_tuned"),
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        predict_with_generate=True,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        num_train_epochs=num_epochs,
        fp16=torch.cuda.is_available(),  # Use mixed precision if GPU is available
        learning_rate=5e-5,
        weight_decay=0.01,
        save_total_limit=2,
        load_best_model_at_end=True
    )

    # Define compute metrics function
    def compute_metrics(pred):
        labels_ids = pred.label_ids
        pred_ids = pred.predictions

        # Replace -100 with pad token id
        labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id

        # Decode predictions and labels
        pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
        labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id
        label_str = processor.tokenizer.batch_decode(labels_ids, skip_special_tokens=True)

        # Compute CER
        cer_scores = []
        for pred, label in zip(pred_str, label_str):
            cer = character_error_rate(label, pred)
            cer_scores.append(cer)

        avg_cer = sum(cer_scores) / len(cer_scores) if cer_scores else 0

        return {
            "cer": avg_cer,
            "accuracy": 1 - avg_cer
        }

    # Create Seq2Seq trainer
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics
    )

    # Train the model
    print("\nFine-tuning TrOCR model...")
    trainer.train()

    # Save the fine-tuned model
    model_save_path = os.path.join(output_base_path, "trocr_fine_tuned_final")
    model.save_pretrained(model_save_path)
    processor.save_pretrained(model_save_path)
    print(f"Fine-tuned model saved to {model_save_path}")

    # Return simple success message - full evaluation would require more code
    return {
        "status": "success",
        "model_path": model_save_path,
        "processor_path": model_save_path
    }

def evaluate_trocr_model_pipeline(results_from_preprocessing):
    """
    Example of how to use the evaluation functions with a TrOCR model

    Args:
        results_from_preprocessing: Results from the preprocessing pipeline

    Returns:
        Evaluation results
    """
    # Import necessary libraries
    from transformers import TrOCRProcessor, VisionEncoderDecoderModel
    import torch

    # Extract data from preprocessing results
    train_loader = results_from_preprocessing["train_loader"]
    val_loader = results_from_preprocessing["val_loader"]
    document_images = results_from_preprocessing["document_images"]
    transcriptions = results_from_preprocessing["transcriptions"]
    output_base_path = results_from_preprocessing["output_base_path"]

    # Check if GPU is available
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Load TrOCR model and processor
    print("\nLoading TrOCR model...")

    # For historical Spanish documents, we can use the base model
    global processor  # Make processor global so prepare_dataset can use it
    processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")
    model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten")

    # Move model to device
    model.to(device)

    # Create post-processor for Spanish historical text
    print("Creating lexicon for post-processing...")
    lexicon = create_lexicon_from_transcriptions(transcriptions)
    lexicon = augment_lexicon_with_variations(lexicon)
    print(f"Created lexicon with {len(lexicon)} words")

    # Create post-processor
    post_processor = SpanishHistoricalPostProcessor(lexicon)

    # Create a save directory for evaluation results
    save_dir = os.path.join(output_base_path, "evaluation_results")
    os.makedirs(save_dir, exist_ok=True)

    # Simple evaluation - just transcribe a few examples
    print("\nTranscribing a few examples...")

    sample_texts = []
    for i, batch in enumerate(val_loader):
        if i >= 2:  # Just do two batches
            break

        images = batch["image"].to(device)
        # Generate predictions
        pixel_values = processor(images=images, return_tensors="pt").pixel_values.to(device)
        generated_ids = model.generate(pixel_values)

        # Decode predictions
        preds = processor.batch_decode(generated_ids, skip_special_tokens=True)

        # Apply post-processing
        preds = [post_processor.process_text(p) for p in preds]

        # Add to samples
        sample_texts.extend(preds)

    # Print samples
    print("\nSample transcriptions:")
    for i, text in enumerate(sample_texts):
        print(f"Sample {i+1}: {text[:100]}...")

    # Return simple result
    return {
        "status": "success",
        "num_samples": len(sample_texts),
        "samples": sample_texts
    }

print("TrOCR model functions defined successfully!")

"""# Cell 8: Main Execution Script


"""

def upload_and_run():
    """
    Upload PDFs and run the preprocessing pipeline with improved error handling
    """
    # Import PIL ImageDraw for creating dummy images if needed
    from PIL import ImageDraw

    print("Step 1: Uploading PDFs")
    pdf_folder = handle_file_uploads()

    print("\nStep 2: Processing PDFs")
    output_base_path = '/content/ocr_data'
    transcriptions_path = '/content/transcriptions'

    # Process the PDFs
    document_images = process_all_pdfs(
        pdf_folder,
        output_base_path,
        dpi=300,
        max_pages_per_pdf=10  # Limit to 10 pages per PDF for demonstration
    )

    # Check if we got any processed images
    total_processed = sum(len(paths) for paths in document_images.values())

    if total_processed == 0:
        print("\nWARNING: No images were processed successfully.")
        print("Creating dummy images for testing...")

        # Create dummy document for testing
        dummy_folder = os.path.join(output_base_path, "processed_images", "dummy")
        os.makedirs(dummy_folder, exist_ok=True)

        # Create a few dummy images
        dummy_images = []
        for i in range(3):
            from PIL import Image, ImageDraw
            img = Image.new('RGB', (1000, 1414), color='white')
            draw = ImageDraw.Draw(img)
            draw.text((100, 100), f"Dummy Page {i+1}", fill='black')

            dummy_path = os.path.join(dummy_folder, f"dummy_page_{i+1:03d}.jpg")
            img.save(dummy_path)
            dummy_images.append(dummy_path)

        document_images["dummy"] = dummy_images

    print("\nStep 3: Creating transcriptions")
    create_dummy_transcriptions(document_images, transcriptions_path)

    # Collect all processed image paths
    print("\nStep 4: Collecting processed image paths...")
    all_processed_images = []
    for doc_id, image_paths in document_images.items():
        all_processed_images.extend(image_paths)

    print(f"Collected {len(all_processed_images)} processed images")

    # Load transcriptions
    print("\nStep 5: Loading transcriptions...")
    transcriptions = load_transcriptions(transcriptions_path, all_processed_images)
    print(f"Loaded {len(transcriptions)} transcriptions")

    # If no transcriptions were found, create dummy ones
    if not transcriptions and all_processed_images:
        print("No transcriptions found. Creating dummy transcriptions...")
        for img_path in all_processed_images:
            filename = os.path.basename(img_path)
            transcriptions[img_path] = f"Dummy transcription for {filename}"
        print(f"Created {len(transcriptions)} dummy transcriptions")

    # Create train-validation split
    print("\nStep 6: Creating train-validation split...")
    train_image_paths, val_image_paths = create_train_val_split(
        all_processed_images,
        transcriptions,
        val_ratio=0.2
    )
    print(f"Train set: {len(train_image_paths)} images")
    print(f"Validation set: {len(val_image_paths)} images")

    # Create data loaders
    print("\nStep 7: Creating data loaders...")
    train_loader, val_loader = create_data_loaders(
        train_image_paths,
        val_image_paths,
        transcriptions,
        batch_size=2  # Reduced batch size for safer execution
    )
    print(f"Created data loaders with batch size 2")

    # Show examples
    if all_processed_images:
        print("\nStep 8: Showing example images")
        try:
            save_example_images(document_images, output_base_path, num_examples=min(2, len(all_processed_images)))
        except Exception as e:
            print(f"Warning: Could not display example images: {str(e)}")

    print("\nPreprocessing completed successfully!")

    # Return the results
    return {
        "train_loader": train_loader,
        "val_loader": val_loader,
        "document_images": document_images,
        "transcriptions": transcriptions,
        "output_base_path": output_base_path
    }

# Modified main entry point with additional error handling
if __name__ == "__main__":
    try:
        # Check if PIL ImageDraw is available
        try:
            from PIL import ImageDraw
        except ImportError:
            print("Installing pillow for ImageDraw...")
            !pip install pillow
            from PIL import ImageDraw

        # Check for PyPDF2
        try:
            from PyPDF2 import PdfReader
        except ImportError:
            print("Installing PyPDF2...")
            !pip install PyPDF2

        # Run the pipeline
        results = upload_and_run()

        # Provide options for next steps
        if results:
            print("\nDo you want to evaluate a TrOCR model on the preprocessed data? (y/n)")
            answer = input()
            if answer.lower() == 'y':
                print("\nSelect an option:")
                print("1. Evaluate a pre-trained TrOCR model")
                print("2. Fine-tune and evaluate a TrOCR model (this may take some time)")
                option = input()

                if option == '1':
                    try:
                        # Install transformers if needed
                        try:
                            import transformers
                        except ImportError:
                            print("Installing transformers library...")
                            !pip install transformers datasets
                            import transformers

                        evaluation_results = evaluate_trocr_model_pipeline(results)
                        print("Evaluation completed!")
                    except Exception as e:
                        print(f"Error during evaluation: {str(e)}")
                        print("Please make sure you have the transformers library installed:")
                        print("!pip install transformers datasets")
                elif option == '2':
                    try:
                        # Install transformers if needed
                        try:
                            import transformers
                        except ImportError:
                            print("Installing transformers library...")
                            !pip install transformers datasets
                            import transformers

                        num_epochs = int(input("Enter the number of epochs for fine-tuning (recommended: 3-5): "))
                        evaluation_results = fine_tune_trocr_model_and_evaluate(results, num_epochs=num_epochs)
                        print("Fine-tuning and evaluation completed!")
                    except Exception as e:
                        print(f"Error during fine-tuning: {str(e)}")
                        print("Please make sure you have the transformers library installed:")
                        print("!pip install transformers datasets")
                else:
                    print("Invalid option. Skipping model evaluation.")
            else:
                print("Skipping model evaluation.")

        print("\nProcessing completed!")
    except Exception as e:
        print(f"Error in main execution: {str(e)}")
        import traceback
        traceback.print_exc()
        print("\nPlease ensure all dependencies are properly installed:")
        print("!apt-get update && apt-get install -y poppler-utils")
        print("!pip install pdf2image PyPDF2 pillow pytesseract opencv-python matplotlib tqdm")

print("Main execution script defined successfully!")
print("You can now run the OCR pipeline by executing:")
print("results = upload_and_run()")

!pip install transformers datasets

def fine_tune_trocr_model_and_evaluate(results_from_preprocessing, num_epochs=5):
    """
    Fine-tune a TrOCR model on historical Spanish documents and evaluate it

    Args:
        results_from_preprocessing: Results from the preprocessing pipeline
        num_epochs: Number of epochs for fine-tuning

    Returns:
        Evaluation results
    """
    # Import necessary libraries
    from transformers import TrOCRProcessor, VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments
    import torch
    from datasets import Dataset

    # Extract data from preprocessing results
    train_loader = results_from_preprocessing["train_loader"]
    val_loader = results_from_preprocessing["val_loader"]
    document_images = results_from_preprocessing["document_images"]
    transcriptions = results_from_preprocessing["transcriptions"]
    output_base_path = results_from_preprocessing["output_base_path"]

    # Check if GPU is available
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Load TrOCR model and processor
    print("\nLoading TrOCR model for fine-tuning...")
    processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")
    model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten")

    # Move model to device
    model.to(device)

    # Prepare datasets for training
    def prepare_dataset(batch):
        # FIX: Process images directly - don't create a nested list
        images = batch["image"]  # These are already tensors or numpy arrays

        # Handle different possible formats
        if isinstance(images[0], list):
            # If we somehow still have nested lists
            images = [img[0] if isinstance(img, list) else img for img in images]

        # Convert to appropriate format if needed
        import numpy as np
        if isinstance(images[0], np.ndarray):
            # Convert numpy arrays to PIL images if needed
            from PIL import Image
            images = [Image.fromarray(img) if img.ndim == 3 else Image.fromarray(np.repeat(img[:, :, np.newaxis], 3, axis=2)) for img in images]

        # Process with the TrOCR processor
        pixel_values = processor(images=images, return_tensors="pt").pixel_values

        # Tokenize the texts
        texts = batch["text"]
        labels = processor.tokenizer(texts, padding="max_length", truncation=True).input_ids

        return {"pixel_values": pixel_values, "labels": labels}

    # Convert PyTorch dataloaders to HuggingFace datasets
    def convert_dataloader_to_dataset(data_loader):
        all_data = []
        for batch in data_loader:
            for i in range(len(batch["image"])):
                item = {
                    "image": batch["image"][i].numpy(),  # Convert tensor to numpy
                    "text": batch["text"][i],
                    "image_path": batch["image_path"][i]
                }
                all_data.append(item)

        return Dataset.from_list(all_data)

    train_dataset = convert_dataloader_to_dataset(train_loader)
    val_dataset = convert_dataloader_to_dataset(val_loader)

    # Apply the preprocessing to the datasets
    train_dataset = train_dataset.map(prepare_dataset, batched=True, batch_size=8)
    val_dataset = val_dataset.map(prepare_dataset, batched=True, batch_size=8)

    # Rest of the function remains the same...

evaluation_results = fine_tune_trocr_model_and_evaluate(results, num_epochs=4)

evaluation_results = evaluate_trocr_model_pipeline(results)