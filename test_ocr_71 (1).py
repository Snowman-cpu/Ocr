# -*- coding: utf-8 -*-
"""test_ocr_71.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gHAvG1LLkjSYt2JLOMJL3mdZDsdyPQi3
"""

import os

import os

# Define base paths
base_path = '/content'
pdf_folder = os.path.join(base_path, 'pdfs')
output_base_path = os.path.join(base_path, 'ocr_data')
transcriptions_path = os.path.join(base_path, 'transcriptions')
training_data_path = os.path.join(output_base_path, 'training_data')
results_path = os.path.join(output_base_path, 'results')

# Create main directories
os.makedirs(pdf_folder, exist_ok=True)
os.makedirs(os.path.join(output_base_path, "images"), exist_ok=True)
os.makedirs(os.path.join(output_base_path, "processed_images"), exist_ok=True)
os.makedirs(os.path.join(output_base_path, "binary_images"), exist_ok=True)
os.makedirs(training_data_path, exist_ok=True)
os.makedirs(transcriptions_path, exist_ok=True)
os.makedirs(results_path, exist_ok=True)

# List of subdirectories to create in binary_images, processed_images, and images
subdirectories = [
    "Buendia - Instruccion-1",
    "Constituciones sinodales Calahorra 1602-2",
    "Ezcaray - Vozes-3",
    "Mendo - Principe perfecto-4",
    "Paredes - Reglas generales-5",
    "PORCONES.228.35  1636-6"
]

# Define paths for binary_images, processed_images, and images directories
binary_images_path = os.path.join(output_base_path, "binary_images")
processed_images_path = os.path.join(output_base_path, "processed_images")
images_path = os.path.join(output_base_path, "images")

# Create subdirectories inside binary_images
for subdir in subdirectories:
    os.makedirs(os.path.join(binary_images_path, subdir), exist_ok=True)

# Create subdirectories inside processed_images
for subdir in subdirectories:
    os.makedirs(os.path.join(processed_images_path, subdir), exist_ok=True)

# Create subdirectories inside images
for subdir in subdirectories:
    os.makedirs(os.path.join(images_path, subdir), exist_ok=True)

print("Setup complete! All directories created.")

"""# Installation and Setup

"""

# Install required system packages for PDF processing
!apt-get update
!apt-get install -y poppler-utils

# Install required Python packages
!pip install pdf2image pytesseract opencv-python matplotlib tqdm
!pip install torch torchvision
!pip install transformers datasets
!pip install pillow seaborn pandas
!pip install PyPDF2 python-docx

"""# 1. Enhanced Transcription Loader with DOCX Support"""

import os
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from tqdm.notebook import tqdm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
import re

# Check device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

class HistoricalDocumentDataset(Dataset):
    """
    Dataset for historical document pages.

    Attributes:
        page_keys: List of keys for page data
        page_data: Dictionary mapping keys to page data
        transform: Optional transform to apply to images
        max_length: Maximum text length to return
    """
    def __init__(self, page_data, transform=None, max_length=512):
        """
        Initialize the dataset.

        Args:
            page_data: Dictionary mapping (doc_name, page_num) to page data
            transform: Optional transform to apply to images
            max_length: Maximum text length
        """
        self.page_keys = list(page_data.keys())
        self.page_data = page_data
        self.transform = transform
        self.max_length = max_length

    def __len__(self) -> int:
        """Return the number of pages in the dataset."""
        return len(self.page_keys)

    def __getitem__(self, idx: int) -> dict:
        """
        Get a dataset item.

        Args:
            idx: Index of the item to get

        Returns:
            Dictionary with image, text, and metadata
        """
        key = self.page_keys[idx]
        data = self.page_data[key]

        # Load image (use processed image by default)
        image_path = data['processed_image']
        try:
            image = Image.open(image_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
        except Exception as e:
            logging.error(f"Error loading image {image_path}: {e}")
            # Create a dummy image in case of error
            image = Image.new('RGB', (384, 384), color='white')
            if self.transform:
                image = self.transform(image)

        # Get text (truncate if necessary)
        text = data['text']
        if len(text) > self.max_length:
            text = text[:self.max_length]

        # Return image, text, and metadata
        return {
            'image': image,
            'text': text,
            'doc_name': key[0],
            'page_num': key[1],
            'image_path': image_path
        }

def get_transforms():
    """
    Get transforms for image preprocessing.

    Returns:
        torchvision.transforms.Compose object with transforms
    """
    return transforms.Compose([
        transforms.Resize((384, 384)),  # TrOCR default size
        transforms.ToTensor(),
    ])

def create_train_val_split(page_transcriptions, val_ratio=0.2, seed=42):
    """
    Split the dataset into training and validation sets.

    Args:
        page_transcriptions: Dictionary mapping (doc_name, page_num) to page data
        val_ratio: Ratio of validation data
        seed: Random seed for reproducibility

    Returns:
        Tuple of (train_data, val_data)
    """
    import random
    random.seed(seed)

    # Create list of keys
    keys = list(page_transcriptions.keys())
    random.shuffle(keys)

    # Calculate split point
    split_idx = int(len(keys) * (1 - val_ratio))

    # Split keys
    train_keys = keys[:split_idx]
    val_keys = keys[split_idx:]

    # Create dictionaries
    train_data = {k: page_transcriptions[k] for k in train_keys}
    val_data = {k: page_transcriptions[k] for k in val_keys}

    print(f"Train set: {len(train_data)} pages")
    print(f"Validation set: {len(val_data)} pages")

    return train_data, val_data

def initialize_trocr():
    """
    Initialize the TrOCR model and processor.

    Returns:
        Tuple of (model, processor)
    """
    print("Loading TrOCR model and processor...")
    processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")
    model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten")

    # Configure model
    model.config.decoder_start_token_id = processor.tokenizer.bos_token_id
    model.config.pad_token_id = processor.tokenizer.pad_token_id
    model = model.to(device)

    return model, processor

def calculate_metrics(predictions, references):
    """
    Calculate Character Error Rate (CER) and Word Error Rate (WER).

    Args:
        predictions: List of predicted texts
        references: List of reference texts

    Returns:
        Dictionary with CER and WER metrics
    """
    def normalize_text(text):
        # Lowercase and remove punctuation
        text = text.lower()
        text = re.sub(r'[^\w\s]', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def levenshtein_distance(s1, s2):
        # Calculate edit distance
        if len(s1) < len(s2):
            return levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]

    # Calculate CER and WER for each pair
    cer_values = []
    wer_values = []

    for pred, ref in zip(predictions, references):
        # Normalize texts
        pred_norm = normalize_text(pred)
        ref_norm = normalize_text(ref)

        # Calculate CER
        if len(ref_norm) > 0:
            cer = levenshtein_distance(pred_norm, ref_norm) / len(ref_norm)
        else:
            cer = 1.0 if len(pred_norm) > 0 else 0.0

        # Calculate WER
        pred_words = pred_norm.split()
        ref_words = ref_norm.split()

        if len(ref_words) > 0:
            wer = levenshtein_distance(pred_words, ref_words) / len(ref_words)
        else:
            wer = 1.0 if len(pred_words) > 0 else 0.0

        cer_values.append(cer)
        wer_values.append(wer)

    # Calculate averages
    avg_cer = sum(cer_values) / len(cer_values) if cer_values else 1.0
    avg_wer = sum(wer_values) / len(wer_values) if wer_values else 1.0

    return {
        'cer': avg_cer,
        'wer': avg_wer,
        'cer_values': cer_values,
        'wer_values': wer_values
    }

def test_trocr(model, processor, page_transcriptions, num_examples=3):
    """
    Test TrOCR on a few example pages.

    Args:
        model: TrOCR model
        processor: TrOCR processor
        page_transcriptions: Dictionary mapping (doc_name, page_num) to page data
        num_examples: Number of examples to test

    Returns:
        List of results
    """
    # Select a few examples
    example_keys = list(page_transcriptions.keys())[:num_examples]

    results = []

    for key in example_keys:
        data = page_transcriptions[key]
        doc_name, page_num = key

        # Load image
        image = Image.open(data['processed_image']).convert('RGB')

        # Process with TrOCR
        pixel_values = processor(image, return_tensors="pt").pixel_values.to(device)
        generated_ids = model.generate(pixel_values, max_length=128)
        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

        # Store results
        results.append({
            'doc_name': doc_name,
            'page_num': page_num,
            'ground_truth': data['text'][:200] + "..." if len(data['text']) > 200 else data['text'],
            'ocr_text': generated_text[:200] + "..." if len(generated_text) > 200 else generated_text,
            'image_path': data['processed_image']
        })

    # Display results
    for result in results:
        print(f"\nDocument: {result['doc_name']}, Page: {result['page_num']}")
        print(f"Ground truth: {result['ground_truth']}")
        print(f"OCR text: {result['ocr_text']}")

        # Display image
        plt.figure(figsize=(10, 10))
        image = Image.open(result['image_path'])
        plt.imshow(np.array(image), cmap='gray')
        plt.title(f"{result['doc_name']} - Page {result['page_num']}")
        plt.axis('off')
        plt.show()

    return results

def run_trocr_ocr(model, processor, page_transcriptions):
    """
    Run TrOCR OCR on all pages in the dataset.

    Args:
        model: TrOCR model
        processor: TrOCR processor
        page_transcriptions: Dictionary mapping (doc_name, page_num) to page data

    Returns:
        Tuple of (results, metrics)
    """
    results = []

    for key in tqdm(page_transcriptions.keys(), desc="Running TrOCR OCR"):
        data = page_transcriptions[key]
        doc_name, page_num = key

        # Load image
        try:
            image = Image.open(data['processed_image']).convert('RGB')

            # Process with TrOCR
            pixel_values = processor(image, return_tensors="pt").pixel_values.to(device)
            generated_ids = model.generate(pixel_values, max_length=128)
            generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

            # Store results
            results.append({
                'doc_name': doc_name,
                'page_num': page_num,
                'ground_truth': data['text'],
                'ocr_text': generated_text,
                'image_path': data['processed_image']
            })
        except Exception as e:
            logging.error(f"Error processing {data['processed_image']}: {e}")

    # Calculate metrics
    predictions = [r['ocr_text'] for r in results]
    references = [r['ground_truth'] for r in results]
    metrics = calculate_metrics(predictions, references)

    print(f"\nEvaluation metrics:")
    print(f"Character Error Rate (CER): {metrics['cer']:.4f}")
    print(f"Word Error Rate (WER): {metrics['wer']:.4f}")

    # Add metrics to results
    for i in range(len(results)):
        results[i]['cer'] = metrics['cer_values'][i]
        results[i]['wer'] = metrics['wer_values'][i]

    return results, metrics

# Execute TrOCR component
print("Creating train-validation split...")
train_data, val_data = create_train_val_split(page_transcriptions)

print("\nInitializing TrOCR model...")
model, processor = initialize_trocr()

print("\nTesting TrOCR on a few examples...")
test_results = test_trocr(model, processor, val_data, num_examples=2)

print("\nRunning TrOCR on all validation pages...")
val_results, val_metrics = run_trocr_ocr(model, processor, val_data)

# Create a DataFrame with the results
df_results = pd.DataFrame(val_results)
print("\nOCR Results:")
display(df_results.head())

"""# 2. TrOCR-Based OCR Component"""

# -*- coding: utf-8 -*-
"""
Historical Document OCR Transcription Loader

This script provides functionality to:
1. Find and load document images from processed and binary directories
2. Extract and normalize document names from image paths
3. Load transcriptions from .txt or .docx files
4. Match document images with corresponding transcriptions
5. Create page-level transcriptions for OCR training
6. Build a PyTorch dataset suitable for OCR model training

Author: Claude (2024)
"""

import os
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
from PIL import Image
from typing import Dict, List, Tuple, Optional, Union, Any
import logging
import difflib
from google.colab import files
import random
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from IPython.display import display

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Import docx if available, otherwise install it
try:
    import docx
    logger.info("python-docx is already installed")
except ImportError:
    logger.info("Installing python-docx package...")
    !pip install python-docx
    import docx

#------------------------------------------------------------------------------
# Document Name Normalization Utilities
#------------------------------------------------------------------------------

def normalize_document_name(doc_name: str) -> str:
    """
    Normalize a document name by removing common suffixes and standardizing format.

    Args:
        doc_name: Raw document name that might include page numbers or other suffixes

    Returns:
        Normalized document name
    """
    # Remove page number suffixes like "-1", "-2", etc.
    doc_name = re.sub(r'-\d+$', '', doc_name)

    # Remove additional identifiers that might be in document names but not in transcription files
    doc_name = re.sub(r' - Instruccion$', '', doc_name)
    doc_name = re.sub(r' - Principe perfecto$', '', doc_name)
    doc_name = re.sub(r' - Vozes$', '', doc_name)
    doc_name = re.sub(r' - Reglas generales$', '', doc_name)
    doc_name = re.sub(r' sinodales Calahorra 1602$', ' sinodales', doc_name)

    return doc_name.strip()

def get_best_transcription_match(doc_name: str, available_transcriptions: List[str]) -> Optional[str]:
    """
    Find the best matching transcription filename for a document name using fuzzy matching.

    Args:
        doc_name: Document name to match
        available_transcriptions: List of available transcription filenames (without extensions)

    Returns:
        Best matching transcription name or None if no good match found
    """
    if not available_transcriptions:
        return None

    # Normalize document name
    normalized_doc_name = normalize_document_name(doc_name)

    # Try exact match first with variations
    for transcription in available_transcriptions:
        # Try direct match
        if normalized_doc_name == transcription:
            return transcription

        # Try without "transcription" suffix
        if normalized_doc_name == transcription.replace(' transcription', ''):
            return transcription

    # If no exact match, try fuzzy matching
    best_match = None
    best_ratio = 0.0

    for transcription in available_transcriptions:
        # Compare normalized doc name with transcription name (without "transcription" suffix)
        base_transcription = transcription.replace(' transcription', '')

        # Calculate similarity ratio
        ratio = difflib.SequenceMatcher(None, normalized_doc_name.lower(), base_transcription.lower()).ratio()

        # Also try with the full transcription name
        full_ratio = difflib.SequenceMatcher(None, normalized_doc_name.lower(), transcription.lower()).ratio()
        ratio = max(ratio, full_ratio)

        if ratio > best_ratio and ratio > 0.6:  # 0.6 is a threshold for good matches
            best_ratio = ratio
            best_match = transcription

    if best_match:
        logger.info(f"Fuzzy matched '{doc_name}' to '{best_match}' with confidence {best_ratio:.2f}")

    return best_match

#------------------------------------------------------------------------------
# Transcription Loader Class
#------------------------------------------------------------------------------

class TranscriptionLoader:
    """
    A class for loading transcriptions from various file formats (.txt, .docx)
    and mapping them to corresponding document images.
    """

    def __init__(self, transcription_dir: str, page_transcription_dir: str):
        """
        Initialize the transcription loader.

        Args:
            transcription_dir: Directory containing original transcription files
            page_transcription_dir: Directory to store page-level transcriptions
        """
        self.transcription_dir = transcription_dir
        self.page_transcription_dir = page_transcription_dir
        os.makedirs(page_transcription_dir, exist_ok=True)

        # Mapping from document names to transcription files
        self.doc_transcription_map = {}
        # List of available transcription basenames (without extensions)
        self.available_transcriptions = []

        # Format handlers
        self.format_handlers = {
            '.txt': self._read_txt_file,
            '.docx': self._read_docx_file
        }

    def find_transcription_files(self) -> Dict[str, str]:
        """
        Find all transcription files in the transcription directory.

        Returns:
            Dictionary mapping document names to transcription file paths
        """
        # Clear existing maps
        self.doc_transcription_map = {}
        self.available_transcriptions = []

        for file in os.listdir(self.transcription_dir):
            file_path = os.path.join(self.transcription_dir, file)
            if not os.path.isfile(file_path):
                continue

            file_ext = os.path.splitext(file)[1].lower()
            if file_ext in self.format_handlers:
                # Get document name without extension
                doc_name = os.path.splitext(file)[0]
                self.doc_transcription_map[doc_name] = file_path
                self.available_transcriptions.append(doc_name)

        if self.doc_transcription_map:
            logger.info(f"Found {len(self.doc_transcription_map)} transcription files "
                       f"({', '.join(f'{ext}' for ext in set(os.path.splitext(f)[1] for f in self.doc_transcription_map.values()))})")
            logger.info(f"Available transcriptions: {', '.join(self.available_transcriptions)}")
        else:
            logger.warning("No transcription files found in directory: " + self.transcription_dir)

        return self.doc_transcription_map

    def _read_txt_file(self, file_path: str) -> str:
        """
        Read text content from a .txt file.

        Args:
            file_path: Path to the .txt file

        Returns:
            Text content as a string
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            # Try different encodings if utf-8 fails
            encodings = ['latin-1', 'iso-8859-1', 'cp1252']
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        logger.warning(f"File {file_path} decoded using {encoding} instead of utf-8")
                        return f.read()
                except UnicodeDecodeError:
                    continue

            # If all else fails, use binary mode and ignore errors
            logger.error(f"Could not decode {file_path} with common encodings, using binary mode")
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read()

    def _read_docx_file(self, file_path: str) -> str:
        """
        Read text content from a .docx file.

        Args:
            file_path: Path to the .docx file

        Returns:
            Text content as a string
        """
        try:
            doc = docx.Document(file_path)
            full_text = []

            # Extract text from paragraphs
            for para in doc.paragraphs:
                full_text.append(para.text)

            # Also check for tables, which might contain transcription data
            for table in doc.tables:
                for row in table.rows:
                    row_texts = [cell.text for cell in row.cells if cell.text.strip()]
                    if row_texts:
                        full_text.append(' | '.join(row_texts))

            return '\n'.join(full_text)
        except Exception as e:
            logger.error(f"Error reading .docx file {file_path}: {str(e)}")
            return f"ERROR: Could not read {file_path}"

    def read_transcription(self, doc_name: str) -> Optional[str]:
        """
        Read transcription for a specific document, handling name variations.

        Args:
            doc_name: Document name (will be normalized and matched)

        Returns:
            Transcription content or None if not found
        """
        # Find best matching transcription
        best_match = get_best_transcription_match(doc_name, self.available_transcriptions)

        if best_match and best_match in self.doc_transcription_map:
            file_path = self.doc_transcription_map[best_match]
            file_ext = os.path.splitext(file_path)[1].lower()

            if file_ext in self.format_handlers:
                logger.info(f"Found transcription for '{doc_name}' -> '{best_match}'")
                return self.format_handlers[file_ext](file_path)

        logger.warning(f"No transcription found for document {doc_name}")
        return None

    def create_page_transcriptions(self, image_mapping: Dict[Tuple[str, int], Dict[str, str]],
                                  max_pages: int = 6) -> Dict[Tuple[str, int], Dict[str, Any]]:
        """
        Create page-level transcriptions by splitting document transcriptions.

        Args:
            image_mapping: Dictionary mapping (doc_name, page_num) to image paths
            max_pages: Maximum number of pages per document

        Returns:
            Dictionary mapping (doc_name, page_num) to page data including transcription
        """
        page_transcriptions = {}

        # Get unique document names
        doc_names = set(doc_name for doc_name, _ in image_mapping.keys())
        logger.info(f"Creating page transcriptions for {len(doc_names)} documents: {', '.join(doc_names)}")

        for doc_name in doc_names:
            # Get transcription for this document
            transcription = self.read_transcription(doc_name)
            if not transcription:
                continue

            # Count lines and estimate lines per page
            lines = transcription.split('\n')
            total_lines = len(lines)
            lines_per_page = max(1, total_lines // max_pages)

            # Split transcription into pages
            for page_num in range(1, max_pages + 1):
                key = (doc_name, page_num)
                if key not in image_mapping:
                    continue

                # Calculate line range for this page
                start_line = (page_num - 1) * lines_per_page
                end_line = min(page_num * lines_per_page, total_lines)

                # Extract page text
                page_text = '\n'.join(lines[start_line:end_line])

                # Save to file
                page_file = os.path.join(self.page_transcription_dir, f"{doc_name}_page_{page_num:03d}.txt")
                with open(page_file, 'w', encoding='utf-8') as f:
                    f.write(page_text)

                # Store in dictionary
                if key in image_mapping:
                    page_transcriptions[key] = {
                        'text': page_text,
                        'processed_image': image_mapping[key].get('processed'),
                        'binary_image': image_mapping[key].get('binary'),
                        'transcription_file': self.doc_transcription_map.get(
                            get_best_transcription_match(doc_name, self.available_transcriptions), '')
                    }

        logger.info(f"Created {len(page_transcriptions)} page-level transcriptions")
        return page_transcriptions

#------------------------------------------------------------------------------
# Image Finding Functions
#------------------------------------------------------------------------------

def extract_doc_info(image_path: str) -> Tuple[Optional[str], Optional[int]]:
    """
    Extract document name and page number from image path.

    This function supports multiple naming patterns:
    - Document_Name_page_XXX.jpg
    - document-name-XXX.jpg (where XXX is the page number)

    Args:
        image_path: Path to the image file

    Returns:
        Tuple of (document_name, page_number) or (None, None) if not parsable
    """
    # Extract filename from path
    filename = os.path.basename(image_path)
    # Remove extension
    filename_without_ext = os.path.splitext(filename)[0]

    # Try different patterns

    # Pattern 1: Document_Name_page_XXX.jpg
    match = re.match(r'(.+?)_page_(\d+)', filename_without_ext)
    if match:
        doc_name = match.group(1)
        page_num = int(match.group(2))
        return doc_name, page_num

    # Pattern 2: Detect document name and page number from directory structure
    parent_dir = os.path.basename(os.path.dirname(image_path))
    if parent_dir and filename_without_ext.endswith(('-1', '-2', '-3', '-4', '-5', '-6')):
        # Extract page number from the end of filename
        page_match = re.search(r'-(\d+)$', filename_without_ext)
        if page_match:
            page_num = int(page_match.group(1))
            # Use parent directory as document name
            return parent_dir, page_num

    # Try to extract from arbitrary filename with page number at the end
    page_match = re.search(r'[-_]p(?:age)?[-_]?(\d+)$', filename_without_ext, re.IGNORECASE)
    if page_match:
        page_num = int(page_match.group(1))
        # Remove page suffix from filename to get document name
        doc_name = re.sub(r'[-_]p(?:age)?[-_]?\d+$', '', filename_without_ext, flags=re.IGNORECASE)
        return doc_name, page_num

    logger.warning(f"Could not parse document name and page from: {image_path}")
    return None, None


def find_all_images(base_path: str = '/content') -> Tuple[List[str], List[str], Dict[Tuple[str, int], Dict[str, str]]]:
    """
    Find all processed and binary images in the specified directories.

    Args:
        base_path: Base directory for all data

    Returns:
        Tuple of (processed_images, binary_images, image_mapping)
    """
    output_base_path = os.path.join(base_path, 'ocr_data')
    processed_dir = os.path.join(output_base_path, "processed_images")
    binary_dir = os.path.join(output_base_path, "binary_images")

    processed_images = []
    binary_images = []
    image_mapping = {}  # Maps (doc_name, page_num) to {'processed': path, 'binary': path}

    # Track document names to help with debugging
    found_doc_names = set()
    doc_page_counts = {}

    # Find processed images
    if os.path.exists(processed_dir):
        for doc_dir in os.listdir(processed_dir):
            doc_path = os.path.join(processed_dir, doc_dir)
            if os.path.isdir(doc_path):
                for img_file in os.listdir(doc_path):
                    if img_file.endswith(('.jpg', '.png')):
                        img_path = os.path.join(doc_path, img_file)
                        processed_images.append(img_path)

                        # Try to extract document name and page number
                        doc_name, page_num = extract_doc_info(img_path)

                        # If that fails, use the directory name as document name
                        # and generate a sequential page number
                        if doc_name is None or page_num is None:
                            doc_name = doc_dir
                            if doc_name not in doc_page_counts:
                                doc_page_counts[doc_name] = 0
                            doc_page_counts[doc_name] += 1
                            page_num = doc_page_counts[doc_name]

                        found_doc_names.add(doc_name)
                        if (doc_name, page_num) not in image_mapping:
                            image_mapping[(doc_name, page_num)] = {'processed': img_path}
    else:
        logger.warning(f"Processed images directory not found: {processed_dir}")

    # Find binary images
    if os.path.exists(binary_dir):
        for doc_dir in os.listdir(binary_dir):
            doc_path = os.path.join(binary_dir, doc_dir)
            if os.path.isdir(doc_path):
                for img_file in os.listdir(doc_path):
                    if img_file.endswith(('.jpg', '.png')):
                        img_path = os.path.join(doc_path, img_file)
                        binary_images.append(img_path)

                        # Extract document name and page number
                        doc_name, page_num = extract_doc_info(img_path)

                        # If that fails, use the directory name as document name
                        # and try to match with an existing processed image
                        if doc_name is None or page_num is None:
                            doc_name = doc_dir
                            # Try to find matching processed image
                            for (d, p), mapping in image_mapping.items():
                                if d == doc_name and os.path.basename(mapping.get('processed', '')) == img_file:
                                    page_num = p
                                    break

                            # If still can't find, generate a sequential page number
                            if page_num is None:
                                if doc_name not in doc_page_counts:
                                    doc_page_counts[doc_name] = 0
                                doc_page_counts[doc_name] += 1
                                page_num = doc_page_counts[doc_name]

                        found_doc_names.add(doc_name)
                        if (doc_name, page_num) in image_mapping:
                            image_mapping[(doc_name, page_num)]['binary'] = img_path
                        else:
                            image_mapping[(doc_name, page_num)] = {'binary': img_path}
    else:
        logger.warning(f"Binary images directory not found: {binary_dir}")

    # Log what we found
    logger.info(f"Found {len(processed_images)} processed images and {len(binary_images)} binary images")
    logger.info(f"Identified {len(found_doc_names)} unique documents: {', '.join(sorted(found_doc_names))}")
    logger.info(f"Created {len(image_mapping)} document-page mappings")

    return processed_images, binary_images, image_mapping

#------------------------------------------------------------------------------
# Main Function to Load Transcriptions and Images
#------------------------------------------------------------------------------

def load_transcriptions_and_images(base_path: str = '/content') -> Dict[Tuple[str, int], Dict[str, Any]]:
    """
    Main function to load transcriptions and map them to images.

    Args:
        base_path: Base directory for all data

    Returns:
        Dictionary mapping (doc_name, page_num) to page data including transcription
    """
    # Define paths
    output_base_path = os.path.join(base_path, 'ocr_data')
    transcriptions_path = os.path.join(base_path, 'transcriptions')
    page_transcriptions_path = os.path.join(base_path, 'page_transcriptions')

    # Create directory for page-level transcriptions
    os.makedirs(page_transcriptions_path, exist_ok=True)

    # Find all images
    logger.info("Finding images...")
    processed_images, binary_images, image_mapping = find_all_images(base_path)

    # Print document names found in image mapping for debugging
    unique_docs = sorted(set(doc_name for doc_name, _ in image_mapping.keys()))
    logger.info(f"Document names found in image paths: {', '.join(unique_docs)}")

    # Load transcriptions
    logger.info("Loading transcriptions...")
    loader = TranscriptionLoader(transcriptions_path, page_transcriptions_path)
    doc_transcription_map = loader.find_transcription_files()

    # Create page-level transcriptions
    logger.info("Creating page-level transcriptions...")
    page_transcriptions = loader.create_page_transcriptions(image_mapping)

    # Check if we have any successful matches
    if not page_transcriptions:
        logger.warning("No page transcriptions created - checking for issues")

        # Check transcription directory contents
        logger.info("Files in transcription directory:")
        for file in os.listdir(transcriptions_path):
            logger.info(f"  - {file}")

        # Try manual matching for each document
        logger.info("Attempting manual matching:")
        for doc_name in unique_docs:
            normalized = normalize_document_name(doc_name)
            best_match = get_best_transcription_match(doc_name, loader.available_transcriptions)
            logger.info(f"  - '{doc_name}' -> normalized: '{normalized}', best match: '{best_match}'")

    # Create dataframe for easy access
    rows = []
    for (doc_name, page_num), data in page_transcriptions.items():
        text_preview = data['text'][:100] + "..." if len(data['text']) > 100 else data['text']
        rows.append({
            'doc_name': doc_name,
            'page_num': page_num,
            'processed_image': data.get('processed_image', ''),
            'binary_image': data.get('binary_image', ''),
            'text_preview': text_preview,
            'text_length': len(data['text']),
            'transcription_file': data.get('transcription_file', '')
        })

    # Create and display dataframe
    if rows:
        df_pages = pd.DataFrame(rows)
        logger.info("Dataset overview:")
        display(df_pages)
    else:
        logger.warning("No rows for dataframe - mapping failed")

    return page_transcriptions

#------------------------------------------------------------------------------
# Visualization Functions
#------------------------------------------------------------------------------

def visualize_page_transcription(doc_name: str, page_num: int, page_transcriptions: Dict[Tuple[str, int], Dict[str, Any]]):
    """
    Visualize a page transcription alongside its image.

    Args:
        doc_name: Document name
        page_num: Page number
        page_transcriptions: Dictionary mapping (doc_name, page_num) to page data
    """
    key = (doc_name, page_num)
    if key not in page_transcriptions:
        print(f"No data found for document '{doc_name}', page {page_num}")
        return

    page_data = page_transcriptions[key]

    # Display image if available
    if 'processed_image' in page_data:
        img_path = page_data['processed_image']
        if img_path and os.path.exists(img_path):
            try:
                img = Image.open(img_path)
                plt.figure(figsize=(10, 14))
                plt.imshow(np.array(img))
                plt.title(f"Document: {doc_name}, Page: {page_num}")
                plt.axis('off')
                plt.show()
            except Exception as e:
                print(f"Error displaying image: {str(e)}")

    # Display transcription
    if 'text' in page_data:
        print("\nTranscription:")
        print("-" * 80)
        print(page_data['text'])
        print("-" * 80)
    else:
        print("No transcription available")

def show_example_pages(page_transcriptions, num_examples=2):
    """
    Display example pages with their transcriptions.

    Args:
        page_transcriptions: Dictionary mapping (doc_name, page_num) to page data
        num_examples: Number of examples to show
    """
    # Check if we have any transcriptions
    if not page_transcriptions:
        print("No page transcriptions available to display")
        return

    # Get unique document names
    doc_names = sorted(set(doc_name for doc_name, _ in page_transcriptions.keys()))
    examples_shown = 0

    print(f"\nShowing examples from {len(doc_names)} documents:")

    for doc_name in doc_names:
        # Find pages for this document
        pages = [page_num for (d, page_num) in page_transcriptions.keys() if d == doc_name]

        if not pages:
            continue

        # Show first page for each document, up to num_examples
        visualize_page_transcription(doc_name, min(pages), page_transcriptions)
        examples_shown += 1

        if examples_shown >= num_examples:
            break

#------------------------------------------------------------------------------
# Manual Transcription Upload Function
#------------------------------------------------------------------------------

def upload_transcription_files():
    """
    Allow user to manually upload transcription files if automatic loading fails.

    Returns:
        Dictionary mapping (doc_name, page_num) to page data including transcription
    """
    print("Please upload your transcription files (.txt or .docx)...")
    uploaded = files.upload()

    # Save uploaded files to the transcriptions directory
    transcriptions_dir = os.path.join('/content', 'transcriptions')
    os.makedirs(transcriptions_dir, exist_ok=True)

    for filename, content in uploaded.items():
        filepath = os.path.join(transcriptions_dir, filename)
        with open(filepath, 'wb') as f:
            f.write(content)
        print(f"Saved: {filename} -> {filepath}")

    print("\nRunning the transcription loader again with newly uploaded files...")
    return load_transcriptions_and_images()

#------------------------------------------------------------------------------
# OCR Dataset Creation
#------------------------------------------------------------------------------

class OCRDataset(Dataset):
    """
    Dataset class for OCR training.
    """
    def __init__(self, page_transcriptions, transform=None, max_length=512):
        """
        Initialize the dataset.

        Args:
            page_transcriptions: Dictionary mapping (doc_name, page_num) to page data
            transform: Optional transform to be applied to the images
            max_length: Maximum sequence length for the transcriptions
        """
        self.samples = []
        for (doc_name, page_num), data in page_transcriptions.items():
            if 'processed_image' in data and 'text' in data:
                img_path = data['processed_image']
                text = data['text']

                if img_path and os.path.exists(img_path):
                    self.samples.append({
                        'doc_name': doc_name,
                        'page_num': page_num,
                        'image_path': img_path,
                        'text': text[:max_length] if len(text) > max_length else text
                    })

        self.transform = transform
        self.max_length = max_length

        logger.info(f"Created dataset with {len(self.samples)} samples")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]

        # Load image
        try:
            image = Image.open(sample['image_path']).convert('RGB')
        except Exception as e:
            logger.error(f"Error loading image {sample['image_path']}: {str(e)}")
            # Create a blank image as fallback
            image = Image.new('RGB', (384, 384), color='white')

        # Apply transforms if any
        if self.transform:
            image = self.transform(image)

        return {
            'image': image,
            'text': sample['text'],
            'doc_name': sample['doc_name'],
            'page_num': sample['page_num'],
            'image_path': sample['image_path']
        }

def create_train_val_split(dataset, val_ratio=0.2):
    """
    Create training and validation datasets.

    Args:
        dataset: Full dataset
        val_ratio: Ratio of validation data

    Returns:
        Tuple of (train_dataset, val_dataset)
    """
    # Set random seed for reproducibility
    random.seed(42)

    # Get indices
    indices = list(range(len(dataset)))
    random.shuffle(indices)

    # Calculate split point
    split = int(len(dataset) * (1 - val_ratio))
    train_indices = indices[:split]
    val_indices = indices[split:]

    # Create datasets
    train_dataset = torch.utils.data.Subset(dataset, train_indices)
    val_dataset = torch.utils.data.Subset(dataset, val_indices)

    logger.info(f"Split dataset into {len(train_dataset)} training and {len(val_dataset)} validation samples")

    return train_dataset, val_dataset

def create_data_loaders(train_dataset, val_dataset, batch_size=4):
    """
    Create data loaders for training and validation.

    Args:
        train_dataset: Training dataset
        val_dataset: Validation dataset
        batch_size: Batch size

    Returns:
        Tuple of (train_loader, val_loader)
    """
    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=2 if torch.cuda.is_available() else 0,
        pin_memory=torch.cuda.is_available()
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2 if torch.cuda.is_available() else 0,
        pin_memory=torch.cuda.is_available()
    )

    return train_loader, val_loader

#------------------------------------------------------------------------------
# Main Execution
#------------------------------------------------------------------------------

def main():
    """
    Main function to execute the complete workflow.
    """
    print("="*80)
    print("HISTORICAL DOCUMENT OCR TRANSCRIPTION LOADER")
    print("="*80)
    print("This script loads document images and their transcriptions,")
    print("creating a dataset for OCR model training.")
    print("\nChecking for existing transcriptions...")

    # Set paths
    base_path = '/content'

    # Load transcriptions and images
    page_transcriptions = load_transcriptions_and_images(base_path)

    # If no transcriptions were found, offer to upload
    if not page_transcriptions:
        print("\nNo transcriptions were successfully mapped to images.")
        print("You can upload transcription files manually and try again.")

        upload_now = input("Upload transcription files now? (y/n): ")
        if upload_now.lower() == 'y':
            page_transcriptions = upload_transcription_files()

            # Show example pages if transcriptions were successfully loaded this time
            if page_transcriptions:
                show_example_pages(page_transcriptions)
    else:
        # If transcriptions were found, show examples
        show_example_pages(page_transcriptions)

    # Create dataset if we have page transcriptions
    if page_transcriptions:
        print("\nCreating OCR dataset...")

        # Define transforms
        transform = transforms.Compose([
            transforms.Resize((384, 384)),
            transforms.ToTensor(),
        ])

        # Create dataset
        dataset = OCRDataset(page_transcriptions, transform=transform)

        # Create train-val split
        train_dataset, val_dataset = create_train_val_split(dataset)

        # Create data loaders
        batch_size = 4
        train_loader, val_loader = create_data_loaders(train_dataset, val_dataset, batch_size)

        print(f"\nDataset created successfully:")
        print(f"  - Total samples: {len(dataset)}")
        print(f"  - Training samples: {len(train_dataset)}")
        print(f"  - Validation samples: {len(val_dataset)}")

        # Show a sample batch if available
        if len(train_loader) > 0:
            sample_batch = next(iter(train_loader))
            print(f"\nSample batch shape: {sample_batch['image'].shape}")

            # Display a sample image and its text
            sample_idx = 0
            sample_img = sample_batch['image'][sample_idx].permute(1, 2, 0).numpy()
            sample_text = sample_batch['text'][sample_idx]
            sample_doc = sample_batch['doc_name'][sample_idx]
            sample_page = sample_batch['page_num'][sample_idx]

            plt.figure(figsize=(10, 14))
            plt.imshow(sample_img)
            plt.title(f"Document: {sample_doc}, Page: {sample_page}")
            plt.axis('off')
            plt.show()

            print(f"Sample text: {sample_text[:200]}...")
            print("\nDataset is ready for OCR model training!")
    else:
        print("\nNo dataset created - no transcriptions were successfully mapped to images.")

    print("\nProcess completed.")
    return page_transcriptions

# Execute the main function if run as a script
if __name__ == "__main__":
    page_transcriptions = main()

"""# 3. BETO Spanish Language Post-Processing"""

import torch
import re
import json
import os
from transformers import BertForMaskedLM, BertTokenizer
from tqdm.notebook import tqdm
import pandas as pd
import logging

# Define paths
base_path = '/content'
lexicon_path = os.path.join(base_path, 'historical_spanish_lexicon')
os.makedirs(lexicon_path, exist_ok=True)

# Initialize logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Initialize BETO model and tokenizer
print("Loading BETO model for Spanish language processing...")
tokenizer = BertTokenizer.from_pretrained("dccuchile/bert-base-spanish-wwm-cased", do_lower_case=False)
model = BertForMaskedLM.from_pretrained("dccuchile/bert-base-spanish-wwm-cased")
model.eval()

# Move to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

class SpanishHistoricalNormalizer:
    """
    Normalizes Spanish historical text with period-specific rules.

    Attributes:
        historical_mappings: Dictionary mapping historical to modern Spanish characters
        abbreviations: Dictionary mapping historical abbreviations to their expanded forms
        common_words: Set of common Spanish words to initialize the lexicon
        lexicon: Set of valid Spanish words for correction
    """

    def __init__(self):
        """Initialize the normalizer with Spanish historical text mappings."""
        # Initialize mappings for historical Spanish text
        self.historical_mappings = {
            # u/v interchangeability
            'u': 'v',
            'v': 'u',
            # i/j interchangeability
            'i': 'j',
            'j': 'i',
            # Long s
            'ſ': 's',
            # Ligatures
            'æ': 'ae',
            'œ': 'oe',
            # Other common variations
            'ç': 'z',
        }

        # Common abbreviations in historical Spanish
        self.abbreviations = {
            'q̃': 'que',
            'ẽ': 'en',
            'õ': 'on',
            'ã': 'an',
            'p̃': 'per',
            'ñ': 'nn',  # In some early texts
        }

        # Common words in historical Spanish to build initial lexicon
        self.common_words = {
            'el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas',  # Articles
            'de', 'del', 'a', 'al', 'en', 'con', 'por', 'para',     # Prepositions
            'y', 'e', 'o', 'u', 'ni', 'que', 'si', 'no',           # Conjunctions
            'este', 'esta', 'estos', 'estas', 'ese', 'esa',        # Demonstratives
            'mi', 'tu', 'su', 'mis', 'tus', 'sus',                 # Possessives
        }

        # Initialize lexicon with common words
        self.lexicon = set(self.common_words)

    def expand_lexicon_from_transcriptions(self, texts):
        """
        Expand lexicon using ground truth transcriptions.

        Args:
            texts: List of transcription texts

        Returns:
            Updated lexicon set
        """
        for text in texts:
            # Normalize and split into words
            clean_text = self.normalize_text(text)
            words = clean_text.split()

            # Add words to lexicon
            for word in words:
                if len(word) > 1:  # Skip single-letter words
                    self.lexicon.add(word.lower())

        logger.info(f"Expanded lexicon to {len(self.lexicon)} words")

        # Save lexicon to file
        lexicon_file = os.path.join(lexicon_path, "spanish_historical_lexicon.txt")
        with open(lexicon_file, 'w', encoding='utf-8') as f:
            for word in sorted(self.lexicon):
                f.write(f"{word}\n")

        return self.lexicon

    def normalize_text(self, text):
        """
        Apply basic normalization to text.

        Args:
            text: Input text

        Returns:
            Normalized text
        """
        # Convert to lowercase
        text = text.lower()

        # Replace historical characters
        for old, new in self.historical_mappings.items():
            text = text.replace(old, new)

        # Replace abbreviations
        for abbr, full in self.abbreviations.items():
            text = text.replace(abbr, full)

        # Remove punctuation and digits
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\d', ' ', text)

        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def find_closest_match(self, word, max_distance=2):
        """
        Find closest match in lexicon.

        Args:
            word: Word to match
            max_distance: Maximum Levenshtein distance for matches

        Returns:
            Closest match in lexicon
        """
        if word.lower() in self.lexicon:
            return word

        # Try variations
        for old, new in self.historical_mappings.items():
            if old in word.lower():
                variation = word.lower().replace(old, new)
                if variation in self.lexicon:
                    return variation

        # Use edit distance if no exact match
        min_distance = float('inf')
        best_match = word

        for lex_word in self.lexicon:
            if abs(len(lex_word) - len(word)) <= max_distance:
                distance = self.levenshtein_distance(word.lower(), lex_word)
                if distance < min_distance and distance <= max_distance:
                    min_distance = distance
                    best_match = lex_word

        return best_match if min_distance <= max_distance else word

    def levenshtein_distance(self, s1, s2):
        """
        Calculate edit distance between two strings.

        Args:
            s1: First string
            s2: Second string

        Returns:
            Levenshtein distance
        """
        if len(s1) < len(s2):
            return self.levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]

    def correct_with_lexicon(self, text):
        """
        Correct text using lexicon.

        Args:
            text: Input text

        Returns:
            Corrected text
        """
        words = re.findall(r'\b\w+\b', text)
        corrected_words = {}

        for word in words:
            if len(word) > 1:  # Skip single-letter words
                corrected = self.find_closest_match(word)
                if corrected != word:
                    corrected_words[word] = corrected

        # Apply corrections
        for word, corrected in corrected_words.items():
            text = re.sub(r'\b' + re.escape(word) + r'\b', corrected, text)

        return text

class BETOPostProcessor:
    """
    Use BETO to correct OCR errors in historical Spanish text.

    Attributes:
        model: BETO model
        tokenizer: BETO tokenizer
        normalizer: Spanish historical normalizer
        device: Device to run the model on
    """

    def __init__(self, model, tokenizer, normalizer):
        """
        Initialize the post-processor.

        Args:
            model: BETO model
            tokenizer: BETO tokenizer
            normalizer: Spanish historical normalizer
        """
        self.model = model
        self.tokenizer = tokenizer
        self.normalizer = normalizer

        # Move to correct device
        self.device = next(model.parameters()).device

    def mask_and_predict(self, text, confidence_threshold=0.8):
        """
        Mask words with low confidence and predict using BETO.

        Args:
            text: Input text
            confidence_threshold: Threshold for applying corrections

        Returns:
            Corrected text
        """
        # Tokenize the text
        tokens = self.tokenizer.tokenize(text)
        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)

        # Find words to mask (simple heuristic: short words, rare characters)
        mask_candidates = []
        for i, token in enumerate(tokens):
            # Skip punctuation and special tokens
            if token.startswith('##') or token in {'[CLS]', '[SEP]', '[PAD]', '[MASK]'}:
                continue

            # Check if token contains rare characters or is very short
            if any(c in token for c in 'ſæœ') or len(token) <= 2:
                mask_candidates.append(i)

        # No candidates to mask
        if not mask_candidates:
            return text

        corrections = {}

        # Process each mask candidate
        for mask_idx in mask_candidates:
            # Create a copy of tokens and mask the candidate
            masked_tokens = tokens.copy()
            original_token = masked_tokens[mask_idx]
            masked_tokens[mask_idx] = '[MASK]'

            # Convert to IDs and create tensor
            masked_ids = self.tokenizer.convert_tokens_to_ids(masked_tokens)
            masked_tensor = torch.tensor([masked_ids]).to(self.device)

            # Get predictions from BETO
            with torch.no_grad():
                outputs = self.model(masked_tensor)
                predictions = outputs[0]

            # Get predicted token
            masked_idx = masked_tokens.index('[MASK]')
            predicted_ids = torch.argsort(predictions[0, masked_idx], descending=True)[:5]
            predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_ids)

            # Get top prediction with confidence
            top_token = predicted_tokens[0]
            top_prob = torch.softmax(predictions[0, masked_idx], dim=0)[predicted_ids[0]].item()

            # Apply correction if confidence is high enough
            if top_prob > confidence_threshold and top_token != original_token:
                corrections[original_token] = (top_token, top_prob)

        # Apply corrections to the text
        corrected_text = text
        for original, (correction, prob) in corrections.items():
            corrected_text = re.sub(r'\b' + re.escape(original) + r'\b', correction, corrected_text)

        return corrected_text

    def fix_historical_specific_errors(self, text):
        """
        Fix specific errors common in historical Spanish OCR.

        Args:
            text: Input text

        Returns:
            Corrected text
        """
        rules = [
            # Fix common OCR errors with long s
            (r'\bdeſ', 'des'),
            (r'ſ', 's'),
            # Fix u/v confusion
            (r'\bvn\b', 'un'),
            (r'\bvna\b', 'una'),
            # Fix i/j confusion
            (r'\bj\b', 'i'),
            # Fix common abbreviations
            (r'q̃', 'que'),
            # Fix ligatures
            (r'æ', 'ae'),
            (r'œ', 'oe'),
            # Fix spacing issues
            (r' +', ' ')
        ]

        for pattern, replacement in rules:
            text = re.sub(pattern, replacement, text)

        return text

    def process_text(self, text):
        """
        Apply full post-processing pipeline to OCR text.

        Args:
            text: Input text

        Returns:
            Processed text
        """
        # Fix historical specific errors
        text = self.fix_historical_specific_errors(text)

        # Apply lexicon-based correction
        text = self.normalizer.correct_with_lexicon(text)

        # Apply BETO-based correction
        text = self.mask_and_predict(text)

        return text

def test_post_processing(ocr_results, num_examples=3):
    """
    Test post-processing on example OCR outputs.

    Args:
        ocr_results: List of OCR results
        num_examples: Number of examples to test

    Returns:
        List of post-processing results
    """
    examples = ocr_results[:num_examples]

    results = []
    for example in examples:
        ocr_text = example['ocr_text']

        # Apply post-processing
        corrected_text = post_processor.process_text(ocr_text)

        # Calculate improvement
        ground_truth = example['ground_truth']

        # Calculate metrics for original and corrected text
        original_metrics = calculate_metrics([ocr_text], [ground_truth])
        corrected_metrics = calculate_metrics([corrected_text], [ground_truth])

        # Store results
        results.append({
            'doc_name': example['doc_name'],
            'page_num': example['page_num'],
            'ground_truth': ground_truth[:200] + "..." if len(ground_truth) > 200 else ground_truth,
            'ocr_text': ocr_text[:200] + "..." if len(ocr_text) > 200 else ocr_text,
            'corrected_text': corrected_text[:200] + "..." if len(corrected_text) > 200 else corrected_text,
            'original_cer': original_metrics['cer'],
            'corrected_cer': corrected_metrics['cer'],
            'original_wer': original_metrics['wer'],
            'corrected_wer': corrected_metrics['wer'],
            'improvement_cer': original_metrics['cer'] - corrected_metrics['cer'],
            'improvement_wer': original_metrics['wer'] - corrected_metrics['wer']
        })

    # Display results
    for result in results:
        print(f"\nDocument: {result['doc_name']}, Page: {result['page_num']}")
        print(f"Ground truth: {result['ground_truth']}")
        print(f"Original OCR: {result['ocr_text']}")
        print(f"Corrected OCR: {result['corrected_text']}")
        print(f"Original CER: {result['original_cer']:.4f}, Original WER: {result['original_wer']:.4f}")
        print(f"Corrected CER: {result['corrected_cer']:.4f}, Corrected WER: {result['corrected_wer']:.4f}")
        print(f"Improvement - CER: {result['improvement_cer']:.4f}, WER: {result['improvement_wer']:.4f}")

    return results

def apply_post_processing(ocr_results):
    """
    Apply post-processing to all OCR results.

    Args:
        ocr_results: List of OCR results

    Returns:
        Tuple of (processed_results, original_metrics, corrected_metrics)
    """
    processed_results = []

    for result in tqdm(ocr_results, desc="Applying post-processing"):
        ocr_text = result['ocr_text']

        # Apply post-processing
        corrected_text = post_processor.process_text(ocr_text)

        # Store processed result
        processed_result = result.copy()
        processed_result['corrected_text'] = corrected_text
        processed_results.append(processed_result)

    # Calculate metrics
    ground_truths = [r['ground_truth'] for r in processed_results]
    ocr_texts = [r['ocr_text'] for r in processed_results]
    corrected_texts = [r['corrected_text'] for r in processed_results]

    # Calculate metrics for original OCR
    original_metrics = calculate_metrics(ocr_texts, ground_truths)

    # Calculate metrics for corrected OCR
    corrected_metrics = calculate_metrics(corrected_texts, ground_truths)

    print(f"\nOriginal metrics:")
    print(f"Character Error Rate (CER): {original_metrics['cer']:.4f}")
    print(f"Word Error Rate (WER): {original_metrics['wer']:.4f}")

    print(f"\nCorrected metrics:")
    print(f"Character Error Rate (CER): {corrected_metrics['cer']:.4f}")
    print(f"Word Error Rate (WER): {corrected_metrics['wer']:.4f}")

    print(f"\nImprovement:")
    print(f"CER improvement: {original_metrics['cer'] - corrected_metrics['cer']:.4f}")
    print(f"WER improvement: {original_metrics['wer'] - corrected_metrics['wer']:.4f}")

    # Add metrics to results
    for i in range(len(processed_results)):
        processed_results[i]['original_cer'] = original_metrics['cer_values'][i]
        processed_results[i]['original_wer'] = original_metrics['wer_values'][i]
        processed_results[i]['corrected_cer'] = corrected_metrics['cer_values'][i]
        processed_results[i]['corrected_wer'] = corrected_metrics['wer_values'][i]
        processed_results[i]['improvement_cer'] = original_metrics['cer_values'][i] - corrected_metrics['cer_values'][i]
        processed_results[i]['improvement_wer'] = original_metrics['wer_values'][i] - corrected_metrics['wer_values'][i]

    return processed_results, original_metrics, corrected_metrics

# Create the post-processor pipeline
print("Building Spanish historical post-processor...")
normalizer = SpanishHistoricalNormalizer()

# Expand lexicon from transcriptions
print("Expanding lexicon from transcriptions...")
texts = [data['text'] for data in page_transcriptions.values()]
normalizer.expand_lexicon_from_transcriptions(texts)

# Create BETO post-processor
post_processor = BETOPostProcessor(model, tokenizer, normalizer)

# Test on examples
print("\nTesting post-processing on example OCR outputs...")
post_processing_examples = test_post_processing(val_results, num_examples=2)

# Apply to all validation results
print("\nApplying post-processing to all validation results...")
processed_results, original_metrics, corrected_metrics = apply_post_processing(val_results)

# Create a DataFrame with the results
df_processed = pd.DataFrame(processed_results)
print("\nProcessed OCR Results:")
display(df_processed.head())

"""# 4. Complete End-to-End Pipeline with Metrics Visualization"""



# Initialize BETO model and tokenizer for post-processing
print("Loading BETO model for Spanish language processing...")
beto_tokenizer = BertTokenizer.from_pretrained("dccuchile/bert-base-spanish-wwm-cased", do_lower_case=False)
beto_model = BertForMaskedLM.from_pretrained("dccuchile/bert-base-spanish-wwm-cased")
beto_model.eval()

# Move BETO model to GPU if available
beto_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
beto_model = beto_model.to(beto_device)

# Create the post-processor with the BETO model
print("Building Spanish historical post-processor...")
normalizer = SpanishHistoricalNormalizer()

# Expand lexicon from transcriptions
print("Expanding lexicon from transcriptions...")
texts = [data['text'] for data in page_transcriptions.values()]
normalizer.expand_lexicon_from_transcriptions(texts)

# Create BETO post-processor using the BETO model (not TrOCR model)
post_processor = BETOPostProcessor(beto_model, beto_tokenizer, normalizer)

# Test on examples
print("\nTesting post-processing on example OCR outputs...")
post_processing_examples = test_post_processing(val_results, num_examples=2)

# Apply to all validation results
print("\nApplying post-processing to all validation results...")
processed_results, original_metrics, corrected_metrics = apply_post_processing(val_results)

# Create a DataFrame with the results
df_processed = pd.DataFrame(processed_results)
print("\nProcessed OCR Results:")
display(df_processed.head())

# Create the full pipeline - Use the TrOCR model here, not the BETO model
print("Creating full OCR pipeline...")
pipeline = HybridOCRPipeline(model, processor, post_processor)  # model here is the TrOCR model

# Create visualizations and report
print("Creating visualizations and report...")
combined_metrics = {
    'original': original_metrics,
    'corrected': corrected_metrics,
    'improvement': {
        'cer': original_metrics['cer'] - corrected_metrics['cer'],
        'wer': original_metrics['wer'] - corrected_metrics['wer']
    }
}

# -*- coding: utf-8 -*-
"""
Fixed OCR script addressing TrOCR model choice and generation length.
"""

import os
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from tqdm.notebook import tqdm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
import re
import logging
import difflib
# Removed google.colab import as it's environment-specific
# from google.colab import files
import random
from IPython.display import display

# Import docx handling from the second script section
try:
    import docx
    # logger.info("python-docx is already installed") # Assuming logger is set up elsewhere
except ImportError:
    # logger.info("Installing python-docx package...") # Assuming logger is set up elsewhere
    # !pip install python-docx # Installation should be done in a separate setup cell
    import docx


# Set up logging (copied from the second script section)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# --- Directory Setup (First part of your script) ---
# (Keep your directory setup code as it was)
# Define base paths
base_path = '/content'
pdf_folder = os.path.join(base_path, 'pdfs')
output_base_path = os.path.join(base_path, 'ocr_data')
transcriptions_path = os.path.join(base_path, 'transcriptions')
training_data_path = os.path.join(output_base_path, 'training_data')
results_path = os.path.join(output_base_path, 'results')
page_transcription_dir = os.path.join(output_base_path, 'page_transcriptions') # Added based on TranscriptionLoader

# Create main directories
os.makedirs(pdf_folder, exist_ok=True)
os.makedirs(os.path.join(output_base_path, "images"), exist_ok=True)
os.makedirs(os.path.join(output_base_path, "processed_images"), exist_ok=True)
os.makedirs(os.path.join(output_base_path, "binary_images"), exist_ok=True)
os.makedirs(training_data_path, exist_ok=True)
os.makedirs(transcriptions_path, exist_ok=True)
os.makedirs(results_path, exist_ok=True)
os.makedirs(page_transcription_dir, exist_ok=True) # Ensure this is created

# List of subdirectories (assuming these names are correct)
subdirectories = [
    "Buendia - Instruccion-1",
    "Constituciones sinodales Calahorra 1602-2",
    "Ezcaray - Vozes-3",
    "Mendo - Principe perfecto-4",
    "Paredes - Reglas generales-5",
    "PORCONES.228.35  1636-6" # Note the double space here, ensure it matches your actual folder name
]

# Define paths for image subdirectories
binary_images_path = os.path.join(output_base_path, "binary_images")
processed_images_path = os.path.join(output_base_path, "processed_images")
images_path = os.path.join(output_base_path, "images")

# Create subdirectories inside image folders
for subdir in subdirectories:
    os.makedirs(os.path.join(binary_images_path, subdir), exist_ok=True)
    os.makedirs(os.path.join(processed_images_path, subdir), exist_ok=True)
    os.makedirs(os.path.join(images_path, subdir), exist_ok=True)

print("Setup complete! All directories created.")


# --- Utility and Class Definitions (Combined from both script sections) ---

# Check device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# (Keep HistoricalDocumentDataset class as it was)
class HistoricalDocumentDataset(Dataset):
    """
    Dataset for historical document pages.

    Attributes:
        page_keys: List of keys for page data
        page_data: Dictionary mapping keys to page data
        transform: Optional transform to apply to images
        max_length: Maximum text length to return
    """
    def __init__(self, page_data, transform=None, max_length=512):
        """
        Initialize the dataset.

        Args:
            page_data: Dictionary mapping (doc_name, page_num) to page data
            transform: Optional transform to apply to images
            max_length: Maximum text length
        """
        self.page_keys = list(page_data.keys())
        self.page_data = page_data
        self.transform = transform
        self.max_length = max_length # Note: This max_length is for the ground truth text, not generation

    def __len__(self) -> int:
        """Return the number of pages in the dataset."""
        return len(self.page_keys)

    def __getitem__(self, idx: int) -> dict:
        """
        Get a dataset item.

        Args:
            idx: Index of the item to get

        Returns:
            Dictionary with image, text, and metadata
        """
        key = self.page_keys[idx]
        data = self.page_data.get(key) # Use .get for safety

        if data is None:
             # Handle case where key might be missing after split/filtering
             logger.error(f"Key {key} not found in page_data during __getitem__")
             # Return a dummy item or raise an error
             dummy_image = torch.zeros((3, 384, 384)) # Match expected tensor shape
             return {
                 'image': dummy_image,
                 'text': "",
                 'doc_name': key[0] if isinstance(key, tuple) else "Unknown",
                 'page_num': key[1] if isinstance(key, tuple) else 0,
                 'image_path': "None"
             }

        # Load image (use processed image by default)
        image_path = data.get('processed_image', None) # Use .get for safety
        if image_path and os.path.exists(image_path):
            try:
                image = Image.open(image_path).convert('RGB')
                if self.transform:
                    image = self.transform(image)
            except Exception as e:
                logger.error(f"Error loading image {image_path}: {e}")
                # Create a dummy image tensor in case of error
                image = torch.zeros((3, 384, 384)) # Match expected tensor shape
        else:
             logger.error(f"Image path missing or invalid for key {key}: {image_path}")
             image = torch.zeros((3, 384, 384)) # Match expected tensor shape


        # Get text (truncate if necessary)
        text = data.get('text', "") # Use .get for safety
        # Text truncation based on self.max_length happens here if needed,
        # but this limit doesn't affect model generation length.
        if len(text) > self.max_length:
            text = text[:self.max_length]


        # Return image, text, and metadata
        return {
            'image': image,
            'text': text,
            'doc_name': key[0],
            'page_num': key[1],
            'image_path': image_path if image_path else "None"
        }

# (Keep get_transforms function as it was)
def get_transforms():
    """
    Get transforms for image preprocessing.

    Returns:
        torchvision.transforms.Compose object with transforms
    """
    return transforms.Compose([
        transforms.Resize((384, 384)),  # TrOCR default size
        transforms.ToTensor(),
        # Add normalization if required by the specific TrOCR model processor
        # transforms.Normalize(mean=[...], std=[...]) # Example
    ])

# (Keep create_train_val_split function as it was)
def create_train_val_split(page_transcriptions, val_ratio=0.2, seed=42):
    """
    Split the dataset into training and validation sets.

    Args:
        page_transcriptions: Dictionary mapping (doc_name, page_num) to page data
        val_ratio: Ratio of validation data
        seed: Random seed for reproducibility

    Returns:
        Tuple of (train_data, val_data)
    """
    import random
    random.seed(seed)

    # Create list of keys
    keys = list(page_transcriptions.keys())
    random.shuffle(keys)

    # Calculate split point
    split_idx = int(len(keys) * (1 - val_ratio))

    # Split keys
    train_keys = keys[:split_idx]
    val_keys = keys[split_idx:]

    # Create dictionaries
    train_data = {k: page_transcriptions[k] for k in train_keys if k in page_transcriptions}
    val_data = {k: page_transcriptions[k] for k in val_keys if k in page_transcriptions}

    print(f"Total pages: {len(page_transcriptions)}")
    print(f"Train set: {len(train_data)} pages")
    print(f"Validation set: {len(val_data)} pages")

    return train_data, val_data


# --- FIXED ---
def initialize_trocr():
    """
    Initialize the TrOCR model and processor.
    Uses a model suited for PRINTED text.
    """
    print("Loading TrOCR model and processor for PRINTED text...")
    # --- FIX 1: Changed model from handwritten to printed ---
    # Common choices: "microsoft/trocr-base-printed" or "microsoft/trocr-large-printed"
    # You might need to experiment to see which works best. Let's try base-printed first.
    model_name = "microsoft/trocr-base-printed"
    try:
        processor = TrOCRProcessor.from_pretrained(model_name)
        model = VisionEncoderDecoderModel.from_pretrained(model_name)
    except OSError as e:
        logger.error(f"Error loading model {model_name}: {e}")
        logger.warning("Attempting to load 'microsoft/trocr-base-stage1' as a fallback.")
        # Fallback to a general OCR model if the specific one fails
        model_name = "microsoft/trocr-base-stage1" # General OCR pretraining
        processor = TrOCRProcessor.from_pretrained(model_name)
        model = VisionEncoderDecoderModel.from_pretrained(model_name)

    # Configure model
    model.config.decoder_start_token_id = processor.tokenizer.bos_token_id
    model.config.pad_token_id = processor.tokenizer.pad_token_id
    # Ensure the model is in evaluation mode if not training
    model.eval()
    model = model.to(device)
    print(f"Loaded model: {model_name}")

    return model, processor

# (Keep calculate_metrics function as it was)
# Note: Your Levenshtein implementation calculates distance between lists of items (words/chars).
# Consider using a library like `jiwer` for potentially more robust WER/CER calculation.
# !pip install jiwer
# import jiwer
# def calculate_metrics_jiwer(predictions, references):
#     wer = jiwer.wer(references, predictions)
#     cer = jiwer.cer(references, predictions)
#     return {'cer': cer, 'wer': wer}
def calculate_metrics(predictions, references):
    """
    Calculate Character Error Rate (CER) and Word Error Rate (WER).
    """
    def normalize_text(text):
        # Basic normalization, you might need more specific rules for historical text
        text = text.lower()
        # Keep basic punctuation that might be important, remove others
        text = re.sub(r'[^\w\s.,;!?-]', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    # Using python-Levenshtein could be faster if installed (!pip install python-Levenshtein)
    # Otherwise, use the provided implementation
    def levenshtein_distance(s1, s2):
        # If s1/s2 are strings, calculate char distance
        # If s1/s2 are lists (of words), calculate word distance
        if len(s1) < len(s2):
            return levenshtein_distance(s2, s1)
        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        return previous_row[-1]

    cer_values = []
    wer_values = []

    for pred, ref in zip(predictions, references):
        pred_norm = normalize_text(pred)
        ref_norm = normalize_text(ref)

        # CER
        if len(ref_norm) > 0:
            cer = levenshtein_distance(pred_norm, ref_norm) / len(ref_norm)
        else:
            cer = 1.0 if len(pred_norm) > 0 else 0.0
        cer_values.append(cer)

        # WER
        pred_words = pred_norm.split()
        ref_words = ref_norm.split()
        if len(ref_words) > 0:
            # Calculate word-level Levenshtein distance
            wer = levenshtein_distance(pred_words, ref_words) / len(ref_words)
        else:
            wer = 1.0 if len(pred_words) > 0 else 0.0
        wer_values.append(wer)

    avg_cer = np.mean(cer_values) if cer_values else 1.0
    avg_wer = np.mean(wer_values) if wer_values else 1.0

    return {
        'cer': avg_cer,
        'wer': avg_wer,
        'cer_values': cer_values, # Return individual values for detailed results df
        'wer_values': wer_values
    }


# --- FIXED ---
def test_trocr(model, processor, page_transcriptions, num_examples=3):
    """
    Test TrOCR on a few example pages.
    Uses increased max_length for generation.
    """
    if not page_transcriptions:
        print("No validation data provided for testing.")
        return []

    # Ensure num_examples is not more than available data
    num_examples = min(num_examples, len(page_transcriptions))
    if num_examples == 0:
        print("No examples to test.")
        return []

    # Select a few examples
    example_keys = random.sample(list(page_transcriptions.keys()), num_examples)

    results = []
    print(f"Testing on {num_examples} examples...")

    for key in example_keys:
        data = page_transcriptions[key]
        doc_name, page_num = key

        image_path = data.get('processed_image')
        if not image_path or not os.path.exists(image_path):
             logger.warning(f"Skipping test for {key} due to missing image: {image_path}")
             continue

        # Load image
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            logger.error(f"Error loading image {image_path} for test: {e}")
            continue

        # Process with TrOCR
        try:
            pixel_values = processor(image, return_tensors="pt").pixel_values.to(device)
             # --- FIX 2: Increased max_length ---
            generated_ids = model.generate(pixel_values, max_length=512) # Increased from 128
            # Decode the generated IDs
            generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        except Exception as e:
             logger.error(f"Error during TrOCR processing for {key}: {e}")
             generated_text = "[OCR Error]"


        # Ground truth for comparison
        ground_truth = data.get('text', "[Ground Truth Missing]")
        gt_preview = ground_truth[:200] + "..." if len(ground_truth) > 200 else ground_truth

        # Store results
        results.append({
            'doc_name': doc_name,
            'page_num': page_num,
            'ground_truth': ground_truth, # Store full GT for potential later use
            'gt_preview': gt_preview,     # Store preview for printing
            'ocr_text': generated_text,
            'image_path': image_path
        })

    # Display results
    for result in results:
        print(f"\nDocument: {result['doc_name']}, Page: {result['page_num']}")
        print(f"Ground truth: {result['gt_preview']}")
        print(f"OCR text: {result['ocr_text']}")

        # Display image
        try:
            plt.figure(figsize=(8, 8)) # Adjusted size
            img_display = Image.open(result['image_path'])
            plt.imshow(np.array(img_display), cmap='gray')
            plt.title(f"{result['doc_name']} - Page {result['page_num']}")
            plt.axis('off')
            plt.show()
        except Exception as e:
            print(f"Error displaying image {result['image_path']}: {e}")


    return results # Return the results list

# --- FIXED ---
def run_trocr_ocr(model, processor, page_transcriptions):
    """
    Run TrOCR OCR on all pages in the provided dataset dictionary.
    Uses increased max_length for generation.
    """
    results = []
    if not page_transcriptions:
        print("No data provided to run_trocr_ocr.")
        return [], {'cer': 1.0, 'wer': 1.0, 'cer_values': [], 'wer_values': []}

    # Ensure model is in evaluation mode
    model.eval()

    with torch.no_grad(): # Disable gradient calculation for inference
        for key in tqdm(page_transcriptions.keys(), desc="Running TrOCR OCR"):
            data = page_transcriptions[key]
            doc_name, page_num = key

            image_path = data.get('processed_image')
            if not image_path or not os.path.exists(image_path):
                logger.warning(f"Skipping OCR for {key} due to missing image: {image_path}")
                # Append a placeholder result to keep indices aligned for metrics
                results.append({
                    'doc_name': doc_name,
                    'page_num': page_num,
                    'ground_truth': data.get('text', ""),
                    'ocr_text': "[OCR Skipped - No Image]",
                    'image_path': image_path if image_path else "None",
                    'cer': 1.0, # Assign max error
                    'wer': 1.0  # Assign max error
                })
                continue

            try:
                image = Image.open(image_path).convert('RGB')

                # Process with TrOCR
                pixel_values = processor(image, return_tensors="pt").pixel_values.to(device)
                # --- FIX 2: Increased max_length ---
                generated_ids = model.generate(pixel_values, max_length=512) # Increased from 128
                generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

                # Store results
                results.append({
                    'doc_name': doc_name,
                    'page_num': page_num,
                    'ground_truth': data.get('text', ""),
                    'ocr_text': generated_text,
                    'image_path': image_path
                    # CER/WER will be added after calculating metrics
                })
            except Exception as e:
                logger.error(f"Error processing {image_path}: {e}")
                # Append placeholder on error
                results.append({
                    'doc_name': doc_name,
                    'page_num': page_num,
                    'ground_truth': data.get('text', ""),
                    'ocr_text': "[OCR Error]",
                    'image_path': image_path,
                    'cer': 1.0, # Assign max error
                    'wer': 1.0  # Assign max error
                })

    # Calculate metrics only if results were generated
    if not results:
         return [], {'cer': 1.0, 'wer': 1.0, 'cer_values': [], 'wer_values': []}

    predictions = [r['ocr_text'] for r in results]
    references = [r['ground_truth'] for r in results]

    # Use the updated calculate_metrics which returns individual scores
    metrics = calculate_metrics(predictions, references)

    print(f"\nEvaluation metrics:")
    print(f"Character Error Rate (CER): {metrics['cer']:.4f}")
    print(f"Word Error Rate (WER): {metrics['wer']:.4f}")

    # Add individual metrics back to the results list
    # Check if metrics calculation produced expected lists
    if len(metrics.get('cer_values', [])) == len(results) and len(metrics.get('wer_values', [])) == len(results):
        for i in range(len(results)):
            # Only update if CER/WER weren't already set due to skipping/errors
            if 'cer' not in results[i]:
                 results[i]['cer'] = metrics['cer_values'][i]
            if 'wer' not in results[i]:
                 results[i]['wer'] = metrics['wer_values'][i]
    else:
        logger.warning(f"Mismatch between results ({len(results)}) and calculated metrics (CER: {len(metrics.get('cer_values',[]))}, WER: {len(metrics.get('wer_values',[]))}). Skipping detailed metric assignment.")
        # Assign average metrics as fallback if individual assignment fails
        for i in range(len(results)):
            if 'cer' not in results[i]: results[i]['cer'] = metrics.get('cer', 1.0)
            if 'wer' not in results[i]: results[i]['wer'] = metrics.get('wer', 1.0)


    return results, metrics


# --- Transcription Loading and Image Finding (Copied from second script section) ---
# (Keep normalize_document_name, get_best_transcription_match functions as they were)
# (Keep TranscriptionLoader class as it was)
# (Keep extract_doc_info, find_all_images functions as they were)
# Make sure the paths used in these functions align with your `base_path` structure

# Example instantiation (adjust paths as needed)
transcription_loader = TranscriptionLoader(
    transcription_dir=transcriptions_path,
    page_transcription_dir=page_transcription_dir
)


# --- Main Execution Logic ---

print("Finding images and transcriptions...")
# Find images
processed_images, binary_images, image_mapping = find_all_images(base_path=base_path)
logger.info(f"Found {len(processed_images)} processed images and {len(binary_images)} binary images.")
logger.info(f"Created mapping for {len(image_mapping)} unique document pages.")

# Find and prepare transcriptions
transcription_loader.find_transcription_files()
# The create_page_transcriptions function needs modification to use the found image_mapping
# and handle the structure correctly. Assuming max 6 pages per doc based on original code.

# Let's refine create_page_transcriptions from the original script to use the structure
# (This replaces the version potentially defined in the second script block)
def create_page_transcriptions_modified(loader: TranscriptionLoader,
                                      image_mapping: Dict[Tuple[str, int], Dict[str, str]],
                                      max_pages_per_doc: Optional[int] = None
                                      ) -> Dict[Tuple[str, int], Dict[str, Any]]:
    """
    Create page-level transcriptions based on image mapping and full transcriptions.
    Splits based on found pages, not estimated lines. Requires page markers in transcriptions.

    Args:
        loader: Instance of TranscriptionLoader
        image_mapping: Dictionary mapping (doc_name, page_num) to image paths
        max_pages_per_doc: Optional limit on pages per document (for filtering)

    Returns:
        Dictionary mapping (doc_name, page_num) to page data including transcription
    """
    page_transcriptions_output = {}
    doc_names = sorted(list(set(doc_name for doc_name, _ in image_mapping.keys())))
    logger.info(f"Attempting to create page transcriptions for {len(doc_names)} documents.")

    for doc_name in doc_names:
        full_transcription = loader.read_transcription(doc_name)
        if not full_transcription:
            logger.warning(f"No full transcription found for {doc_name}, skipping page creation.")
            continue

        # Find pages relevant to this document in the image mapping
        doc_pages = sorted([page_num for d, page_num in image_mapping.keys() if d == doc_name])
        if max_pages_per_doc:
            doc_pages = [p for p in doc_pages if p <= max_pages_per_doc]

        if not doc_pages:
            logger.warning(f"No image pages found for {doc_name} in mapping.")
            continue

        logger.info(f"Processing {doc_name}, found pages: {doc_pages}")

        # --- Page Splitting Logic ---
        # This requires a specific format in your transcription files (e.g., "PDF p2", "Page 2")
        # If your files don't have markers, this simple split won't work well.
        # Using regex to find page markers like "PDF pX" or similar
        page_markers = list(re.finditer(r'(?:PDF p|Page |p)(\d+)', full_transcription, re.IGNORECASE))

        if not page_markers:
             logger.warning(f"No standard page markers found in transcription for {doc_name}. Attempting naive split.")
             # --- Naive Split (Fallback - Less Accurate) ---
             lines = full_transcription.split('\n')
             total_lines = len(lines)
             num_found_pages = len(doc_pages)
             lines_per_page = max(1, total_lines // num_found_pages if num_found_pages > 0 else total_lines)

             for i, page_num in enumerate(doc_pages):
                 key = (doc_name, page_num)
                 if key not in image_mapping: continue

                 start_line = i * lines_per_page
                 end_line = min((i + 1) * lines_per_page, total_lines) if i < num_found_pages - 1 else total_lines
                 page_text = '\n'.join(lines[start_line:end_line]).strip()

                 if not page_text:
                     logger.warning(f"Empty page text generated for {key} via naive split.")

                 page_transcriptions_output[key] = {
                     'text': page_text,
                     'processed_image': image_mapping[key].get('processed'),
                     'binary_image': image_mapping[key].get('binary'),
                     'transcription_file': loader.doc_transcription_map.get(
                         get_best_transcription_match(doc_name, loader.available_transcriptions), '')
                 }

        else:
            # --- Split by Markers ---
            current_page_num = 0
            start_index = 0
            marker_map = {} # Map marker page num to text start/end

            # Assume first page starts at the beginning unless marker says otherwise
            first_marker_page = int(page_markers[0].group(1))
            if first_marker_page > 1:
                 marker_map[1] = (0, page_markers[0].start())

            for i, marker in enumerate(page_markers):
                page_num_marker = int(marker.group(1))
                start_index = marker.end() # Text starts after the marker usually

                if i + 1 < len(page_markers):
                    end_index = page_markers[i+1].start()
                else:
                    end_index = len(full_transcription) # To the end of the document

                marker_map[page_num_marker] = (start_index, end_index)

            # Assign text based on mapped markers and found image pages
            for page_num in doc_pages:
                 key = (doc_name, page_num)
                 if key not in image_mapping: continue

                 if page_num in marker_map:
                      start_idx, end_idx = marker_map[page_num]
                      page_text = full_transcription[start_idx:end_idx].strip()

                      # Basic cleanup: remove potential leading/trailing whitespace or markers
                      page_text = re.sub(r'^(?:PDF p|Page |p)\d+\s*', '', page_text, flags=re.IGNORECASE).strip()

                      if not page_text:
                          logger.warning(f"Empty page text generated for {key} using markers.")

                      page_transcriptions_output[key] = {
                          'text': page_text,
                          'processed_image': image_mapping[key].get('processed'),
                          'binary_image': image_mapping[key].get('binary'),
                          'transcription_file': loader.doc_transcription_map.get(
                              get_best_transcription_match(doc_name, loader.available_transcriptions), '')
                      }
                 else:
                      logger.warning(f"No specific marker found for page {page_num} in {doc_name}. Skipping.")


    logger.info(f"Created {len(page_transcriptions_output)} page-level transcriptions entries.")
    return page_transcriptions_output


# Create page transcriptions using the modified function
page_transcriptions = create_page_transcriptions_modified(transcription_loader, image_mapping)


# Create dataset splits using the generated page_transcriptions
print("\nCreating train-validation split...")
if not page_transcriptions:
     print("ERROR: No page transcriptions were generated. Cannot create train/val split.")
     # Handle error state - perhaps exit or use dummy data
     train_data, val_data = {}, {}
else:
    train_data, val_data = create_train_val_split(page_transcriptions)


# --- Run OCR Pipeline ---
print("\nInitializing TrOCR model...")
model, processor = initialize_trocr()

print("\nTesting TrOCR on a few validation examples...")
# Pass the actual val_data dictionary to test_trocr
test_results = test_trocr(model, processor, val_data, num_examples=2)

print("\nRunning TrOCR on all validation pages...")
# Pass the actual val_data dictionary to run_trocr_ocr
val_results, val_metrics = run_trocr_ocr(model, processor, val_data)

# --- Display Results ---
if val_results:
    # Create a DataFrame with the results
    # Select columns to display initially for clarity
    display_cols = ['doc_name', 'page_num', 'cer', 'wer', 'ocr_text', 'ground_truth']
    df_results = pd.DataFrame(val_results)
     # Ensure ground truth doesn't overflow display
    df_results['ground_truth_preview'] = df_results['ground_truth'].apply(lambda x: x[:100] + "..." if len(x)>100 else x)
    df_results['ocr_text_preview'] = df_results['ocr_text'].apply(lambda x: x[:100] + "..." if len(x)>100 else x)

    print("\nOCR Validation Results (DataFrame):")
    display(df_results[['doc_name', 'page_num', 'cer', 'wer', 'ocr_text_preview', 'ground_truth_preview']].head())

    # Save results to CSV
    results_csv_path = os.path.join(results_path, 'ocr_validation_results.csv')
    try:
        df_results.to_csv(results_csv_path, index=False, encoding='utf-8')
        print(f"Validation results saved to {results_csv_path}")
    except Exception as e:
        print(f"Error saving results to CSV: {e}")

else:
    print("No validation results generated.")

print("\n--- OCR Process Completed ---")

# --- Post-Processing and Full Pipeline (Placeholder) ---
# You would integrate your BETO post-processing here, operating on df_results['ocr_text']
# print("\nLoading BETO model for Spanish language processing...")
# print("Building Spanish historical post-processor...")
# ... (Your post-processing code) ...
# corrected_texts = apply_post_processing(df_results['ocr_text'])
# df_results['corrected_text'] = corrected_texts
# corrected_metrics = calculate_metrics(df_results['corrected_text'], df_results['ground_truth'])
# print("Corrected Metrics:", corrected_metrics)

# print("\nCreating full OCR pipeline...")
# print("Creating visualizations and report...")
# ... (Your report generation code) ...