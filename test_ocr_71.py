# -*- coding: utf-8 -*-
"""test_ocr_71.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gHAvG1LLkjSYt2JLOMJL3mdZDsdyPQi3
"""

import os

import os

# Define base paths
base_path = '/content'
pdf_folder = os.path.join(base_path, 'pdfs')
output_base_path = os.path.join(base_path, 'ocr_data')
transcriptions_path = os.path.join(base_path, 'transcriptions')
training_data_path = os.path.join(output_base_path, 'training_data')
results_path = os.path.join(output_base_path, 'results')

# Create main directories
os.makedirs(pdf_folder, exist_ok=True)
os.makedirs(os.path.join(output_base_path, "images"), exist_ok=True)
os.makedirs(os.path.join(output_base_path, "processed_images"), exist_ok=True)
os.makedirs(os.path.join(output_base_path, "binary_images"), exist_ok=True)
os.makedirs(training_data_path, exist_ok=True)
os.makedirs(transcriptions_path, exist_ok=True)
os.makedirs(results_path, exist_ok=True)

# List of subdirectories to create in binary_images, processed_images, and images
subdirectories = [
    "Buendia - Instruccion-1",
    "Constituciones sinodales Calahorra 1602-2",
    "Ezcaray - Vozes-3",
    "Mendo - Principe perfecto-4",
    "Paredes - Reglas generales-5",
    "PORCONES.228.35  1636-6"
]

# Define paths for binary_images, processed_images, and images directories
binary_images_path = os.path.join(output_base_path, "binary_images")
processed_images_path = os.path.join(output_base_path, "processed_images")
images_path = os.path.join(output_base_path, "images")

# Create subdirectories inside binary_images
for subdir in subdirectories:
    os.makedirs(os.path.join(binary_images_path, subdir), exist_ok=True)

# Create subdirectories inside processed_images
for subdir in subdirectories:
    os.makedirs(os.path.join(processed_images_path, subdir), exist_ok=True)

# Create subdirectories inside images
for subdir in subdirectories:
    os.makedirs(os.path.join(images_path, subdir), exist_ok=True)

print("Setup complete! All directories created.")

"""# Installation and Setup

"""

# Install required system packages for PDF processing
!apt-get update
!apt-get install -y poppler-utils

# Install required Python packages
!pip install pdf2image pytesseract opencv-python matplotlib tqdm
!pip install torch torchvision
!pip install transformers datasets
!pip install pillow seaborn pandas
!pip install PyPDF2 python-docx

"""# 1. Enhanced Transcription Loader with DOCX Support"""

import os
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from tqdm.notebook import tqdm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
import re

# Check device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

class HistoricalDocumentDataset(Dataset):
    """
    Dataset for historical document pages.

    Attributes:
        page_keys: List of keys for page data
        page_data: Dictionary mapping keys to page data
        transform: Optional transform to apply to images
        max_length: Maximum text length to return
    """
    def __init__(self, page_data, transform=None, max_length=512):
        """
        Initialize the dataset.

        Args:
            page_data: Dictionary mapping (doc_name, page_num) to page data
            transform: Optional transform to apply to images
            max_length: Maximum text length
        """
        self.page_keys = list(page_data.keys())
        self.page_data = page_data
        self.transform = transform
        self.max_length = max_length

    def __len__(self) -> int:
        """Return the number of pages in the dataset."""
        return len(self.page_keys)

    def __getitem__(self, idx: int) -> dict:
        """
        Get a dataset item.

        Args:
            idx: Index of the item to get

        Returns:
            Dictionary with image, text, and metadata
        """
        key = self.page_keys[idx]
        data = self.page_data[key]

        # Load image (use processed image by default)
        image_path = data['processed_image']
        try:
            image = Image.open(image_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
        except Exception as e:
            logging.error(f"Error loading image {image_path}: {e}")
            # Create a dummy image in case of error
            image = Image.new('RGB', (384, 384), color='white')
            if self.transform:
                image = self.transform(image)

        # Get text (truncate if necessary)
        text = data['text']
        if len(text) > self.max_length:
            text = text[:self.max_length]

        # Return image, text, and metadata
        return {
            'image': image,
            'text': text,
            'doc_name': key[0],
            'page_num': key[1],
            'image_path': image_path
        }

def get_transforms():
    """
    Get transforms for image preprocessing.

    Returns:
        torchvision.transforms.Compose object with transforms
    """
    return transforms.Compose([
        transforms.Resize((384, 384)),  # TrOCR default size
        transforms.ToTensor(),
    ])

def create_train_val_split(page_transcriptions, val_ratio=0.2, seed=42):
    """
    Split the dataset into training and validation sets.

    Args:
        page_transcriptions: Dictionary mapping (doc_name, page_num) to page data
        val_ratio: Ratio of validation data
        seed: Random seed for reproducibility

    Returns:
        Tuple of (train_data, val_data)
    """
    import random
    random.seed(seed)

    # Create list of keys
    keys = list(page_transcriptions.keys())
    random.shuffle(keys)

    # Calculate split point
    split_idx = int(len(keys) * (1 - val_ratio))

    # Split keys
    train_keys = keys[:split_idx]
    val_keys = keys[split_idx:]

    # Create dictionaries
    train_data = {k: page_transcriptions[k] for k in train_keys}
    val_data = {k: page_transcriptions[k] for k in val_keys}

    print(f"Train set: {len(train_data)} pages")
    print(f"Validation set: {len(val_data)} pages")

    return train_data, val_data

def initialize_trocr():
    """
    Initialize the TrOCR model and processor.

    Returns:
        Tuple of (model, processor)
    """
    print("Loading TrOCR model and processor...")
    processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")
    model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten")

    # Configure model
    model.config.decoder_start_token_id = processor.tokenizer.bos_token_id
    model.config.pad_token_id = processor.tokenizer.pad_token_id
    model = model.to(device)

    return model, processor

def calculate_metrics(predictions, references):
    """
    Calculate Character Error Rate (CER) and Word Error Rate (WER).

    Args:
        predictions: List of predicted texts
        references: List of reference texts

    Returns:
        Dictionary with CER and WER metrics
    """
    def normalize_text(text):
        # Lowercase and remove punctuation
        text = text.lower()
        text = re.sub(r'[^\w\s]', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def levenshtein_distance(s1, s2):
        # Calculate edit distance
        if len(s1) < len(s2):
            return levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]

    # Calculate CER and WER for each pair
    cer_values = []
    wer_values = []

    for pred, ref in zip(predictions, references):
        # Normalize texts
        pred_norm = normalize_text(pred)
        ref_norm = normalize_text(ref)

        # Calculate CER
        if len(ref_norm) > 0:
            cer = levenshtein_distance(pred_norm, ref_norm) / len(ref_norm)
        else:
            cer = 1.0 if len(pred_norm) > 0 else 0.0

        # Calculate WER
        pred_words = pred_norm.split()
        ref_words = ref_norm.split()

        if len(ref_words) > 0:
            wer = levenshtein_distance(pred_words, ref_words) / len(ref_words)
        else:
            wer = 1.0 if len(pred_words) > 0 else 0.0

        cer_values.append(cer)
        wer_values.append(wer)

    # Calculate averages
    avg_cer = sum(cer_values) / len(cer_values) if cer_values else 1.0
    avg_wer = sum(wer_values) / len(wer_values) if wer_values else 1.0

    return {
        'cer': avg_cer,
        'wer': avg_wer,
        'cer_values': cer_values,
        'wer_values': wer_values
    }

def test_trocr(model, processor, page_transcriptions, num_examples=3):
    """
    Test TrOCR on a few example pages.

    Args:
        model: TrOCR model
        processor: TrOCR processor
        page_transcriptions: Dictionary mapping (doc_name, page_num) to page data
        num_examples: Number of examples to test

    Returns:
        List of results
    """
    # Select a few examples
    example_keys = list(page_transcriptions.keys())[:num_examples]

    results = []

    for key in example_keys:
        data = page_transcriptions[key]
        doc_name, page_num = key

        # Load image
        image = Image.open(data['processed_image']).convert('RGB')

        # Process with TrOCR
        pixel_values = processor(image, return_tensors="pt").pixel_values.to(device)
        generated_ids = model.generate(pixel_values, max_length=128)
        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

        # Store results
        results.append({
            'doc_name': doc_name,
            'page_num': page_num,
            'ground_truth': data['text'][:200] + "..." if len(data['text']) > 200 else data['text'],
            'ocr_text': generated_text[:200] + "..." if len(generated_text) > 200 else generated_text,
            'image_path': data['processed_image']
        })

    # Display results
    for result in results:
        print(f"\nDocument: {result['doc_name']}, Page: {result['page_num']}")
        print(f"Ground truth: {result['ground_truth']}")
        print(f"OCR text: {result['ocr_text']}")

        # Display image
        plt.figure(figsize=(10, 10))
        image = Image.open(result['image_path'])
        plt.imshow(np.array(image), cmap='gray')
        plt.title(f"{result['doc_name']} - Page {result['page_num']}")
        plt.axis('off')
        plt.show()

    return results

def run_trocr_ocr(model, processor, page_transcriptions):
    """
    Run TrOCR OCR on all pages in the dataset.

    Args:
        model: TrOCR model
        processor: TrOCR processor
        page_transcriptions: Dictionary mapping (doc_name, page_num) to page data

    Returns:
        Tuple of (results, metrics)
    """
    results = []

    for key in tqdm(page_transcriptions.keys(), desc="Running TrOCR OCR"):
        data = page_transcriptions[key]
        doc_name, page_num = key

        # Load image
        try:
            image = Image.open(data['processed_image']).convert('RGB')

            # Process with TrOCR
            pixel_values = processor(image, return_tensors="pt").pixel_values.to(device)
            generated_ids = model.generate(pixel_values, max_length=128)
            generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

            # Store results
            results.append({
                'doc_name': doc_name,
                'page_num': page_num,
                'ground_truth': data['text'],
                'ocr_text': generated_text,
                'image_path': data['processed_image']
            })
        except Exception as e:
            logging.error(f"Error processing {data['processed_image']}: {e}")

    # Calculate metrics
    predictions = [r['ocr_text'] for r in results]
    references = [r['ground_truth'] for r in results]
    metrics = calculate_metrics(predictions, references)

    print(f"\nEvaluation metrics:")
    print(f"Character Error Rate (CER): {metrics['cer']:.4f}")
    print(f"Word Error Rate (WER): {metrics['wer']:.4f}")

    # Add metrics to results
    for i in range(len(results)):
        results[i]['cer'] = metrics['cer_values'][i]
        results[i]['wer'] = metrics['wer_values'][i]

    return results, metrics

# Execute TrOCR component
print("Creating train-validation split...")
train_data, val_data = create_train_val_split(page_transcriptions)

print("\nInitializing TrOCR model...")
model, processor = initialize_trocr()

print("\nTesting TrOCR on a few examples...")
test_results = test_trocr(model, processor, val_data, num_examples=2)

print("\nRunning TrOCR on all validation pages...")
val_results, val_metrics = run_trocr_ocr(model, processor, val_data)

# Create a DataFrame with the results
df_results = pd.DataFrame(val_results)
print("\nOCR Results:")
display(df_results.head())

"""# 2. TrOCR-Based OCR Component"""

# -*- coding: utf-8 -*-
"""
Historical Document OCR Transcription Loader

This script provides functionality to:
1. Find and load document images from processed and binary directories
2. Extract and normalize document names from image paths
3. Load transcriptions from .txt or .docx files
4. Match document images with corresponding transcriptions
5. Create page-level transcriptions for OCR training
6. Build a PyTorch dataset suitable for OCR model training

Author: Claude (2024)
"""

import os
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
from PIL import Image
from typing import Dict, List, Tuple, Optional, Union, Any
import logging
import difflib
from google.colab import files
import random
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from IPython.display import display

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Import docx if available, otherwise install it
try:
    import docx
    logger.info("python-docx is already installed")
except ImportError:
    logger.info("Installing python-docx package...")
    !pip install python-docx
    import docx

#------------------------------------------------------------------------------
# Document Name Normalization Utilities
#------------------------------------------------------------------------------

def normalize_document_name(doc_name: str) -> str:
    """
    Normalize a document name by removing common suffixes and standardizing format.

    Args:
        doc_name: Raw document name that might include page numbers or other suffixes

    Returns:
        Normalized document name
    """
    # Remove page number suffixes like "-1", "-2", etc.
    doc_name = re.sub(r'-\d+$', '', doc_name)

    # Remove additional identifiers that might be in document names but not in transcription files
    doc_name = re.sub(r' - Instruccion$', '', doc_name)
    doc_name = re.sub(r' - Principe perfecto$', '', doc_name)
    doc_name = re.sub(r' - Vozes$', '', doc_name)
    doc_name = re.sub(r' - Reglas generales$', '', doc_name)
    doc_name = re.sub(r' sinodales Calahorra 1602$', ' sinodales', doc_name)

    return doc_name.strip()

def get_best_transcription_match(doc_name: str, available_transcriptions: List[str]) -> Optional[str]:
    """
    Find the best matching transcription filename for a document name using fuzzy matching.

    Args:
        doc_name: Document name to match
        available_transcriptions: List of available transcription filenames (without extensions)

    Returns:
        Best matching transcription name or None if no good match found
    """
    if not available_transcriptions:
        return None

    # Normalize document name
    normalized_doc_name = normalize_document_name(doc_name)

    # Try exact match first with variations
    for transcription in available_transcriptions:
        # Try direct match
        if normalized_doc_name == transcription:
            return transcription

        # Try without "transcription" suffix
        if normalized_doc_name == transcription.replace(' transcription', ''):
            return transcription

    # If no exact match, try fuzzy matching
    best_match = None
    best_ratio = 0.0

    for transcription in available_transcriptions:
        # Compare normalized doc name with transcription name (without "transcription" suffix)
        base_transcription = transcription.replace(' transcription', '')

        # Calculate similarity ratio
        ratio = difflib.SequenceMatcher(None, normalized_doc_name.lower(), base_transcription.lower()).ratio()

        # Also try with the full transcription name
        full_ratio = difflib.SequenceMatcher(None, normalized_doc_name.lower(), transcription.lower()).ratio()
        ratio = max(ratio, full_ratio)

        if ratio > best_ratio and ratio > 0.6:  # 0.6 is a threshold for good matches
            best_ratio = ratio
            best_match = transcription

    if best_match:
        logger.info(f"Fuzzy matched '{doc_name}' to '{best_match}' with confidence {best_ratio:.2f}")

    return best_match

#------------------------------------------------------------------------------
# Transcription Loader Class
#------------------------------------------------------------------------------

class TranscriptionLoader:
    """
    A class for loading transcriptions from various file formats (.txt, .docx)
    and mapping them to corresponding document images.
    """

    def __init__(self, transcription_dir: str, page_transcription_dir: str):
        """
        Initialize the transcription loader.

        Args:
            transcription_dir: Directory containing original transcription files
            page_transcription_dir: Directory to store page-level transcriptions
        """
        self.transcription_dir = transcription_dir
        self.page_transcription_dir = page_transcription_dir
        os.makedirs(page_transcription_dir, exist_ok=True)

        # Mapping from document names to transcription files
        self.doc_transcription_map = {}
        # List of available transcription basenames (without extensions)
        self.available_transcriptions = []

        # Format handlers
        self.format_handlers = {
            '.txt': self._read_txt_file,
            '.docx': self._read_docx_file
        }

    def find_transcription_files(self) -> Dict[str, str]:
        """
        Find all transcription files in the transcription directory.

        Returns:
            Dictionary mapping document names to transcription file paths
        """
        # Clear existing maps
        self.doc_transcription_map = {}
        self.available_transcriptions = []

        for file in os.listdir(self.transcription_dir):
            file_path = os.path.join(self.transcription_dir, file)
            if not os.path.isfile(file_path):
                continue

            file_ext = os.path.splitext(file)[1].lower()
            if file_ext in self.format_handlers:
                # Get document name without extension
                doc_name = os.path.splitext(file)[0]
                self.doc_transcription_map[doc_name] = file_path
                self.available_transcriptions.append(doc_name)

        if self.doc_transcription_map:
            logger.info(f"Found {len(self.doc_transcription_map)} transcription files "
                       f"({', '.join(f'{ext}' for ext in set(os.path.splitext(f)[1] for f in self.doc_transcription_map.values()))})")
            logger.info(f"Available transcriptions: {', '.join(self.available_transcriptions)}")
        else:
            logger.warning("No transcription files found in directory: " + self.transcription_dir)

        return self.doc_transcription_map

    def _read_txt_file(self, file_path: str) -> str:
        """
        Read text content from a .txt file.

        Args:
            file_path: Path to the .txt file

        Returns:
            Text content as a string
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            # Try different encodings if utf-8 fails
            encodings = ['latin-1', 'iso-8859-1', 'cp1252']
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        logger.warning(f"File {file_path} decoded using {encoding} instead of utf-8")
                        return f.read()
                except UnicodeDecodeError:
                    continue

            # If all else fails, use binary mode and ignore errors
            logger.error(f"Could not decode {file_path} with common encodings, using binary mode")
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read()

    def _read_docx_file(self, file_path: str) -> str:
        """
        Read text content from a .docx file.

        Args:
            file_path: Path to the .docx file

        Returns:
            Text content as a string
        """
        try:
            doc = docx.Document(file_path)
            full_text = []

            # Extract text from paragraphs
            for para in doc.paragraphs:
                full_text.append(para.text)

            # Also check for tables, which might contain transcription data
            for table in doc.tables:
                for row in table.rows:
                    row_texts = [cell.text for cell in row.cells if cell.text.strip()]
                    if row_texts:
                        full_text.append(' | '.join(row_texts))

            return '\n'.join(full_text)
        except Exception as e:
            logger.error(f"Error reading .docx file {file_path}: {str(e)}")
            return f"ERROR: Could not read {file_path}"

    def read_transcription(self, doc_name: str) -> Optional[str]:
        """
        Read transcription for a specific document, handling name variations.

        Args:
            doc_name: Document name (will be normalized and matched)

        Returns:
            Transcription content or None if not found
        """
        # Find best matching transcription
        best_match = get_best_transcription_match(doc_name, self.available_transcriptions)

        if best_match and best_match in self.doc_transcription_map:
            file_path = self.doc_transcription_map[best_match]
            file_ext = os.path.splitext(file_path)[1].lower()

            if file_ext in self.format_handlers:
                logger.info(f"Found transcription for '{doc_name}' -> '{best_match}'")
                return self.format_handlers[file_ext](file_path)

        logger.warning(f"No transcription found for document {doc_name}")
        return None

    def create_page_transcriptions(self, image_mapping: Dict[Tuple[str, int], Dict[str, str]],
                                  max_pages: int = 6) -> Dict[Tuple[str, int], Dict[str, Any]]:
        """
        Create page-level transcriptions by splitting document transcriptions.

        Args:
            image_mapping: Dictionary mapping (doc_name, page_num) to image paths
            max_pages: Maximum number of pages per document

        Returns:
            Dictionary mapping (doc_name, page_num) to page data including transcription
        """
        page_transcriptions = {}

        # Get unique document names
        doc_names = set(doc_name for doc_name, _ in image_mapping.keys())
        logger.info(f"Creating page transcriptions for {len(doc_names)} documents: {', '.join(doc_names)}")

        for doc_name in doc_names:
            # Get transcription for this document
            transcription = self.read_transcription(doc_name)
            if not transcription:
                continue

            # Count lines and estimate lines per page
            lines = transcription.split('\n')
            total_lines = len(lines)
            lines_per_page = max(1, total_lines // max_pages)

            # Split transcription into pages
            for page_num in range(1, max_pages + 1):
                key = (doc_name, page_num)
                if key not in image_mapping:
                    continue

                # Calculate line range for this page
                start_line = (page_num - 1) * lines_per_page
                end_line = min(page_num * lines_per_page, total_lines)

                # Extract page text
                page_text = '\n'.join(lines[start_line:end_line])

                # Save to file
                page_file = os.path.join(self.page_transcription_dir, f"{doc_name}_page_{page_num:03d}.txt")
                with open(page_file, 'w', encoding='utf-8') as f:
                    f.write(page_text)

                # Store in dictionary
                if key in image_mapping:
                    page_transcriptions[key] = {
                        'text': page_text,
                        'processed_image': image_mapping[key].get('processed'),
                        'binary_image': image_mapping[key].get('binary'),
                        'transcription_file': self.doc_transcription_map.get(
                            get_best_transcription_match(doc_name, self.available_transcriptions), '')
                    }

        logger.info(f"Created {len(page_transcriptions)} page-level transcriptions")
        return page_transcriptions

#------------------------------------------------------------------------------
# Image Finding Functions
#------------------------------------------------------------------------------

def extract_doc_info(image_path: str) -> Tuple[Optional[str], Optional[int]]:
    """
    Extract document name and page number from image path.

    This function supports multiple naming patterns:
    - Document_Name_page_XXX.jpg
    - document-name-XXX.jpg (where XXX is the page number)

    Args:
        image_path: Path to the image file

    Returns:
        Tuple of (document_name, page_number) or (None, None) if not parsable
    """
    # Extract filename from path
    filename = os.path.basename(image_path)
    # Remove extension
    filename_without_ext = os.path.splitext(filename)[0]

    # Try different patterns

    # Pattern 1: Document_Name_page_XXX.jpg
    match = re.match(r'(.+?)_page_(\d+)', filename_without_ext)
    if match:
        doc_name = match.group(1)
        page_num = int(match.group(2))
        return doc_name, page_num

    # Pattern 2: Detect document name and page number from directory structure
    parent_dir = os.path.basename(os.path.dirname(image_path))
    if parent_dir and filename_without_ext.endswith(('-1', '-2', '-3', '-4', '-5', '-6')):
        # Extract page number from the end of filename
        page_match = re.search(r'-(\d+)$', filename_without_ext)
        if page_match:
            page_num = int(page_match.group(1))
            # Use parent directory as document name
            return parent_dir, page_num

    # Try to extract from arbitrary filename with page number at the end
    page_match = re.search(r'[-_]p(?:age)?[-_]?(\d+)$', filename_without_ext, re.IGNORECASE)
    if page_match:
        page_num = int(page_match.group(1))
        # Remove page suffix from filename to get document name
        doc_name = re.sub(r'[-_]p(?:age)?[-_]?\d+$', '', filename_without_ext, flags=re.IGNORECASE)
        return doc_name, page_num

    logger.warning(f"Could not parse document name and page from: {image_path}")
    return None, None


def find_all_images(base_path: str = '/content') -> Tuple[List[str], List[str], Dict[Tuple[str, int], Dict[str, str]]]:
    """
    Find all processed and binary images in the specified directories.

    Args:
        base_path: Base directory for all data

    Returns:
        Tuple of (processed_images, binary_images, image_mapping)
    """
    output_base_path = os.path.join(base_path, 'ocr_data')
    processed_dir = os.path.join(output_base_path, "processed_images")
    binary_dir = os.path.join(output_base_path, "binary_images")

    processed_images = []
    binary_images = []
    image_mapping = {}  # Maps (doc_name, page_num) to {'processed': path, 'binary': path}

    # Track document names to help with debugging
    found_doc_names = set()
    doc_page_counts = {}

    # Find processed images
    if os.path.exists(processed_dir):
        for doc_dir in os.listdir(processed_dir):
            doc_path = os.path.join(processed_dir, doc_dir)
            if os.path.isdir(doc_path):
                for img_file in os.listdir(doc_path):
                    if img_file.endswith(('.jpg', '.png')):
                        img_path = os.path.join(doc_path, img_file)
                        processed_images.append(img_path)

                        # Try to extract document name and page number
                        doc_name, page_num = extract_doc_info(img_path)

                        # If that fails, use the directory name as document name
                        # and generate a sequential page number
                        if doc_name is None or page_num is None:
                            doc_name = doc_dir
                            if doc_name not in doc_page_counts:
                                doc_page_counts[doc_name] = 0
                            doc_page_counts[doc_name] += 1
                            page_num = doc_page_counts[doc_name]

                        found_doc_names.add(doc_name)
                        if (doc_name, page_num) not in image_mapping:
                            image_mapping[(doc_name, page_num)] = {'processed': img_path}
    else:
        logger.warning(f"Processed images directory not found: {processed_dir}")

    # Find binary images
    if os.path.exists(binary_dir):
        for doc_dir in os.listdir(binary_dir):
            doc_path = os.path.join(binary_dir, doc_dir)
            if os.path.isdir(doc_path):
                for img_file in os.listdir(doc_path):
                    if img_file.endswith(('.jpg', '.png')):
                        img_path = os.path.join(doc_path, img_file)
                        binary_images.append(img_path)

                        # Extract document name and page number
                        doc_name, page_num = extract_doc_info(img_path)

                        # If that fails, use the directory name as document name
                        # and try to match with an existing processed image
                        if doc_name is None or page_num is None:
                            doc_name = doc_dir
                            # Try to find matching processed image
                            for (d, p), mapping in image_mapping.items():
                                if d == doc_name and os.path.basename(mapping.get('processed', '')) == img_file:
                                    page_num = p
                                    break

                            # If still can't find, generate a sequential page number
                            if page_num is None:
                                if doc_name not in doc_page_counts:
                                    doc_page_counts[doc_name] = 0
                                doc_page_counts[doc_name] += 1
                                page_num = doc_page_counts[doc_name]

                        found_doc_names.add(doc_name)
                        if (doc_name, page_num) in image_mapping:
                            image_mapping[(doc_name, page_num)]['binary'] = img_path
                        else:
                            image_mapping[(doc_name, page_num)] = {'binary': img_path}
    else:
        logger.warning(f"Binary images directory not found: {binary_dir}")

    # Log what we found
    logger.info(f"Found {len(processed_images)} processed images and {len(binary_images)} binary images")
    logger.info(f"Identified {len(found_doc_names)} unique documents: {', '.join(sorted(found_doc_names))}")
    logger.info(f"Created {len(image_mapping)} document-page mappings")

    return processed_images, binary_images, image_mapping

#------------------------------------------------------------------------------
# Main Function to Load Transcriptions and Images
#------------------------------------------------------------------------------

def load_transcriptions_and_images(base_path: str = '/content') -> Dict[Tuple[str, int], Dict[str, Any]]:
    """
    Main function to load transcriptions and map them to images.

    Args:
        base_path: Base directory for all data

    Returns:
        Dictionary mapping (doc_name, page_num) to page data including transcription
    """
    # Define paths
    output_base_path = os.path.join(base_path, 'ocr_data')
    transcriptions_path = os.path.join(base_path, 'transcriptions')
    page_transcriptions_path = os.path.join(base_path, 'page_transcriptions')

    # Create directory for page-level transcriptions
    os.makedirs(page_transcriptions_path, exist_ok=True)

    # Find all images
    logger.info("Finding images...")
    processed_images, binary_images, image_mapping = find_all_images(base_path)

    # Print document names found in image mapping for debugging
    unique_docs = sorted(set(doc_name for doc_name, _ in image_mapping.keys()))
    logger.info(f"Document names found in image paths: {', '.join(unique_docs)}")

    # Load transcriptions
    logger.info("Loading transcriptions...")
    loader = TranscriptionLoader(transcriptions_path, page_transcriptions_path)
    doc_transcription_map = loader.find_transcription_files()

    # Create page-level transcriptions
    logger.info("Creating page-level transcriptions...")
    page_transcriptions = loader.create_page_transcriptions(image_mapping)

    # Check if we have any successful matches
    if not page_transcriptions:
        logger.warning("No page transcriptions created - checking for issues")

        # Check transcription directory contents
        logger.info("Files in transcription directory:")
        for file in os.listdir(transcriptions_path):
            logger.info(f"  - {file}")

        # Try manual matching for each document
        logger.info("Attempting manual matching:")
        for doc_name in unique_docs:
            normalized = normalize_document_name(doc_name)
            best_match = get_best_transcription_match(doc_name, loader.available_transcriptions)
            logger.info(f"  - '{doc_name}' -> normalized: '{normalized}', best match: '{best_match}'")

    # Create dataframe for easy access
    rows = []
    for (doc_name, page_num), data in page_transcriptions.items():
        text_preview = data['text'][:100] + "..." if len(data['text']) > 100 else data['text']
        rows.append({
            'doc_name': doc_name,
            'page_num': page_num,
            'processed_image': data.get('processed_image', ''),
            'binary_image': data.get('binary_image', ''),
            'text_preview': text_preview,
            'text_length': len(data['text']),
            'transcription_file': data.get('transcription_file', '')
        })

    # Create and display dataframe
    if rows:
        df_pages = pd.DataFrame(rows)
        logger.info("Dataset overview:")
        display(df_pages)
    else:
        logger.warning("No rows for dataframe - mapping failed")

    return page_transcriptions

#------------------------------------------------------------------------------
# Visualization Functions
#------------------------------------------------------------------------------

def visualize_page_transcription(doc_name: str, page_num: int, page_transcriptions: Dict[Tuple[str, int], Dict[str, Any]]):
    """
    Visualize a page transcription alongside its image.

    Args:
        doc_name: Document name
        page_num: Page number
        page_transcriptions: Dictionary mapping (doc_name, page_num) to page data
    """
    key = (doc_name, page_num)
    if key not in page_transcriptions:
        print(f"No data found for document '{doc_name}', page {page_num}")
        return

    page_data = page_transcriptions[key]

    # Display image if available
    if 'processed_image' in page_data:
        img_path = page_data['processed_image']
        if img_path and os.path.exists(img_path):
            try:
                img = Image.open(img_path)
                plt.figure(figsize=(10, 14))
                plt.imshow(np.array(img))
                plt.title(f"Document: {doc_name}, Page: {page_num}")
                plt.axis('off')
                plt.show()
            except Exception as e:
                print(f"Error displaying image: {str(e)}")

    # Display transcription
    if 'text' in page_data:
        print("\nTranscription:")
        print("-" * 80)
        print(page_data['text'])
        print("-" * 80)
    else:
        print("No transcription available")

def show_example_pages(page_transcriptions, num_examples=2):
    """
    Display example pages with their transcriptions.

    Args:
        page_transcriptions: Dictionary mapping (doc_name, page_num) to page data
        num_examples: Number of examples to show
    """
    # Check if we have any transcriptions
    if not page_transcriptions:
        print("No page transcriptions available to display")
        return

    # Get unique document names
    doc_names = sorted(set(doc_name for doc_name, _ in page_transcriptions.keys()))
    examples_shown = 0

    print(f"\nShowing examples from {len(doc_names)} documents:")

    for doc_name in doc_names:
        # Find pages for this document
        pages = [page_num for (d, page_num) in page_transcriptions.keys() if d == doc_name]

        if not pages:
            continue

        # Show first page for each document, up to num_examples
        visualize_page_transcription(doc_name, min(pages), page_transcriptions)
        examples_shown += 1

        if examples_shown >= num_examples:
            break

#------------------------------------------------------------------------------
# Manual Transcription Upload Function
#------------------------------------------------------------------------------

def upload_transcription_files():
    """
    Allow user to manually upload transcription files if automatic loading fails.

    Returns:
        Dictionary mapping (doc_name, page_num) to page data including transcription
    """
    print("Please upload your transcription files (.txt or .docx)...")
    uploaded = files.upload()

    # Save uploaded files to the transcriptions directory
    transcriptions_dir = os.path.join('/content', 'transcriptions')
    os.makedirs(transcriptions_dir, exist_ok=True)

    for filename, content in uploaded.items():
        filepath = os.path.join(transcriptions_dir, filename)
        with open(filepath, 'wb') as f:
            f.write(content)
        print(f"Saved: {filename} -> {filepath}")

    print("\nRunning the transcription loader again with newly uploaded files...")
    return load_transcriptions_and_images()

#------------------------------------------------------------------------------
# OCR Dataset Creation
#------------------------------------------------------------------------------

class OCRDataset(Dataset):
    """
    Dataset class for OCR training.
    """
    def __init__(self, page_transcriptions, transform=None, max_length=512):
        """
        Initialize the dataset.

        Args:
            page_transcriptions: Dictionary mapping (doc_name, page_num) to page data
            transform: Optional transform to be applied to the images
            max_length: Maximum sequence length for the transcriptions
        """
        self.samples = []
        for (doc_name, page_num), data in page_transcriptions.items():
            if 'processed_image' in data and 'text' in data:
                img_path = data['processed_image']
                text = data['text']

                if img_path and os.path.exists(img_path):
                    self.samples.append({
                        'doc_name': doc_name,
                        'page_num': page_num,
                        'image_path': img_path,
                        'text': text[:max_length] if len(text) > max_length else text
                    })

        self.transform = transform
        self.max_length = max_length

        logger.info(f"Created dataset with {len(self.samples)} samples")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]

        # Load image
        try:
            image = Image.open(sample['image_path']).convert('RGB')
        except Exception as e:
            logger.error(f"Error loading image {sample['image_path']}: {str(e)}")
            # Create a blank image as fallback
            image = Image.new('RGB', (384, 384), color='white')

        # Apply transforms if any
        if self.transform:
            image = self.transform(image)

        return {
            'image': image,
            'text': sample['text'],
            'doc_name': sample['doc_name'],
            'page_num': sample['page_num'],
            'image_path': sample['image_path']
        }

def create_train_val_split(dataset, val_ratio=0.2):
    """
    Create training and validation datasets.

    Args:
        dataset: Full dataset
        val_ratio: Ratio of validation data

    Returns:
        Tuple of (train_dataset, val_dataset)
    """
    # Set random seed for reproducibility
    random.seed(42)

    # Get indices
    indices = list(range(len(dataset)))
    random.shuffle(indices)

    # Calculate split point
    split = int(len(dataset) * (1 - val_ratio))
    train_indices = indices[:split]
    val_indices = indices[split:]

    # Create datasets
    train_dataset = torch.utils.data.Subset(dataset, train_indices)
    val_dataset = torch.utils.data.Subset(dataset, val_indices)

    logger.info(f"Split dataset into {len(train_dataset)} training and {len(val_dataset)} validation samples")

    return train_dataset, val_dataset

def create_data_loaders(train_dataset, val_dataset, batch_size=4):
    """
    Create data loaders for training and validation.

    Args:
        train_dataset: Training dataset
        val_dataset: Validation dataset
        batch_size: Batch size

    Returns:
        Tuple of (train_loader, val_loader)
    """
    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=2 if torch.cuda.is_available() else 0,
        pin_memory=torch.cuda.is_available()
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2 if torch.cuda.is_available() else 0,
        pin_memory=torch.cuda.is_available()
    )

    return train_loader, val_loader

#------------------------------------------------------------------------------
# Main Execution
#------------------------------------------------------------------------------

def main():
    """
    Main function to execute the complete workflow.
    """
    print("="*80)
    print("HISTORICAL DOCUMENT OCR TRANSCRIPTION LOADER")
    print("="*80)
    print("This script loads document images and their transcriptions,")
    print("creating a dataset for OCR model training.")
    print("\nChecking for existing transcriptions...")

    # Set paths
    base_path = '/content'

    # Load transcriptions and images
    page_transcriptions = load_transcriptions_and_images(base_path)

    # If no transcriptions were found, offer to upload
    if not page_transcriptions:
        print("\nNo transcriptions were successfully mapped to images.")
        print("You can upload transcription files manually and try again.")

        upload_now = input("Upload transcription files now? (y/n): ")
        if upload_now.lower() == 'y':
            page_transcriptions = upload_transcription_files()

            # Show example pages if transcriptions were successfully loaded this time
            if page_transcriptions:
                show_example_pages(page_transcriptions)
    else:
        # If transcriptions were found, show examples
        show_example_pages(page_transcriptions)

    # Create dataset if we have page transcriptions
    if page_transcriptions:
        print("\nCreating OCR dataset...")

        # Define transforms
        transform = transforms.Compose([
            transforms.Resize((384, 384)),
            transforms.ToTensor(),
        ])

        # Create dataset
        dataset = OCRDataset(page_transcriptions, transform=transform)

        # Create train-val split
        train_dataset, val_dataset = create_train_val_split(dataset)

        # Create data loaders
        batch_size = 4
        train_loader, val_loader = create_data_loaders(train_dataset, val_dataset, batch_size)

        print(f"\nDataset created successfully:")
        print(f"  - Total samples: {len(dataset)}")
        print(f"  - Training samples: {len(train_dataset)}")
        print(f"  - Validation samples: {len(val_dataset)}")

        # Show a sample batch if available
        if len(train_loader) > 0:
            sample_batch = next(iter(train_loader))
            print(f"\nSample batch shape: {sample_batch['image'].shape}")

            # Display a sample image and its text
            sample_idx = 0
            sample_img = sample_batch['image'][sample_idx].permute(1, 2, 0).numpy()
            sample_text = sample_batch['text'][sample_idx]
            sample_doc = sample_batch['doc_name'][sample_idx]
            sample_page = sample_batch['page_num'][sample_idx]

            plt.figure(figsize=(10, 14))
            plt.imshow(sample_img)
            plt.title(f"Document: {sample_doc}, Page: {sample_page}")
            plt.axis('off')
            plt.show()

            print(f"Sample text: {sample_text[:200]}...")
            print("\nDataset is ready for OCR model training!")
    else:
        print("\nNo dataset created - no transcriptions were successfully mapped to images.")

    print("\nProcess completed.")
    return page_transcriptions

# Execute the main function if run as a script
if __name__ == "__main__":
    page_transcriptions = main()

"""# 3. BETO Spanish Language Post-Processing"""

import torch
import re
import json
import os
from transformers import BertForMaskedLM, BertTokenizer
from tqdm.notebook import tqdm
import pandas as pd
import logging

# Define paths
base_path = '/content'
lexicon_path = os.path.join(base_path, 'historical_spanish_lexicon')
os.makedirs(lexicon_path, exist_ok=True)

# Initialize logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Initialize BETO model and tokenizer
print("Loading BETO model for Spanish language processing...")
tokenizer = BertTokenizer.from_pretrained("dccuchile/bert-base-spanish-wwm-cased", do_lower_case=False)
model = BertForMaskedLM.from_pretrained("dccuchile/bert-base-spanish-wwm-cased")
model.eval()

# Move to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

class SpanishHistoricalNormalizer:
    """
    Normalizes Spanish historical text with period-specific rules.

    Attributes:
        historical_mappings: Dictionary mapping historical to modern Spanish characters
        abbreviations: Dictionary mapping historical abbreviations to their expanded forms
        common_words: Set of common Spanish words to initialize the lexicon
        lexicon: Set of valid Spanish words for correction
    """

    def __init__(self):
        """Initialize the normalizer with Spanish historical text mappings."""
        # Initialize mappings for historical Spanish text
        self.historical_mappings = {
            # u/v interchangeability
            'u': 'v',
            'v': 'u',
            # i/j interchangeability
            'i': 'j',
            'j': 'i',
            # Long s
            'ſ': 's',
            # Ligatures
            'æ': 'ae',
            'œ': 'oe',
            # Other common variations
            'ç': 'z',
        }

        # Common abbreviations in historical Spanish
        self.abbreviations = {
            'q̃': 'que',
            'ẽ': 'en',
            'õ': 'on',
            'ã': 'an',
            'p̃': 'per',
            'ñ': 'nn',  # In some early texts
        }

        # Common words in historical Spanish to build initial lexicon
        self.common_words = {
            'el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas',  # Articles
            'de', 'del', 'a', 'al', 'en', 'con', 'por', 'para',     # Prepositions
            'y', 'e', 'o', 'u', 'ni', 'que', 'si', 'no',           # Conjunctions
            'este', 'esta', 'estos', 'estas', 'ese', 'esa',        # Demonstratives
            'mi', 'tu', 'su', 'mis', 'tus', 'sus',                 # Possessives
        }

        # Initialize lexicon with common words
        self.lexicon = set(self.common_words)

    def expand_lexicon_from_transcriptions(self, texts):
        """
        Expand lexicon using ground truth transcriptions.

        Args:
            texts: List of transcription texts

        Returns:
            Updated lexicon set
        """
        for text in texts:
            # Normalize and split into words
            clean_text = self.normalize_text(text)
            words = clean_text.split()

            # Add words to lexicon
            for word in words:
                if len(word) > 1:  # Skip single-letter words
                    self.lexicon.add(word.lower())

        logger.info(f"Expanded lexicon to {len(self.lexicon)} words")

        # Save lexicon to file
        lexicon_file = os.path.join(lexicon_path, "spanish_historical_lexicon.txt")
        with open(lexicon_file, 'w', encoding='utf-8') as f:
            for word in sorted(self.lexicon):
                f.write(f"{word}\n")

        return self.lexicon

    def normalize_text(self, text):
        """
        Apply basic normalization to text.

        Args:
            text: Input text

        Returns:
            Normalized text
        """
        # Convert to lowercase
        text = text.lower()

        # Replace historical characters
        for old, new in self.historical_mappings.items():
            text = text.replace(old, new)

        # Replace abbreviations
        for abbr, full in self.abbreviations.items():
            text = text.replace(abbr, full)

        # Remove punctuation and digits
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\d', ' ', text)

        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def find_closest_match(self, word, max_distance=2):
        """
        Find closest match in lexicon.

        Args:
            word: Word to match
            max_distance: Maximum Levenshtein distance for matches

        Returns:
            Closest match in lexicon
        """
        if word.lower() in self.lexicon:
            return word

        # Try variations
        for old, new in self.historical_mappings.items():
            if old in word.lower():
                variation = word.lower().replace(old, new)
                if variation in self.lexicon:
                    return variation

        # Use edit distance if no exact match
        min_distance = float('inf')
        best_match = word

        for lex_word in self.lexicon:
            if abs(len(lex_word) - len(word)) <= max_distance:
                distance = self.levenshtein_distance(word.lower(), lex_word)
                if distance < min_distance and distance <= max_distance:
                    min_distance = distance
                    best_match = lex_word

        return best_match if min_distance <= max_distance else word

    def levenshtein_distance(self, s1, s2):
        """
        Calculate edit distance between two strings.

        Args:
            s1: First string
            s2: Second string

        Returns:
            Levenshtein distance
        """
        if len(s1) < len(s2):
            return self.levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]

    def correct_with_lexicon(self, text):
        """
        Correct text using lexicon.

        Args:
            text: Input text

        Returns:
            Corrected text
        """
        words = re.findall(r'\b\w+\b', text)
        corrected_words = {}

        for word in words:
            if len(word) > 1:  # Skip single-letter words
                corrected = self.find_closest_match(word)
                if corrected != word:
                    corrected_words[word] = corrected

        # Apply corrections
        for word, corrected in corrected_words.items():
            text = re.sub(r'\b' + re.escape(word) + r'\b', corrected, text)

        return text

class BETOPostProcessor:
    """
    Use BETO to correct OCR errors in historical Spanish text.

    Attributes:
        model: BETO model
        tokenizer: BETO tokenizer
        normalizer: Spanish historical normalizer
        device: Device to run the model on
    """

    def __init__(self, model, tokenizer, normalizer):
        """
        Initialize the post-processor.

        Args:
            model: BETO model
            tokenizer: BETO tokenizer
            normalizer: Spanish historical normalizer
        """
        self.model = model
        self.tokenizer = tokenizer
        self.normalizer = normalizer

        # Move to correct device
        self.device = next(model.parameters()).device

    def mask_and_predict(self, text, confidence_threshold=0.8):
        """
        Mask words with low confidence and predict using BETO.

        Args:
            text: Input text
            confidence_threshold: Threshold for applying corrections

        Returns:
            Corrected text
        """
        # Tokenize the text
        tokens = self.tokenizer.tokenize(text)
        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)

        # Find words to mask (simple heuristic: short words, rare characters)
        mask_candidates = []
        for i, token in enumerate(tokens):
            # Skip punctuation and special tokens
            if token.startswith('##') or token in {'[CLS]', '[SEP]', '[PAD]', '[MASK]'}:
                continue

            # Check if token contains rare characters or is very short
            if any(c in token for c in 'ſæœ') or len(token) <= 2:
                mask_candidates.append(i)

        # No candidates to mask
        if not mask_candidates:
            return text

        corrections = {}

        # Process each mask candidate
        for mask_idx in mask_candidates:
            # Create a copy of tokens and mask the candidate
            masked_tokens = tokens.copy()
            original_token = masked_tokens[mask_idx]
            masked_tokens[mask_idx] = '[MASK]'

            # Convert to IDs and create tensor
            masked_ids = self.tokenizer.convert_tokens_to_ids(masked_tokens)
            masked_tensor = torch.tensor([masked_ids]).to(self.device)

            # Get predictions from BETO
            with torch.no_grad():
                outputs = self.model(masked_tensor)
                predictions = outputs[0]

            # Get predicted token
            masked_idx = masked_tokens.index('[MASK]')
            predicted_ids = torch.argsort(predictions[0, masked_idx], descending=True)[:5]
            predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_ids)

            # Get top prediction with confidence
            top_token = predicted_tokens[0]
            top_prob = torch.softmax(predictions[0, masked_idx], dim=0)[predicted_ids[0]].item()

            # Apply correction if confidence is high enough
            if top_prob > confidence_threshold and top_token != original_token:
                corrections[original_token] = (top_token, top_prob)

        # Apply corrections to the text
        corrected_text = text
        for original, (correction, prob) in corrections.items():
            corrected_text = re.sub(r'\b' + re.escape(original) + r'\b', correction, corrected_text)

        return corrected_text

    def fix_historical_specific_errors(self, text):
        """
        Fix specific errors common in historical Spanish OCR.

        Args:
            text: Input text

        Returns:
            Corrected text
        """
        rules = [
            # Fix common OCR errors with long s
            (r'\bdeſ', 'des'),
            (r'ſ', 's'),
            # Fix u/v confusion
            (r'\bvn\b', 'un'),
            (r'\bvna\b', 'una'),
            # Fix i/j confusion
            (r'\bj\b', 'i'),
            # Fix common abbreviations
            (r'q̃', 'que'),
            # Fix ligatures
            (r'æ', 'ae'),
            (r'œ', 'oe'),
            # Fix spacing issues
            (r' +', ' ')
        ]

        for pattern, replacement in rules:
            text = re.sub(pattern, replacement, text)

        return text

    def process_text(self, text):
        """
        Apply full post-processing pipeline to OCR text.

        Args:
            text: Input text

        Returns:
            Processed text
        """
        # Fix historical specific errors
        text = self.fix_historical_specific_errors(text)

        # Apply lexicon-based correction
        text = self.normalizer.correct_with_lexicon(text)

        # Apply BETO-based correction
        text = self.mask_and_predict(text)

        return text

def test_post_processing(ocr_results, num_examples=3):
    """
    Test post-processing on example OCR outputs.

    Args:
        ocr_results: List of OCR results
        num_examples: Number of examples to test

    Returns:
        List of post-processing results
    """
    examples = ocr_results[:num_examples]

    results = []
    for example in examples:
        ocr_text = example['ocr_text']

        # Apply post-processing
        corrected_text = post_processor.process_text(ocr_text)

        # Calculate improvement
        ground_truth = example['ground_truth']

        # Calculate metrics for original and corrected text
        original_metrics = calculate_metrics([ocr_text], [ground_truth])
        corrected_metrics = calculate_metrics([corrected_text], [ground_truth])

        # Store results
        results.append({
            'doc_name': example['doc_name'],
            'page_num': example['page_num'],
            'ground_truth': ground_truth[:200] + "..." if len(ground_truth) > 200 else ground_truth,
            'ocr_text': ocr_text[:200] + "..." if len(ocr_text) > 200 else ocr_text,
            'corrected_text': corrected_text[:200] + "..." if len(corrected_text) > 200 else corrected_text,
            'original_cer': original_metrics['cer'],
            'corrected_cer': corrected_metrics['cer'],
            'original_wer': original_metrics['wer'],
            'corrected_wer': corrected_metrics['wer'],
            'improvement_cer': original_metrics['cer'] - corrected_metrics['cer'],
            'improvement_wer': original_metrics['wer'] - corrected_metrics['wer']
        })

    # Display results
    for result in results:
        print(f"\nDocument: {result['doc_name']}, Page: {result['page_num']}")
        print(f"Ground truth: {result['ground_truth']}")
        print(f"Original OCR: {result['ocr_text']}")
        print(f"Corrected OCR: {result['corrected_text']}")
        print(f"Original CER: {result['original_cer']:.4f}, Original WER: {result['original_wer']:.4f}")
        print(f"Corrected CER: {result['corrected_cer']:.4f}, Corrected WER: {result['corrected_wer']:.4f}")
        print(f"Improvement - CER: {result['improvement_cer']:.4f}, WER: {result['improvement_wer']:.4f}")

    return results

def apply_post_processing(ocr_results):
    """
    Apply post-processing to all OCR results.

    Args:
        ocr_results: List of OCR results

    Returns:
        Tuple of (processed_results, original_metrics, corrected_metrics)
    """
    processed_results = []

    for result in tqdm(ocr_results, desc="Applying post-processing"):
        ocr_text = result['ocr_text']

        # Apply post-processing
        corrected_text = post_processor.process_text(ocr_text)

        # Store processed result
        processed_result = result.copy()
        processed_result['corrected_text'] = corrected_text
        processed_results.append(processed_result)

    # Calculate metrics
    ground_truths = [r['ground_truth'] for r in processed_results]
    ocr_texts = [r['ocr_text'] for r in processed_results]
    corrected_texts = [r['corrected_text'] for r in processed_results]

    # Calculate metrics for original OCR
    original_metrics = calculate_metrics(ocr_texts, ground_truths)

    # Calculate metrics for corrected OCR
    corrected_metrics = calculate_metrics(corrected_texts, ground_truths)

    print(f"\nOriginal metrics:")
    print(f"Character Error Rate (CER): {original_metrics['cer']:.4f}")
    print(f"Word Error Rate (WER): {original_metrics['wer']:.4f}")

    print(f"\nCorrected metrics:")
    print(f"Character Error Rate (CER): {corrected_metrics['cer']:.4f}")
    print(f"Word Error Rate (WER): {corrected_metrics['wer']:.4f}")

    print(f"\nImprovement:")
    print(f"CER improvement: {original_metrics['cer'] - corrected_metrics['cer']:.4f}")
    print(f"WER improvement: {original_metrics['wer'] - corrected_metrics['wer']:.4f}")

    # Add metrics to results
    for i in range(len(processed_results)):
        processed_results[i]['original_cer'] = original_metrics['cer_values'][i]
        processed_results[i]['original_wer'] = original_metrics['wer_values'][i]
        processed_results[i]['corrected_cer'] = corrected_metrics['cer_values'][i]
        processed_results[i]['corrected_wer'] = corrected_metrics['wer_values'][i]
        processed_results[i]['improvement_cer'] = original_metrics['cer_values'][i] - corrected_metrics['cer_values'][i]
        processed_results[i]['improvement_wer'] = original_metrics['wer_values'][i] - corrected_metrics['wer_values'][i]

    return processed_results, original_metrics, corrected_metrics

# Create the post-processor pipeline
print("Building Spanish historical post-processor...")
normalizer = SpanishHistoricalNormalizer()

# Expand lexicon from transcriptions
print("Expanding lexicon from transcriptions...")
texts = [data['text'] for data in page_transcriptions.values()]
normalizer.expand_lexicon_from_transcriptions(texts)

# Create BETO post-processor
post_processor = BETOPostProcessor(model, tokenizer, normalizer)

# Test on examples
print("\nTesting post-processing on example OCR outputs...")
post_processing_examples = test_post_processing(val_results, num_examples=2)

# Apply to all validation results
print("\nApplying post-processing to all validation results...")
processed_results, original_metrics, corrected_metrics = apply_post_processing(val_results)

# Create a DataFrame with the results
df_processed = pd.DataFrame(processed_results)
print("\nProcessed OCR Results:")
display(df_processed.head())

"""# 4. Complete End-to-End Pipeline with Metrics Visualization"""

import os
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
from tqdm.notebook import tqdm
from IPython.display import display, HTML
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Create output directories for results
base_path = '/content'
output_base_path = os.path.join(base_path, 'ocr_data')
results_path = os.path.join(output_base_path, 'results')
visualizations_path = os.path.join(results_path, 'visualizations')

os.makedirs(results_path, exist_ok=True)
os.makedirs(visualizations_path, exist_ok=True)

class HybridOCRPipeline:
    """
    End-to-end pipeline combining TrOCR with BETO post-processing.

    Attributes:
        trocr_model: TrOCR model
        trocr_processor: TrOCR processor
        beto_post_processor: BETO post-processor
        device: Device to run the model on
    """

    def __init__(self, trocr_model, trocr_processor, beto_post_processor):
        """
        Initialize the pipeline.

        Args:
            trocr_model: TrOCR model
            trocr_processor: TrOCR processor
            beto_post_processor: BETO post-processor
        """
        self.trocr_model = trocr_model
        self.trocr_processor = trocr_processor
        self.beto_post_processor = beto_post_processor
        self.device = next(trocr_model.parameters()).device

    def process_image(self, image_path):
        """
        Process a single image through the full pipeline.

        Args:
            image_path: Path to the image

        Returns:
            Dictionary with OCR results
        """
        # Load image
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            logger.error(f"Error loading image {image_path}: {e}")
            return None

        # Process with TrOCR
        pixel_values = self.trocr_processor(image, return_tensors="pt").pixel_values.to(self.device)
        generated_ids = self.trocr_model.generate(pixel_values, max_length=128)
        ocr_text = self.trocr_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

        # Apply BETO post-processing
        corrected_text = self.beto_post_processor.process_text(ocr_text)

        return {
            'ocr_text': ocr_text,
            'corrected_text': corrected_text
        }

    def process_batch(self, image_paths, ground_truths=None):
        """
        Process a batch of images, optionally with ground truth for evaluation.

        Args:
            image_paths: List of image paths
            ground_truths: Optional list of ground truth texts

        Returns:
            List of results
        """
        results = []

        for i, image_path in enumerate(tqdm(image_paths, desc="Processing images")):
            # Process image
            result = self.process_image(image_path)
            if result is None:
                continue

            # Add image path
            result['image_path'] = image_path

            # Add ground truth if available
            if ground_truths is not None and i < len(ground_truths):
                result['ground_truth'] = ground_truths[i]

            results.append(result)

        return results

    def evaluate(self, results):
        """
        Evaluate results if ground truth is available.

        Args:
            results: List of results

        Returns:
            Dictionary with evaluation metrics
        """
        if not results or 'ground_truth' not in results[0]:
            logger.warning("No ground truth available for evaluation")
            return None

        # Extract texts
        ground_truths = [r['ground_truth'] for r in results]
        ocr_texts = [r['ocr_text'] for r in results]
        corrected_texts = [r['corrected_text'] for r in results]

        # Calculate metrics
        original_metrics = calculate_metrics(ocr_texts, ground_truths)
        corrected_metrics = calculate_metrics(corrected_texts, ground_truths)

        return {
            'original': original_metrics,
            'corrected': corrected_metrics,
            'improvement': {
                'cer': original_metrics['cer'] - corrected_metrics['cer'],
                'wer': original_metrics['wer'] - corrected_metrics['wer']
            }
        }

def visualize_ocr_results(results, metrics, output_folder):
    """
    Create detailed visualizations of OCR results.

    Args:
        results: List of OCR results
        metrics: Dictionary with metrics
        output_folder: Output folder for visualizations
    """

    # 1. Error rates by document
    def plot_error_rates_by_document():
        # Group by document
        df = pd.DataFrame(results)
        df['doc_name'] = df['image_path'].apply(lambda x: os.path.basename(os.path.dirname(x)))

        # Calculate average error rates by document
        doc_metrics = df.groupby('doc_name').agg({
            'original_cer': 'mean',
            'corrected_cer': 'mean',
            'original_wer': 'mean',
            'corrected_wer': 'mean'
        }).reset_index()

        # Reshape for plotting
        plot_data = pd.melt(
            doc_metrics,
            id_vars=['doc_name'],
            value_vars=['original_cer', 'corrected_cer', 'original_wer', 'corrected_wer'],
            var_name='Metric',
            value_name='Error Rate'
        )

        # Create plot
        plt.figure(figsize=(12, 8))
        sns.barplot(x='doc_name', y='Error Rate', hue='Metric', data=plot_data)
        plt.title('Error Rates by Document')
        plt.xlabel('Document')
        plt.ylabel('Error Rate')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(output_folder, 'error_rates_by_document.png'))
        plt.close()

    # 2. Overall metrics comparison
    def plot_overall_metrics():
        # Create data
        metrics_data = {
            'Metric': ['Character Error Rate (CER)', 'Word Error Rate (WER)'],
            'Original': [metrics['original']['cer'], metrics['original']['wer']],
            'Corrected': [metrics['corrected']['cer'], metrics['corrected']['wer']]
        }

        # Convert to DataFrame
        df_metrics = pd.DataFrame(metrics_data)

        # Reshape for plotting
        plot_data = pd.melt(
            df_metrics,
            id_vars=['Metric'],
            value_vars=['Original', 'Corrected'],
            var_name='Pipeline Stage',
            value_name='Error Rate'
        )

        # Create plot
        plt.figure(figsize=(10, 6))
        sns.barplot(x='Metric', y='Error Rate', hue='Pipeline Stage', data=plot_data)
        plt.title('OCR Error Rates Before and After Correction')
        plt.ylabel('Error Rate')
        plt.ylim(0, 1)

        # Add text annotations
        for i, metric in enumerate(['cer', 'wer']):
            improvement = metrics['improvement'][metric]
            plt.text(
                i,
                max(metrics['original'][metric], metrics['corrected'][metric]) + 0.05,
                f"Improvement: {improvement:.4f} ({improvement/metrics['original'][metric]*100:.1f}%)",
                ha='center'
            )

        plt.tight_layout()
        plt.savefig(os.path.join(output_folder, 'overall_metrics.png'))
        plt.close()

    # 3. Example comparisons
    def create_example_comparisons(num_examples=3):
        for i, result in enumerate(results[:num_examples]):
            # Extract info
            image_path = result['image_path']
            doc_name = os.path.basename(os.path.dirname(image_path))
            page_name = os.path.basename(image_path)

            # Create figure
            fig, axes = plt.subplots(1, 2, figsize=(15, 10))

            # Display image
            image = Image.open(image_path)
            axes[0].imshow(np.array(image), cmap='gray')
            axes[0].set_title(f"Document: {doc_name}\nPage: {page_name}")
            axes[0].axis('off')

            # Display text comparison
            text_content = (
                f"Ground Truth:\n{result.get('ground_truth', 'N/A')[:200]}...\n\n"
                f"Original OCR:\n{result['ocr_text'][:200]}...\n\n"
                f"Corrected OCR:\n{result['corrected_text'][:200]}...\n\n"
                f"Metrics:\n"
                f"Original - CER: {result.get('original_cer', 'N/A'):.4f}, WER: {result.get('original_wer', 'N/A'):.4f}\n"
                f"Corrected - CER: {result.get('corrected_cer', 'N/A'):.4f}, WER: {result.get('corrected_wer', 'N/A'):.4f}\n"
                f"Improvement - CER: {result.get('improvement_cer', 'N/A'):.4f}, WER: {result.get('improvement_wer', 'N/A'):.4f}"
            )

            axes[1].text(0.05, 0.95, text_content, transform=axes[1].transAxes,
                         verticalalignment='top', wrap=True, fontsize=10)
            axes[1].set_title("OCR Results Comparison")
            axes[1].axis('off')

            plt.tight_layout()
            plt.savefig(os.path.join(output_folder, f'example_{i+1}.png'))
            plt.close()

    # 4. Error distribution
    def plot_error_distribution():
        # Extract CER and WER improvements
        improvements_cer = [r.get('improvement_cer', 0) for r in results]
        improvements_wer = [r.get('improvement_wer', 0) for r in results]

        # Create figure
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))

        # Plot CER improvements
        sns.histplot(improvements_cer, kde=True, ax=axes[0])
        axes[0].set_title('CER Improvement Distribution')
        axes[0].set_xlabel('CER Improvement')
        axes[0].axvline(x=0, color='red', linestyle='--')

        # Plot WER improvements
        sns.histplot(improvements_wer, kde=True, ax=axes[1])
        axes[1].set_title('WER Improvement Distribution')
        axes[1].set_xlabel('WER Improvement')
        axes[1].axvline(x=0, color='red', linestyle='--')

        plt.tight_layout()
        plt.savefig(os.path.join(output_folder, 'improvement_distribution.png'))
        plt.close()

    # Execute visualization functions
    try:
        logger.info("Creating visualizations...")
        plot_error_rates_by_document()
        plot_overall_metrics()
        create_example_comparisons()
        plot_error_distribution()
        logger.info(f"Visualizations saved to {output_folder}")
    except Exception as e:
        logger.error(f"Error creating visualizations: {e}")

def create_html_report(results, metrics, output_folder):
    """
    Create a detailed HTML report of OCR results.

    Args:
        results: List of OCR results
        metrics: Dictionary with metrics
        output_folder: Output folder for the report

    Returns:
        Path to the HTML report
    """

    # Format metrics for display
    def format_metrics(metrics_dict):
        return f"""
        <h3>Character Error Rate (CER)</h3>
        <p>Original: {metrics_dict['original']['cer']:.4f}</p>
        <p>Corrected: {metrics_dict['corrected']['cer']:.4f}</p>
        <p>Improvement: {metrics_dict['improvement']['cer']:.4f} ({metrics_dict['improvement']['cer']/metrics_dict['original']['cer']*100:.1f}%)</p>

        <h3>Word Error Rate (WER)</h3>
        <p>Original: {metrics_dict['original']['wer']:.4f}</p>
        <p>Corrected: {metrics_dict['corrected']['wer']:.4f}</p>
        <p>Improvement: {metrics_dict['improvement']['wer']:.4f} ({metrics_dict['improvement']['wer']/metrics_dict['original']['wer']*100:.1f}%)</p>
        """

    # Create examples HTML
    def create_examples_html(results, num_examples=5):
        examples_html = ""

        for i, result in enumerate(results[:num_examples]):
            # Extract document info
            image_path = result['image_path']
            doc_name = os.path.basename(os.path.dirname(image_path))
            page_name = os.path.basename(image_path)

            # Create example HTML
            examples_html += f"""
            <div class="example">
                <h3>Example {i+1}: {doc_name} - {page_name}</h3>
                <div class="example-content">
                    <div class="example-image">
                        <img src="{os.path.relpath(image_path, output_folder)}" alt="Document Image" style="max-width: 400px;">
                    </div>
                    <div class="example-text">
                        <h4>Ground Truth:</h4>
                        <pre>{result.get('ground_truth', 'N/A')[:300]}...</pre>

                        <h4>Original OCR:</h4>
                        <pre>{result['ocr_text'][:300]}...</pre>

                        <h4>Corrected OCR:</h4>
                        <pre>{result['corrected_text'][:300]}...</pre>

                        <h4>Metrics:</h4>
                        <p>Original - CER: {result.get('original_cer', 'N/A'):.4f}, WER: {result.get('original_wer', 'N/A'):.4f}</p>
                        <p>Corrected - CER: {result.get('corrected_cer', 'N/A'):.4f}, WER: {result.get('corrected_wer', 'N/A'):.4f}</p>
                        <p>Improvement - CER: {result.get('improvement_cer', 'N/A'):.4f}, WER: {result.get('improvement_wer', 'N/A'):.4f}</p>
                    </div>
                </div>
            </div>
            <hr>
            """

        return examples_html

    # Create report HTML
    html_content = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Historical Spanish OCR Report</title>
        <style>
            body {{ font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }}
            h1, h2, h3 {{ color: #2c3e50; }}
            pre {{ background-color: #f8f9fa; padding: 10px; border-radius: 5px; white-space: pre-wrap; }}
            .metrics {{ display: flex; }}
            .metrics-section {{ flex: 1; margin: 10px; padding: 15px; background-color: #f8f9fa; border-radius: 5px; }}
            .example {{ margin-bottom: 30px; }}
            .example-content {{ display: flex; }}
            .example-image {{ flex: 1; margin-right: 20px; }}
            .example-text {{ flex: 2; }}
            img {{ max-width: 100%; }}
            .visualization {{ margin: 20px 0; text-align: center; }}
            hr {{ border: 0; height: 1px; background-color: #ddd; margin: 30px 0; }}
        </style>
    </head>
    <body>
        <h1>Historical Spanish OCR Report</h1>

        <h2>Summary</h2>
        <p>This report presents the results of OCR processing on historical Spanish documents using a hybrid TrOCR-BETO system.</p>

        <div class="metrics">
            <div class="metrics-section">
                <h2>Overall Metrics</h2>
                {format_metrics(metrics)}
            </div>

            <div class="metrics-section">
                <h2>Dataset Information</h2>
                <p>Total documents: {len(set(os.path.basename(os.path.dirname(r['image_path'])) for r in results))}</p>
                <p>Total pages: {len(results)}</p>
            </div>
        </div>

        <h2>Visualizations</h2>

        <div class="visualization">
            <h3>Error Rates by Document</h3>
            <img src="visualizations/error_rates_by_document.png" alt="Error Rates by Document">
        </div>

        <div class="visualization">
            <h3>Overall Metrics Comparison</h3>
            <img src="visualizations/overall_metrics.png" alt="Overall Metrics">
        </div>

        <div class="visualization">
            <h3>Improvement Distribution</h3>
            <img src="visualizations/improvement_distribution.png" alt="Improvement Distribution">
        </div>

        <h2>Examples</h2>
        {create_examples_html(results)}

        <h2>Methodology</h2>
        <p>The OCR process consists of two main steps:</p>
        <ol>
            <li><strong>Base OCR</strong>: Using Microsoft's TrOCR model for initial text recognition from document images.</li>
            <li><strong>Post-Processing</strong>: Using BETO (Spanish BERT) and historical Spanish lexicon to correct OCR errors and improve accuracy.</li>
        </ol>

        <h2>Conclusion</h2>
        <p>The hybrid TrOCR-BETO system achieves significant improvements over the base TrOCR model alone,
        demonstrating the effectiveness of leveraging Spanish language models for historical document OCR.</p>
    </body>
    </html>
    """

    # Save HTML report
    report_path = os.path.join(output_folder, 'ocr_report.html')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(html_content)

    logger.info(f"HTML report saved to {report_path}")
    return report_path

def test_on_single_image(image_path, pipeline):
    """
    Test the OCR pipeline on a single image.

    Args:
        image_path: Path to the image
        pipeline: OCR pipeline

    Returns:
        OCR result
    """
    logger.info(f"Processing image: {image_path}")

    # Process image
    result = pipeline.process_image(image_path)

    # Display results
    print(f"Original OCR text: {result['ocr_text'][:100]}...")
    print(f"Corrected OCR text: {result['corrected_text'][:100]}...")

    # Display image
    plt.figure(figsize=(10, 10))
    image = Image.open(image_path)
    plt.imshow(np.array(image), cmap='gray')
    plt.title(f"Image: {os.path.basename(image_path)}")
    plt.axis('off')
    plt.show()

    return result

def find_test_image(output_base_path):
    """
    Find a test image from the processed images.

    Args:
        output_base_path: Base path for output files

    Returns:
        Path to a test image or None
    """
    for doc_dir in os.listdir(os.path.join(output_base_path, "processed_images")):
        doc_path = os.path.join(output_base_path, "processed_images", doc_dir)
        if os.path.isdir(doc_path):
            for img_file in os.listdir(doc_path):
                if img_file.endswith(('.jpg', '.png')):
                    return os.path.join(doc_path, img_file)
    return None

# Create the full pipeline
print("Creating full OCR pipeline...")
pipeline = HybridOCRPipeline(model, processor, post_processor)

# Create visualizations and report
print("Creating visualizations and report...")
combined_metrics = {
    'original': original_metrics,
    'corrected': corrected_metrics,
    'improvement': {
        'cer': original_metrics['cer'] - corrected_metrics['cer'],
        'wer': original_metrics['wer'] - corrected_metrics['wer']
    }
}

# Create visualizations
visualize_ocr_results(processed_results, combined_metrics, visualizations_path)

# Create HTML report
report_path = create_html_report(processed_results, combined_metrics, results_path)

# Test on a new image
test_image = find_test_image(output_base_path)
if test_image:
    test_result = test_on_single_image(test_image, pipeline)
else:
    logger.warning("No test image found")

# Save all results to CSV for further analysis
df_results = pd.DataFrame([
    {
        'doc_name': os.path.basename(os.path.dirname(r['image_path'])),
        'page_num': os.path.basename(r['image_path']),
        'original_cer': r.get('original_cer', 'N/A'),
        'corrected_cer': r.get('corrected_cer', 'N/A'),
        'original_wer': r.get('original_wer', 'N/A'),
        'corrected_wer': r.get('corrected_wer', 'N/A'),
        'improvement_cer': r.get('improvement_cer', 'N/A'),
        'improvement_wer': r.get('improvement_wer', 'N/A')
    }
    for r in processed_results
])

# Save to CSV
csv_path = os.path.join(results_path, 'ocr_results.csv')
df_results.to_csv(csv_path, index=False)
logger.info(f"Results saved to {csv_path}")

# Display summary
print("\nResults summary:")
display(df_results.describe())